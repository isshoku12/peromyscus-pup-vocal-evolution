{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544129e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the purpose of this notebook is to train Random Forest models on warbleR features\n",
    "# of vocalizations annotated in the notebook annotations_from_umap.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11784f",
   "metadata": {},
   "source": [
    "# preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940e262",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c69844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#file system\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#data handling\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#plotting\n",
    "import seaborn as sns \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#machine learning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# custom modules\n",
    "from src import parameters, machinelearning, segmentation, spectrogramming, features, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639172ef",
   "metadata": {},
   "source": [
    "## get path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root directory for the warbleR features for each species\n",
    "features_root = '/peromyscus-pup-vocal-evolution/data/features/'\n",
    "\n",
    "#root directory for the amplitude segmented developmental time course warbleR features\n",
    "segments_root = '/peromyscus-pup-vocal-evolution/data/audio/segments/'\n",
    "\n",
    "#root directory for the annotated vocalizations for each species\n",
    "annotations_root = '/peromyscus-pup-vocal-evolution/data/annotations/'\n",
    "\n",
    "#root to directory where models will be saved\n",
    "models_root = '/peromyscus-pup-vocal-evolution/data/models/random_forest/'\n",
    "\n",
    "#path to all warbleR features\n",
    "warbleR_features = '/peromyscus-pup-vocal-evolution/data/features/acoustic_features/'\n",
    "\n",
    "#annotated scratches from March 2022\n",
    "annotations_20220321 = '/peromyscus-pup-vocal-evolution/data/features/acoustic_features/'\n",
    "\n",
    "#params dict path\n",
    "params_dict_path = '/peromyscus-pup-vocal-evolution/parameters/'\n",
    "\n",
    "#clipping measurements\n",
    "clipping_path = '/peromyscus-pup-vocal-evolution/data/clipping/all_development_clipping.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbe2ec",
   "metadata": {},
   "source": [
    "# Get the hand annotated vocalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21a5114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features for developmental time course to train models\n",
    "dataset='development'\n",
    "iteration = '20220913_063001'\n",
    "\n",
    "#get the features\n",
    "warbleR_features = features.get(dataset, iteration, features_root)\n",
    "\n",
    "#add species info and drop Mus\n",
    "warbleR_features['species'] = [i.split('_')[0] for i in warbleR_features['source_file']]\n",
    "warbleR_features = warbleR_features.loc[~warbleR_features['species'].isin(['MU', 'MZ'])]\n",
    "\n",
    "#make sure you have no duplicates\n",
    "assert warbleR_features.duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4514247",
   "metadata": {},
   "source": [
    "## collect the annotated files and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807ebd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the vocalizations annoatted with Annotate from UMAP.ipynb\n",
    "\n",
    "species_list = ['BW', 'BK', 'NB', 'SW', 'PO', 'LO', 'GO', 'LL']\n",
    "annotated_files = []\n",
    "\n",
    "#get the annotations\n",
    "print('getting annotations...')\n",
    "all_combined = annotation.get(annotations_root, species_list)\n",
    "\n",
    "#change from the old label\n",
    "all_combined['human_label'] = ['USV' if i=='whistle' else i for i in all_combined['human_label']]\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2286bd7",
   "metadata": {},
   "source": [
    "## get the warbleR features for annotated vocalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8597165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge annotations with warbleR features\n",
    "labeled_warbleR_features = all_combined.merge(warbleR_features, on='source_file', how='left')\n",
    "\n",
    "#clean up the species column\n",
    "assert labeled_warbleR_features['species_x'].equals(labeled_warbleR_features['species_y'])\n",
    "labeled_warbleR_features = labeled_warbleR_features.drop(columns = ['species_x'])\n",
    "labeled_warbleR_features = labeled_warbleR_features.rename(columns={'species_y':'species'})\n",
    "annotations_df = labeled_warbleR_features.reset_index(drop=True)\n",
    "annotations_df['removal_flag'] = [float(i.split('_')[-6][-1]) if not i.split('_')[-6] == 'nan' else float('NaN') for i in annotations_df['source_file']]\n",
    "\n",
    "#make sure you only have cry and USV features\n",
    "assert set(annotations_df['human_label'].unique()) == set(['cry', 'USV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf2821",
   "metadata": {},
   "source": [
    "## drop any pups whose recordings have data transfer artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa36cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pups to exclude - these recordings were found to contain artefacts resulting from a data-transfer issue\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "to_drop = params_dict['excluded_pups']['development']\n",
    "\n",
    "# #drop them\n",
    "annotations_df['pup'] = [i.split('_clip')[0]+'.wav' for i in annotations_df['source_file']]\n",
    "print('dropping all vocs from', len(to_drop), 'pups: ', len(annotations_df.loc[annotations_df['pup'].isin(to_drop)]), 'vocs total out of', len(annotations_df))\n",
    "annotations_df = annotations_df.loc[~annotations_df['pup'].isin(to_drop)]\n",
    "assert len(annotations_df.loc[annotations_df['pup'].isin(to_drop)]) == 0\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3d999",
   "metadata": {},
   "source": [
    "## drop clipped vocalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07b3a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping_df = pd.read_csv(clipping_path)\n",
    "annotations_df_not_clipped = annotations_df.merge(clipping_df, how='left', on='source_file')\n",
    "annotations_df_not_clipped = annotations_df_not_clipped.loc[annotations_df_not_clipped['percent_clipped']==0]\n",
    "annotations_df_not_clipped = annotations_df_not_clipped.loc[annotations_df_not_clipped['removal_flag']==0]\n",
    "assert(annotations_df_not_clipped['removal_flag'].sum()==0)\n",
    "assert(annotations_df_not_clipped['percent_clipped'].sum()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7beaee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total label counts:\\n\\t')\n",
    "print('cry:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'cry']))\n",
    "print('whistle:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'USV']))\n",
    "print('scratch:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'scratch']))\n",
    "\n",
    "print('\\n')\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    print('\\tcry:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'cry'].loc[annotations_df_not_clipped['species'] == species]))\n",
    "    print('\\twhistle:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'USV'].loc[annotations_df_not_clipped['species'] == species]))\n",
    "    print('\\tscratch:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'scratch'].loc[annotations_df_not_clipped['species'] == species]))\n",
    "    print('\\ttotal:', len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'cry'].loc[annotations_df_not_clipped['species'] == species])+len(annotations_df_not_clipped.loc[annotations_df_not_clipped['human_label'] == 'whistle'].loc[annotations_df_not_clipped['species'] == species]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb9c8e",
   "metadata": {},
   "source": [
    "# Train a model to predict species from vocalization type (cry or USV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14fc87",
   "metadata": {},
   "source": [
    "## Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f19a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sampling parameters\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "seed = params_dict['figure_2_panels_AB']['sampling_seed']\n",
    "num_to_sample = 800\n",
    "\n",
    "#sample\n",
    "print('sampling...')\n",
    "ds_df = annotation.sample(seed, num_to_sample, annotations_df_not_clipped)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da0ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some useful info\n",
    "print('annotations by species:\\n')\n",
    "for i in species_list:\n",
    "    print(i,'cry :', len(ds_df.loc[ds_df['species'] == i].loc[ds_df['human_label'] == 'cry']))\n",
    "    print(i,'USV :', len(ds_df.loc[ds_df['species'] == i].loc[ds_df['human_label'] == 'USV']))\n",
    "    \n",
    "print('annotations by label:\\n')\n",
    "print('cry:', len(ds_df.loc[ds_df['human_label'] == 'cry']))\n",
    "print('USV:', len(ds_df.loc[ds_df['human_label'] == 'USV']))\n",
    "print('scratch:', len(ds_df.loc[ds_df['human_label'] == 'scratch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787916ff",
   "metadata": {},
   "source": [
    "## Train the USV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e70e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T10:46:16.922599Z",
     "start_time": "2023-01-09T10:46:16.919015Z"
    }
   },
   "outputs": [],
   "source": [
    "#voc_type to predict from\n",
    "voc_type = 'USV'\n",
    "\n",
    "#features for prediction\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "features_for_training = params_dict['figure_2_panels_AB']['features']\n",
    "\n",
    "#features and labels for training\n",
    "features_targets = ds_df.loc[ds_df['human_label'] == voc_type] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410645be",
   "metadata": {},
   "source": [
    "### check for missing and duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features_targets[features_for_training].isnull().values.any() == False\n",
    "print('No missing data')\n",
    "    \n",
    "assert features_targets.duplicated(subset=['source_file']).sum() == 0\n",
    "print('No duplicate data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3530a89",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the parameters\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "model_type = params_dict['figure_2_panels_AB']['model_type']\n",
    "split_random_state = params_dict['figure_2_panels_AB']['split_random_state']\n",
    "test_size = params_dict['figure_2_panels_AB']['test_size']\n",
    "\n",
    "#choose the data and the label and convert to numpy array\n",
    "target = 'species'\n",
    "USV_X = np.array(features_targets[features_for_training])\n",
    "USV_y = np.array(features_targets[target])\n",
    "\n",
    "#split the data\n",
    "print('splitting the data...')\n",
    "USV_X_train, USV_X_test, USV_y_train, USV_y_test = train_test_split(USV_X, USV_y, test_size = test_size, random_state = split_random_state)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d27c00",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43b9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training params\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "training_random_state = params_dict['figure_2_panels_AB']['training_random_state']\n",
    "n_estimators = params_dict['figure_2_panels_AB']['training_n_estimators']\n",
    "criterion = params_dict['figure_2_panels_AB']['training_criterion']\n",
    "bootstrap = params_dict['figure_2_panels_AB']['training_bootstrap']\n",
    "oob_score = params_dict['figure_2_panels_AB']['training_oob_score']\n",
    "\n",
    "#train\n",
    "print('training model...')\n",
    "USV_model = RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                   criterion = criterion, \n",
    "                                   random_state = training_random_state, \n",
    "                                   bootstrap = bootstrap,\n",
    "                                   oob_score = oob_score)\n",
    "USV_model.fit(USV_X_train, USV_y_train)\n",
    "\n",
    "#show out of bag score\n",
    "print(USV_model.oob_score_)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2c8a4",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9acf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training params\n",
    "machinelearning.save(model = USV_model,\n",
    "                     models_root = models_root, \n",
    "                     training_df = features_targets, \n",
    "                     feature_set = features_for_training, \n",
    "                     target = 'species', \n",
    "                     test_size = test_size, \n",
    "                     n_estimators = n_estimators, \n",
    "                     criterion = criterion, \n",
    "                     split_random_state = split_random_state,\n",
    "                     training_random_state = training_random_state,\n",
    "                     model_type = model_type,\n",
    "                     bootstrap = bootstrap, \n",
    "                     oob_score = USV_model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a39dd",
   "metadata": {},
   "source": [
    "## Train the cry model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b672aed",
   "metadata": {},
   "source": [
    "### same as above but now for cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11be7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc_type to predict from\n",
    "\n",
    "voc_type = 'cry'\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "#features for prediction\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "features_for_training = params_dict['figure_2_panels_AB']['features']\n",
    "\n",
    "#features and labels for training\n",
    "features_targets = ds_df.loc[ds_df['human_label'] == voc_type] \n",
    "\n",
    "#check for missing and duplicate data\n",
    "assert features_targets[features_for_training].isnull().values.any() == False\n",
    "print('No missing data')   \n",
    "assert features_targets.duplicated(subset=['source_file']).sum() == 0\n",
    "print('No duplicate data')\n",
    "\n",
    "#split the data\n",
    "model_type = params_dict['figure_2_panels_AB']['model_type']\n",
    "split_random_state = params_dict['figure_2_panels_AB']['split_random_state']\n",
    "test_size = params_dict['figure_2_panels_AB']['test_size']\n",
    "target = 'species'\n",
    "cry_X = np.array(features_targets[features_for_training])\n",
    "cry_y = np.array(features_targets[target])\n",
    "print('splitting the data...')\n",
    "cry_X_train, cry_X_test, cry_y_train, cry_y_test = train_test_split(cry_X, cry_y, test_size = test_size, random_state = split_random_state)\n",
    "print('done.')\n",
    "\n",
    "#get training params\n",
    "print('training...')\n",
    "training_random_state = params_dict['figure_2_panels_AB']['training_random_state']\n",
    "n_estimators = params_dict['figure_2_panels_AB']['training_n_estimators']\n",
    "criterion = params_dict['figure_2_panels_AB']['training_criterion']\n",
    "bootstrap = params_dict['figure_2_panels_AB']['training_bootstrap']\n",
    "oob_score = params_dict['figure_2_panels_AB']['training_oob_score']\n",
    "\n",
    "#train\n",
    "cry_model = RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                   criterion = criterion, \n",
    "                                   random_state = training_random_state, \n",
    "                                   bootstrap = bootstrap,\n",
    "                                   oob_score = oob_score)\n",
    "cry_model.fit(cry_X_train, cry_y_train)\n",
    "\n",
    "#show out of bag score\n",
    "print(cry_model.oob_score_)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388efce8",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed81bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training params\n",
    "machinelearning.save(model = cry_model,\n",
    "                     models_root = models_root, \n",
    "                     training_df = features_targets, \n",
    "                     feature_set = features_for_training, \n",
    "                     target = 'species', \n",
    "                     test_size = test_size, \n",
    "                     n_estimators = n_estimators, \n",
    "                     criterion = criterion, \n",
    "                     split_random_state = split_random_state,\n",
    "                     training_random_state = training_random_state,\n",
    "                     model_type = model_type,\n",
    "                     bootstrap = bootstrap, \n",
    "                     oob_score = USV_model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877aaa59",
   "metadata": {},
   "source": [
    "## evaluate (Figure 2 Panel A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4b4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "normalize_over = 'columns'\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "#Evaluate\n",
    "labels = ['BK', 'NB', 'SW', 'BW', 'PO', 'LO', 'GO', 'LL']\n",
    "cry_y_pred = cry_model.predict(cry_X_test)\n",
    "USV_y_pred = USV_model.predict(USV_X_test)\n",
    "\n",
    "#set up the axes for plot\n",
    "fig, axes = plt.subplots(nrows = 1, \n",
    "                         ncols = 2, \n",
    "                         figsize = [10,5], \n",
    "                         constrained_layout=True, \n",
    "                         dpi=600)\n",
    "\n",
    "if normalize_over == 'rows':\n",
    "    cry_cm = confusion_matrix(cry_y_test,cry_y_pred, labels = labels, normalize = 'true')\n",
    "    USV_cm = confusion_matrix(USV_y_test,USV_y_pred, labels = labels, normalize = 'true')\n",
    "    \n",
    "elif normalize_over == 'columns':\n",
    "    cry_cm = confusion_matrix(cry_y_test,cry_y_pred, labels = labels, normalize = 'pred')\n",
    "    USV_cm = confusion_matrix(USV_y_test,USV_y_pred, labels = labels, normalize = 'pred')\n",
    "    \n",
    "\n",
    "#normalize by row sum\n",
    "cry_cm_df = pd.DataFrame(cry_cm, columns=labels, index=labels)\n",
    "#cry_cm_df = cry_cm_df.div(cry_cm_df.sum(axis=0), axis=1)\n",
    "\n",
    "USV_cm_df = pd.DataFrame(USV_cm, columns=labels, index=labels)\n",
    "#USV_cm_df = USV_cm_df.div(USV_cm_df.sum(axis=0), axis=1)\n",
    "\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "\n",
    "#plot cry confusion matrix\n",
    "sns.heatmap(cry_cm_df, \n",
    "            annot=True, \n",
    "            annot_kws={\"size\": 12, \"color\":'lightgrey'}, \n",
    "            fmt=\".2f\",\n",
    "            cmap='viridis', \n",
    "            xticklabels=True, \n",
    "            yticklabels=True, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            square=True, \n",
    "            ax = axes[0], \n",
    "            cbar=False) \n",
    "\n",
    "#plot USV confusion matrix\n",
    "sns.heatmap(USV_cm_df, \n",
    "            annot=True, \n",
    "            annot_kws={\"size\": 12, \"color\":'lightgrey'}, \n",
    "            fmt=\".2f\",\n",
    "            cmap='viridis', \n",
    "            xticklabels=True, \n",
    "            yticklabels=True, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            square=True, \n",
    "            ax = axes[1], \n",
    "            cbar=True) \n",
    "\n",
    "cry_report = classification_report(cry_y_test, cry_y_pred, output_dict=True)\n",
    "USV_report = classification_report(USV_y_test, USV_y_pred, output_dict=True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('', dpi=600)\n",
    "    plt.savefig('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2212f",
   "metadata": {},
   "source": [
    "## train model to predict species from varying amounts of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c14b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate machine learning models trained on varying amounts of data\n",
    "\n",
    "#features for prediction\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "features_for_training = params_dict['figure_2_panels_AB']['features']\n",
    "\n",
    "#species\n",
    "species_list = ['BW', 'BK', 'NB', 'SW', 'PO', 'LO', 'GO', 'LL']\n",
    "\n",
    "#sample sizes to train on for each species\n",
    "sample_sizes = [20, 40, 200, 400, 600, 800, 1000, 1200, 1400]\n",
    "\n",
    "random_state = 123456\n",
    "\n",
    "print('cry')\n",
    "cry_prediction_df = machinelearning.get_metric_by_sample_size(\n",
    "                                                              voc_type = 'cry', \n",
    "                                                              voc_df = annotations_df_not_clipped, \n",
    "                                                              sample_sizes = sample_sizes,\n",
    "                                                              features = features_for_training, \n",
    "                                                              random_state = random_state, \n",
    "                                                              test_size = 0.2, \n",
    "                                                              target ='species', \n",
    "                                                              n_estimators = 100)\n",
    "print('USV')\n",
    "USV_prediction_df = machinelearning.get_metric_by_sample_size(\n",
    "                                                              voc_type = 'USV',\n",
    "                                                              voc_df = annotations_df_not_clipped, \n",
    "                                                              sample_sizes = sample_sizes,\n",
    "                                                              features = features_for_training, \n",
    "                                                              random_state = random_state, \n",
    "                                                              test_size = 0.2, \n",
    "                                                              target ='species', \n",
    "                                                              n_estimators = 100)\n",
    "\n",
    "evaluation_df = pd.concat([cry_prediction_df, USV_prediction_df])\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cc9b9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = evaluation_df.rename(columns={'f1-score':'f1score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0c0aa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = ''\n",
    "save_name = 'figure_2B_data.csv'\n",
    "evaluation_df.to_csv(os.path.join(save_path, save_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d4bd4",
   "metadata": {},
   "source": [
    "## plot colored by vocalization type (Figure 2 Panel B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6793d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save=False\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "save_dir = ''\n",
    "version = '20230207'\n",
    "metric = 'precision'\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "fontsize=9\n",
    "\n",
    "voc_name_color_dict = {'cry': 'deeppink', \n",
    "               'USV': 'thistle'}\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "species_color_dict = {'BW':'darkblue',\n",
    "              'NB': 'dodgerblue',\n",
    "              'BK': 'steelblue',\n",
    "              'SW': 'blue',\n",
    "              'PO': 'orange',\n",
    "              'LO': 'goldenrod',\n",
    "              'GO': 'green',\n",
    "              'LL': 'forestgreen',\n",
    "              'MU': 'mediumspringgreen', \n",
    "              'MZ': 'turquoise'}\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, \n",
    "                       nrows=1, \n",
    "                       constrained_layout=True, \n",
    "                       sharey=False, figsize = [4,2], dpi=600)\n",
    "\n",
    "\n",
    "model_df = evaluation_df.loc[evaluation_df['species'].isin(species_list)]\n",
    "model_df = evaluation_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "sns.stripplot(x='sample_size', \n",
    "                     y=metric, \n",
    "                     data=evaluation_df,\n",
    "                     jitter=False,\n",
    "                     dodge=True,\n",
    "                     hue='voc_type', \n",
    "                     palette=voc_name_color_dict, \n",
    "                     ax=ax, alpha=.75, s=5)\n",
    "\n",
    "sns.boxplot(x='sample_size', \n",
    "                     y=metric, \n",
    "                     data=evaluation_df, \n",
    "                     hue='voc_type',\n",
    "                dodge=True,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .1, \"color\":'black', 'alpha':0},\n",
    "                whiskerprops={\"linewidth\": .1, \"color\":'black', 'alpha':0},\n",
    "                width = .5, \n",
    "                showcaps=False,\n",
    "                ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylim([0,1])\n",
    "ax.legend([],[], frameon=False)\n",
    "\n",
    "ax.xaxis.set_tick_params(width=.5, rotation = 90, length = ytick_length, pad = ytick_pad)\n",
    "ax.yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "ax.set_yticks([0,.2, .4, .6, .8, 1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.axhline(y=(1/8), linestyle= 'dashed', color='black', linewidth=0.5)\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(.5)\n",
    "for label in (ax.get_yticklabels() + ax.get_xticklabels()):\n",
    "    label.set_fontname('Arial')\n",
    "    label.set_fontsize(fontsize)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if save:\n",
    "    save_name = ('_').join([metric, 'by_sample_size', version])\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.jpeg', dpi=600)\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.svg')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca881ec0",
   "metadata": {},
   "source": [
    "## Figure 2 panel B statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "597fb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lme4)\n",
    "\n",
    "fig2B_data_path <- \"\"\n",
    "fig2B <- read.csv(fig2B_data_path)\n",
    "fig2B <- subset(fig2B, species != \"accuracy\" & species != \"macro avg\" & species != \"weighted avg\")\n",
    "\n",
    "\n",
    "#mixed effect model\n",
    "rf.model <- lm(precision ~ sample_size + voc_type + species, data = fig2B)\n",
    "summary(rf.model)\n",
    "TukeyHSD(rf.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4fc5c",
   "metadata": {},
   "source": [
    "# Train a model to predict vocalization type (cry, USV, or nonvocal) from features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9a1bb",
   "metadata": {},
   "source": [
    "## downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e6d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sampling parameters\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "seed = params_dict['supp_figure_2']['sampling_seed']\n",
    "num_to_sample = params_dict['supp_figure_2']['num_to_sample']\n",
    "\n",
    "#sample\n",
    "print('sampling...')\n",
    "ds_df = annotation.sample(seed, num_to_sample, annotations_df)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4c946",
   "metadata": {},
   "source": [
    "## add the annotated nonvocal sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d106702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get annotated scratch sounds\n",
    "features_for_training = params_dict['supp_figure_2']['features']\n",
    "nonvocal_df = pd.read_csv(annotations_20220321)\n",
    "nonvocal_df['human_label'] = [i.split('_')[0] for i in nonvocal_df['source_file']]\n",
    "nonvocal_df_scratch = nonvocal_df.loc[nonvocal_df['human_label'] == 'scratch']\n",
    "nonvocal_df_scratch['species'] = [i.split('_')[1] for i in nonvocal_df_scratch['source_file']]\n",
    "nonvocal_df_scratch = nonvocal_df_scratch[features_for_training+['source_file','species', 'human_label']]\n",
    "\n",
    "#merge with vocalizations\n",
    "ds_df = ds_df.drop(columns = ['umap1', 'umap2', 'hdbscan_label', 'pup'])\n",
    "assert sorted(nonvocal_df_scratch.columns) == sorted(ds_df.columns)\n",
    "ds_df = pd.concat([ds_df, nonvocal_df_scratch]).reset_index(drop=True)\n",
    "\n",
    "#make sure you have all of the labels\n",
    "assert set(ds_df['human_label'].unique()) == set(['cry', 'USV', 'scratch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0319c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show annotations by species \n",
    "print('annotations by species:\\n')\n",
    "for i in species_list:\n",
    "    print(i,':', len(ds_df.loc[ds_df['species'] == i]))\n",
    "    \n",
    "# show annotations by label\n",
    "print('\\n')\n",
    "print('annotations by label:\\n')\n",
    "print('cry:', len(ds_df.loc[ds_df['human_label'] == 'cry']))\n",
    "print('USV:', len(ds_df.loc[ds_df['human_label'] == 'USV']))\n",
    "print('scratch:', len(ds_df.loc[ds_df['human_label'] == 'scratch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbdfc7",
   "metadata": {},
   "source": [
    "## select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8909f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features for prediction\n",
    "features_for_training = params_dict['supp_figure_2']['features']\n",
    "\n",
    "#features and labels for training\n",
    "features_targets = ds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7bca0",
   "metadata": {},
   "source": [
    "## check for missing data and make sure there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc3cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features_targets[features_for_training].isnull().values.any() == False\n",
    "print('No missing data')\n",
    "    \n",
    "assert features_targets.duplicated(subset=['source_file']).sum() == 0\n",
    "print('No duplicate data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05087265",
   "metadata": {},
   "source": [
    "## choose the model type and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c552ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get params\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "model_type = params_dict['supp_figure_2']['model_type']\n",
    "split_random_state = params_dict['supp_figure_2']['split_random_state']\n",
    "test_size = params_dict['supp_figure_2']['test_size']\n",
    "\n",
    "#choose the data and the label and convert to numpy array\n",
    "target = 'human_label'\n",
    "X = np.array(features_targets[features_for_training])\n",
    "y = np.array(features_targets[target])\n",
    "\n",
    "#split the data\n",
    "print('splitting the data...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = split_random_state)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9652e",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59bd3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get training params\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "training_random_state = params_dict['supp_figure_2']['training_random_state']\n",
    "n_estimators = params_dict['supp_figure_2']['training_n_estimators']\n",
    "criterion = params_dict['supp_figure_2']['training_criterion']\n",
    "bootstrap = params_dict['supp_figure_2']['training_bootstrap']\n",
    "oob_score = params_dict['supp_figure_2']['training_oob_score']\n",
    "\n",
    "#train\n",
    "print('training...')\n",
    "voc_type_model = RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                   criterion = criterion, \n",
    "                                   random_state = training_random_state, \n",
    "                                   bootstrap = bootstrap,\n",
    "                                   oob_score = oob_score)\n",
    "voc_type_model.fit(X_train, y_train)\n",
    "\n",
    "#show out of bag score\n",
    "print(voc_type_model.oob_score_)  \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aeb010",
   "metadata": {},
   "source": [
    "## evaluate (Supplementary Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73490c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on held out data and plot a confusion matrix\n",
    "\n",
    "save = False\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "y_pred = voc_type_model.predict(X_test)\n",
    "labels = ['cry', 'USV', 'scratch']\n",
    "cm = confusion_matrix(y_test,y_pred, labels = labels, normalize='true')\n",
    "fig = plt.figure(figsize=[3,3], dpi=600)\n",
    "\n",
    "# #normalize by row sum\n",
    "cm_df = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "# cm_df = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cm_df, \n",
    "            annot=True, \n",
    "            annot_kws={\"size\": 9}, \n",
    "            cmap='viridis', \n",
    "            xticklabels=True, \n",
    "            yticklabels=True, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            square=True) # font size\n",
    "\n",
    "classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('.jpeg', dpi=600)\n",
    "    plt.savefig('.svg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cfd78",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79439279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training params\n",
    "machinelearning.save(model = voc_type_model,\n",
    "                     models_root = models_root, \n",
    "                     training_df = features_targets, \n",
    "                     feature_set = features_for_training, \n",
    "                     target = 'voc_type', \n",
    "                     test_size = test_size, \n",
    "                     n_estimators = n_estimators, \n",
    "                     criterion = criterion, \n",
    "                     split_random_state = split_random_state,\n",
    "                     training_random_state = training_random_state,\n",
    "                     model_type = model_type,\n",
    "                     bootstrap = bootstrap, \n",
    "                     oob_score = voc_type_model.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "363612d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and training parameters\n",
    "\n",
    "#model ID\n",
    "model_ID = '20230203_044016'\n",
    "\n",
    "#path to model for labeling amplitude segmented vocalizations as cry, scratch or whistle\n",
    "voc_type_model_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_voc_type_model.pkl')\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_params')\n",
    "\n",
    "#load the model\n",
    "voc_type_model = pickle.load(open(voc_type_model_path, 'rb'))\n",
    "\n",
    "#load the training parameters\n",
    "voc_type_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                                  save_name=os.path.split(voc_type_params_path)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d83e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on held out data and plot a confusion matrix\n",
    "\n",
    "save = True\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "y_pred = voc_type_model.predict(X_test)\n",
    "labels = ['cry', 'USV', 'scratch']\n",
    "cm = confusion_matrix(y_test,y_pred, labels = labels, normalize='true')\n",
    "fig = plt.figure(figsize=[3,3], dpi=600)\n",
    "\n",
    "# #normalize by row sum\n",
    "cm_df = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "# cm_df = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cm_df, \n",
    "            annot=True, \n",
    "            annot_kws={\"size\": 9}, \n",
    "            cmap='viridis', \n",
    "            xticklabels=True, \n",
    "            yticklabels=True, \n",
    "            vmin=0, \n",
    "            vmax=1, \n",
    "            square=True) # font size\n",
    "\n",
    "classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig('.jpeg', dpi=600)\n",
    "    plt.savefig('.svg') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manuscript",
   "language": "python",
   "name": "manuscript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
