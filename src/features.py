#this file contains functions related to calculating, aggregating, and organizing features of 
#vocalizations and pups who made them

#file system
import os
import numpy as np
from numpy.lib.stride_tricks import as_strided
import pandas as pd
from tqdm import tqdm

#data
from scipy.io import wavfile
from scipy.signal import stft
from scipy.signal import hilbert
from datetime import date, datetime


def get(dataset, iteration, features_root):
    """
    Get the warbleR features saved as csv for a given dataset. Note this is 
    not intended to be a versatile function, just a helper to reproduce figures.

    Arguments:
        dataset (str): The dataset - must be one of 'development', 'bw_po_cf', 'bw_po_f1', or 'bw_po_f2'
        iteration (str): the ID of the iteration for the features - must be int he format YYYYMMDD_HHMMSS (ie generated by parameters.get_time())
        features_root (path): path to the directory containing the features csv files, with one sub-directory for each dataset
           
    Returns:
       features_df (dataframe): a dataframe of the features with all species/treatments 
       
    """
    
    to_combine = []
    features_dir = os.path.join(features_root, dataset, iteration)
    assert os.path.exists(features_dir)
    
    if dataset == 'development':
        files = ['BKwarbler_features.csv', 'BWwarbler_features.csv', 'GOwarbler_features.csv', 'LLwarbler_features.csv', 'LOwarbler_features.csv', 'MUwarbler_features.csv', 'MZwarbler_features.csv', 'NBwarbler_features.csv', 'POwarbler_features.csv', 'SWwarbler_features.csv']
    elif dataset == 'bw_po_cf':
        files = ['BWwarbler_features.csv', 'CF-BWwarbler_features.csv', 'CF-POwarbler_features.csv', 'POwarbler_features.csv']
    elif dataset == 'bw_po_f1':
        files = ['BW-PO-cross-F1warbler_features.csv', 'cross-BWwarbler_features.csv', 'cross-POwarbler_features.csv']
    elif dataset == 'bw_po_f2':
        files = ['ch1warbler_features.csv', 'ch2warbler_features.csv', 'ch3warbler_features.csv', 'ch4warbler_features.csv', 'ch5warbler_features.csv', 'ch6warbler_features.csv', 'ch7warbler_features.csv', 'ch8warbler_features.csv']
    
    for file in files:
        temp = pd.read_csv(os.path.join(features_dir, file))
        to_combine.append(temp)
    warbleR_features = pd.concat(to_combine).reset_index(drop=True)
    
    return warbleR_features
    

    
def get_filename_keys(dataset):
    """
    Get the category names used to label each file in a dataset

    Arguments:
        dataset (str): the dataset the pup comes from. Must be one of 'development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2'

    Returns:
       filename_keys (list): a list of keys to name the information in each section of each recording's file name
    """
    
    assert dataset in ['development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2']

    #these are keys for each dataset that correspond to information in each file name
    development_keys = ['species', 'breeding_pair','litter_number', 'pup_number', 'mic_channel', 'weight_mg', 'sex', 'start_temp', 'end_temp', 
                        'removal_flag', 'age', 'date', 'time']
    bw_po_cf_keys = ['species', 'breeding_pair', 'pup_number', 'age', 'weight_mg', 'sex', 'litter_size', 'date', 't ime']
    bw_po_f1_keys = development_keys.copy()
    bw_po_f2_keys = ['mic_chanel_prefix', 'cross_prefix', 'breeding_pair','family', 'litter_number', 'pup_number', 'mic_channel', 'weight_mg', 
                     'sex', 'start_temp', 'end_temp', 'removal_flag', 'age', 'date', 'time']

    if dataset == 'development':
        keys = development_keys    
    elif dataset == 'bw_po_cf':
        keys = bw_po_cf_keys
    elif dataset == 'bw_po_f1':
        keys = bw_po_f1_keys
    elif dataset == 'bw_po_f2':
        keys = bw_po_f2_keys

    return keys


def get_pup_metadata(source_path, dataset):
    """
    Get meta data for each pup in a dataset: species, dam/sire, litterID, littersize, weight, sex, age, temperatures.

    Arguments:
        source_path (str): the full path to the recording for which you want metdata
        dataset (str): the dataset the pup comes from. Must be one of 'development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2'

    Returns:
        meta_data (dict): a dictionary of the metadata, which can be further aggregated into a dataframe with multiple pups
    """
    #initialize dict and assert dataset is correct
    #"source_file" can refer to a raw recording from a pup (1 pup for each raw recording)
    #or it can refer to a single clip from a single vocalization from a pup
    #here it refers to the latter, but you should fix this potential ambiguity

    pup_dict = {}
    source_file = os.path.split(source_path)[-1]
    assert dataset in ['development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2']

    #get the keys to use for this dataset
    keys = get_filename_keys(dataset)

    #get the values from the file name and add to dict
    values = source_file.split('.')[0].split('_')
    assert len(keys) == len(values), "keys and values don't match"
    for key, value in zip(keys,values):
        pup_dict[key]=value
        pup_dict['pup'] = source_file

    return pup_dict


def aggregate_pup(source_path, features, features_df, drop_clipped):
    """
    Aggregate warbleR acoustic features for a single pup

    Arguments:
        source_path (str): the full path to the raw recording for which you want metdata
        features (list): list of warbleR features you want to aggregate
        features_df (dataframe): dataframe with the features you want to use
        drop_clipped (bool): if True, drop vocalizations that are clipped from acoustic feature aggregation (but not from counts)

    Returns:
        aggregate_data (dict): a dictionary of the metadata, which can be further aggregated into a dataframe with multiple pups

    """

    #get the pup name and assert dataset is what you expect
    pup = os.path.split(source_path)[-1]
    print(pup)
    assert len(features) == 26, "There should be 26 features"
    assert np.any(features_df.duplicated()) == 0, "There are duplicates in your features dataframe"
    assert pup.split('.')[0] in list(features_df['pup']), "the pup printed above isn't in the 'pup' column in the features_path file"
    assert 'percent_clipped' in features_df.columns, "You haven't added a column indicating which vocalizations are clipped"

    #get the vocalizations 
    feats = features_df.loc[features_df['pup'] == pup.split('.')[0]]

    #get the summary stats: cry count, USV count, scratch count, and mean, median, min, max, and standard deviation of each vocalization of each type

    #count data
    feats_dict = {}
    feats_dict['pup'] = pup
    feats_dict['cry_count'] = len(feats.loc[feats['predicted_label'] == 'cry'])
    feats_dict['USV_count'] = len(feats.loc[feats['predicted_label'] == 'USV'])
    feats_dict['scratch_count'] = len(feats.loc[feats['predicted_label'] == 'scratch'])
    feats_dict['total_sounds_detected'] = feats_dict['cry_count']+feats_dict['USV_count']+feats_dict['scratch_count']
    feats_dict['total_vocalizations_detected'] = feats_dict['cry_count']+feats_dict['USV_count']

    for feature in features:

        if drop_clipped:
            feats_no_clipping = feats.loc[feats['percent_clipped'] == 0]
            assert sum(feats_no_clipping['percent_clipped']) == 0

            if not feature in ['duration']: #don't get spectral features from clipped vocs
                variances = feats_no_clipping[[feature, 'predicted_label']].groupby(by=['predicted_label']).mean()
                means = feats_no_clipping[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.var)
                mins = feats_no_clipping[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.min)  
                maxs = feats_no_clipping[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.max)
                meds = feats_no_clipping[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.median) 

            else: #still get duration from clipped vocalizations
                variances = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).mean()
                means = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.var)
                mins = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.min)  
                maxs = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.max)
                meds = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.median) 
        
        else:
            variances = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).mean()
            means = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.var)
            mins = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.min)  
            maxs = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.max)
            meds = feats[[feature, 'predicted_label']].groupby(by=['predicted_label']).aggregate(np.median) 


        try: cry_mean = means.loc['cry'][0]
        except: cry_mean = float('NaN')

        try: cry_variance = variances.loc['cry'][0]
        except:cry_variance = float('NaN')

        try: cry_min = mins.loc['cry'][0]
        except:cry_min = float('NaN')

        try: cry_max = maxs.loc['cry'][0]
        except:cry_max = float('NaN')

        try: cry_med = meds.loc['cry'][0]
        except:cry_med = float('NaN')

        try: USV_mean = means.loc['USV'][0]
        except: USV_mean = float('NaN')

        try: USV_variance = variances.loc['USV'][0]
        except: USV_variance = float('NaN')

        try: USV_min = mins.loc['USV'][0]
        except: USV_min = float('NaN')

        try: USV_max = maxs.loc['USV'][0]
        except: USV_max = float('NaN')

        try: USV_med = meds.loc['USV'][0]
        except: USV_med = float('NaN')


        feats_dict[f"cry_{feature}_mean"] = cry_mean
        feats_dict[f"USV_{feature}_mean"] = USV_mean

        feats_dict[f"cry_{feature}_variance"] = cry_variance
        feats_dict[f"USV_{feature}_variance"] = USV_variance

        feats_dict[f"cry_{feature}_min"] = cry_min
        feats_dict[f"USV_{feature}_min"] = USV_min

        feats_dict[f"cry_{feature}_max"] = cry_max
        feats_dict[f"USV_{feature}_max"] = USV_max

        feats_dict[f"cry_{feature}_med"] = cry_med
        feats_dict[f"USV_{feature}_med"] = USV_med


    return feats_dict



def aggregate_all_pups(source_list, dataset, features, features_df, drop_clipped):
    """
    For each pup in source_list, aggregate warbleR acoustic features by pup, get pup metdata, then combine them into a single dataframe

    Arguments:
        source_list (list): list of full paths to source_files (one per pup) you want to process
        dataset (str): the dataset the pups come from. Must be one of 'development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2'
        features (list): the features to aggregate
        features_df (dataframe): dataframe of the features to aggregate
        drop_clipped (bool): if True, drop vocalizations that are clipped from acoustic feature aggregation (but not from counts)

    Returns:
        all_pup_data (dataframe): a dictionary of the metadata, which can be further aggregated into a dataframe with multiple pups

    """

    #check inputs 
    assert dataset in ['development', 'bw_po_f1', 'bw_po_cf', 'bw_po_f2']

    #get the metadata for every pup
    all_pup_metadata = []
    for source_path in source_list:
        pup_metadata = get_pup_metadata(source_path=source_path, dataset=dataset)
        pup_metadata = pd.DataFrame.from_records([pup_metadata])
        all_pup_metadata.append(pup_metadata)
    print('done collecting metadata...')

    #get the aggregate features for every pup
    all_pup_features = []
    for source_path in source_list:
        pup_features = aggregate_pup(source_path=source_path, features=features, features_df=features_df, drop_clipped=drop_clipped)
        pup_features = pd.DataFrame.from_records([pup_features])
        all_pup_features.append(pup_features)
    print('done collecting pup features...')

    #add all the pups toegther
    all_pup_features_df = pd.concat(all_pup_features)
    all_pup_metadata_df = pd.concat(all_pup_metadata)
    
    return all_pup_features_df, all_pup_metadata_df
        


def get_clipping(clip_path, threshold):
    
    """
    Get the percent of wav file that is clipped

    Arguments:
        clip_path (str): the path to the wav file
        threshold (float): percent of maximum value (32767) possible to be read with 16-bit encoding above which you will call audio "clipped"
    Returns:
        percent_clipped (float): percent of frames in the wav file that exceed the clipping threshold
    """

    #check inputs
    assert os.path.exists(clip_path)
    assert 0 < threshold < 1, "Threshold must be a value between 0 and 1"
    
    #calculate percent clipped
    fs, audio = wavfile.read(clip_path)
    rect_wav = np.abs(audio)
    clipping_limit = threshold*32767
    clipped = rect_wav[rect_wav>clipping_limit]
    percent_clipped = len(clipped)/len(audio)

    return percent_clipped
    
#same as above but iterate through a directory of clips
#returns two lists: the clipping percents and the corres
def get_clipping_batch(wav_dir, threshold, species = None):
    
    """
    Run get_clipping() on a batch of vocalization clips in a directory

    Arguments:
        wav_dir (str): the path to the directory containing the wav clips for which you want to evaluate clipping
        threshold (float): percent of maximum value (32767) possible to be read with 16-bit encoding above which you will call audio "clipped"
        species (str): optional 2 letter code for species if you only want to process one species at a time
    Returns:
        clipping_df (dataframe): a dataframe where each row is wav file (eg vocalization) and columns are path to wav file, % clipped, and threshold
    """

    if species != None:
        to_process = [os.path.join(wav_dir,i) for i in os.listdir(wav_dir) if i.startswith(species) and i.endswith('.wav')]

    else:
        to_process = [os.path.join(wav_dir,i) for i in os.listdir(wav_dir) if not i.startswith('.') and i.endswith('.wav')]

    source_files = []
    clipping_percents = []
    for wav in tqdm(to_process):
        #get clipping percent
        percent_clipped = get_clipping(clip_path=wav, threshold=threshold)
        
        #update 
        source_files.append(wav.split('/')[-1])
        clipping_percents.append(percent_clipped)

    clipping_df = pd.DataFrame()
    clipping_df['source_file'] = source_files
    clipping_df['percent_clipped'] = clipping_percents
    clipping_df['clipping_threshold'] = threshold*32767
    
    return clipping_df

def write_warbleR_job_scripts(dataset, save_root, wav_root, script_dir, path_to_warbleR_extract):
    """
    Write sbatch job files to run warbleR_feature_extraction.R on a computing cluster 
    
    Required processing steps:
        1. You have a csv of all pups to process with a column called species, which will be used to group the features into directories
        2. You have a directory containing one wav clip for every vocalization in the above csv (no subdirectories)
    
    Arguments:
        dataset (str): one of ['bw_po_cf', 'bw_po_f1', 'bw_po_f2', development] (cross foster, F1, F2, development)
        save_root (str): the place where csv of acoustic features will be saved
        wav_root (str): the place containing the wav files (one per vocalization) to get features from
        script_dir (str): the place to save the sbatch scripts (one per species)
        path_to_warbleR_extract (str): path to the file warbleR_extract.R
    
    Returns
        None
    """
    
    #check inputs
    assert dataset in ['bw_po_cf', 'bw_po_f1', 'bw_po_f2', 'development']
    assert os.path.exists(save_root)
    assert os.path.exists(wav_root)

    #get the species that you have segments for - note that for the non_development data sets these are not strictly species but some other 
    #useful way of grouping the recordings (treatment/mic channel)
    if dataset == 'bw_po_cf':

        species_list = sorted(['BW', 'PO', 'CF-BW', 'CF-PO'])

    elif dataset == 'bw_po_f1':
       
        species_list = sorted(['BW-PO-cross-F1', 'cross-BW', 'cross-PO'])

    elif dataset == 'bw_po_f2':
        
        species_list = sorted(['ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8'])
        
    elif dataset == 'development':
        
        species_list = sorted(['BW', 'GO', 'LL', 'LO', 'MU', 'MZ', 'NB', 'PO', 'SW', 'BK'])

    #make a dictionary for paths
    paths_dict = {}
    for species in species_list:
        paths_dict[species] = {}

    #make the path to the directory where features will be saved
    today = str(date.today())
    today = ('').join(today.split('-'))
    now = str(datetime.now())
    time = now.split(' ')[-1]
    time = ('').join(time.split('.')[0].split(':'))
    save_path = os.path.join(save_root,('_').join([today,time]))
    os.mkdir(save_path)
    
    #populate the dictionary
    for species in species_list:

        #get the path to the raw clips for which features will be calculated
        wav_path = os.path.join(wav_root, species)

        #add the the paths to dictions
        paths_dict[species]['wav_path'] = wav_path
        paths_dict[species]['save_path'] = save_path
        #for each species, write an .sbatch file (no job array here) with
        lines = [
        '#!/bin/bash\n', 
        '#\n', 
        '#SBATCH -J warb # A single job name for the array\n', 
        '#SBATCH -p hoekstra,shared\n', 
        '#SBATCH -c 1 # one core\n', 
        '#SBATCH -t 0-8:00 # Running time of 8 hours\n', 
        '#SBATCH --mem 16000 # Memory request of 24 GB\n', 
        '#SBATCH -o '+species+'_warblExtract_%A_%a.out # Standard output\n', 
        '#SBATCH -e '+species+'_warblExtract_%A_%a.err # Standard error\n',  
        '\n', 
        '#load R\n',
        'module load intel/2017c impi/2017.4.239 FFTW/3.3.8-fasrc01\n',
        'module load R/4.0.2-fasrc01\n',
        'export R_LIBS_USER=$HOME/apps/R_4.0.2:$R_LIBS_USER\n',
        '\n',
        '#set directories\n',
        'SPECIES='+ species + '\n',
        'WAVSDIR='+ paths_dict[species]['wav_path'] +'\n',
        'SAVEDIR='+ paths_dict[species]['save_path'] +'\n',
        '\n',
        '#run warbleR extract\n',
        'Rscript /n/hoekstra_lab_tier1/Users/njourjine/public_repositories/peromyscus-pup-vocal-evolution/scripts/warbleR_feature_extraction.R'+" $SPECIES"+" $WAVSDIR"+" $SAVEDIR"
        ]

        #write lines
        sbatch_name = 'warbleR_extract_'+species+'.sbatch'
        sbatch_save_path = os.path.join(script_dir, sbatch_name)
        with open(sbatch_save_path, 'w') as f:
            f.writelines(lines)
    
    
    #write a parent .sh script to run all of the species' sbatch scripts
    parent_lines =  []
    for species in species_list:
        parent_lines.extend(['echo ', species, '\n',
                           'sbatch warbleR_extract_', species, '.sbatch\n',
                           'sleep 5\n'])
        
    sh_name = 'warbleR_extract_parent.sh'
    sh_save_path = os.path.join(script_dir, sh_name)
    with open(sh_save_path, 'w') as f:
            f.writelines(parent_lines)
            
    print('wrote job scripts to:\n\t', script_dir)


        




   