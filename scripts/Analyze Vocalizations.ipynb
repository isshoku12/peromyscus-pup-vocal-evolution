{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ffcab6",
   "metadata": {},
   "source": [
    "# preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfedc91",
   "metadata": {},
   "source": [
    "## load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f88f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#file system\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#data \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#plotting\n",
    "import seaborn as sns \n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "# custom code\n",
    "from src import features, annotation, parameters, segmentation, spectrogramming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea92b16",
   "metadata": {},
   "source": [
    "## define path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a6dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: the directories written here are suggestions for useful places to put the raw and processed data\n",
    "\n",
    "#paths to raw unprocessed recordings for each of the four datasets (from Dryad: https://doi.org/10.5061/dryad.g79cnp5ts)\n",
    "all_wav_development = '/peromyscus-pup-vocal-evolution/data/audio/raw/development/'\n",
    "all_wav_bw_po_cf = '/peromyscus-pup-vocal-evolution/data/audio/raw/bw_po_cf/'\n",
    "all_wav_bw_po_f1 = '/peromyscus-pup-vocal-evolution/data/audio/raw/bw_po_f1/'\n",
    "all_wav_bw_po_f2 = '/peromyscus-pup-vocal-evolution/data/audio/raw/bw_po_f2/'\n",
    "\n",
    "#path to directory containing files for recording lengths so they don't have to be re-calculated multiple times\n",
    "recording_lengths_dir = '/peromyscus-pup-vocal-evolution/data/recording_lengths/'\n",
    "\n",
    "#path to directories containing annotations generated by Annotate from UMAP.ipynb\n",
    "annotations_root = '/peromyscus-pup-vocal-evolution/data/annotations/'\n",
    "\n",
    "#path to directory containing acoustic features of each vocalization generated by Prepare warbleR Job Scripts.ipynb\n",
    "acoustic_features_root = '/peromyscus-pup-vocal-evolution/data/features/acoustic_features/'\n",
    "\n",
    "#path to directories containing wav files for each vocalization generated by Segmenting and UMAP.ipynb\n",
    "clips_root = '/peromyscus-pup-vocal-evolution/data/audio/clips/'\n",
    "\n",
    "#path to directories containing spectrograms generated by Segmenting and UMAP.ipynb\n",
    "specs_root = '/peromyscus-pup-vocal-evolution/data/features/spectrograms/'\n",
    "\n",
    "#path to directory containing random forest model for labeling vocalizations generated by Train Models on Features.ipynb\n",
    "models_root = '/peromyscus-pup-vocal-evolution/data/models/random_forest/'\n",
    "\n",
    "#path to csv file with spectrogram value thresholds for determining background noise level (generated by Segmenting and UMAP.ipynb)\n",
    "noise_floors_path = '/peromyscus-pup-vocal-evolution/data/audio/noise/all_noise_floors.csv'\n",
    "\n",
    "#path to acoustic features for the development dataset, generated by Prepare warbleR Job Scripts.ipynb\n",
    "amplitude_acoustic_features = os.path.join(acoustic_features_root,'amplitude_segmented/development/')\n",
    "\n",
    "#path to the wav files for each vocalization in the development dataset, generated by Segmenting and UMAP\n",
    "amplitude_voc_clips = os.path.join(clips_root,'development/')\n",
    "\n",
    "#path to umap embeddings with hdbscan labels, ie the .feather files in processed_data/supplemental_figure_1/umap_embeddings from the Dryad dataset (https://doi.org/10.5061/dryad.g79cnp5ts)\n",
    "amplitude_umap_HDBSCAN_labeled = os.path.join(annotations_root, 'annotations_from_umap/')\n",
    "\n",
    "#csv files with information about clipping for each vocalization, generated by Segmenting and UMAP.ipynb\n",
    "clipping_path = '/peromyscus-pup-vocal-evolution/data/clipping/all_development_clipping.csv'\n",
    "clipping_path_bw_po_cf = '/peromyscus-pup-vocal-evolution/data/clipping/all_bw_po_cf_clipping.csv'\n",
    "clipping_path_bw_po_f1 = '/peromyscus-pup-vocal-evolution/data/clipping/all_bw_po_f1_clipping.csv'\n",
    "clipping_path_bw_po_f2 = '/peromyscus-pup-vocal-evolution/data/clipping/all_bw_po_f2_clipping.csv'\n",
    "\n",
    "#path to dictionary containing analysis parameters\n",
    "params_dict_path = '/peromyscus-pup-vocal-evolution/parameters/'\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "\n",
    "#csv file with sound pressure levels for each vocalization generated by Prepare warbleR Job Scripts.ipynb\n",
    "spl_path = '/peromyscus-pup-vocal-evolution/data/SPL/all_development_SPL.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135bf8b",
   "metadata": {},
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496a193",
   "metadata": {},
   "source": [
    "## color UMAP by HDBSCAN cluster (Figure 1C top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b448482",
   "metadata": {},
   "source": [
    "### collect UMAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd950e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the directories you want and print the embeddings you have available to plot\n",
    "coords_dir = amplitude_umap_HDBSCAN_labeled\n",
    "coords_list = os.listdir(coords_dir)\n",
    "\n",
    "print('coordinates available...')\n",
    "for i in coords_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "108873ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose species decide what to save\n",
    "species = 'MZ'\n",
    "save_umap_plot = False\n",
    "save_labeled_umap_plot = False\n",
    "save_label_verification = False\n",
    "\n",
    "#load the coordinates\n",
    "coords_path = glob.glob(coords_dir+species+'/*labeled.feather')[0]\n",
    "df_umap = pd.read_feather(coords_path)\n",
    "\n",
    "#just get the coordinates\n",
    "df_umap_small = df_umap[['umap1', 'umap2', 'source_file', 'label']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d1523",
   "metadata": {},
   "source": [
    "### plot with HDBSCAN labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "127dea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide to save or not\n",
    "save_labeled_umap_plot = False\n",
    "\n",
    "#show the umap plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "if species in ['MU', 'MZ']: #color dictionary for MU and MZ - make points ignored by HDBSCAN the same color as the\n",
    "                            #the ones it considered \"clusters\"\n",
    "    HDBSCAN_color_dict = {0: 'thistle', \n",
    "                          -1:'thistle'}\n",
    "else: #color dictionary for Peromyscus taxa\n",
    "    HDBSCAN_color_dict = {0: 'thistle', \n",
    "                          1: 'deeppink'}\n",
    "\n",
    "colmap = df_umap['label'].map(HDBSCAN_color_dict)\n",
    "\n",
    "ax = plt.scatter(\n",
    "    df_umap['umap1'],\n",
    "    df_umap['umap2'],\n",
    "    c = colmap,\n",
    "    s = .5,\n",
    "    alpha = .75, \n",
    "    cmap=None)\n",
    "\n",
    "xlims = [-15,15]\n",
    "ylims = [-15,15]\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(ylims)\n",
    "sns.despine()\n",
    "\n",
    "if save_labeled_umap_plot:\n",
    "    save_dir = ''\n",
    "    save_name = ('_').join([species,'final_amplitude_HDBSCAN_labeled_umap.jpeg'])\n",
    "    plt.savefig(os.path.join(save_dir,save_name),dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce157a",
   "metadata": {},
   "source": [
    "## Average spectrograms (Figure 1C middle/bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bd3065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the path to the vocalization clips and make sure it exists\n",
    "clips_dir = os.path.join(amplitude_voc_clips, species)\n",
    "assert os.path.exists(clips_dir)\n",
    "\n",
    "#get path to each wav clip\n",
    "all_files = [os.path.join(clips_dir,i) for i in df_umap['source_file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a63c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the spectrogram parameters\n",
    "spec_params = {\n",
    "    'min_freq': 5000, # minimum frequency\n",
    "    'max_freq': 125000, # maximum frequency\n",
    "    'nperseg': 512, # FFT\n",
    "    'noverlap': 512 // 4, # FFT\n",
    "    'spec_min_val': 0.5, # minimum log-spectrogram value \n",
    "    'fs': 250000, # audio samplerate\n",
    "    'fill_value': 0.5,\n",
    "    'max_duration':0.5, #set the max duration of each spectrogram to 0.5 seconds for all vocs and species\n",
    "    'num_time_bins':256,\n",
    "    'num_freq_bins':256,\n",
    "    'spec_max_val':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "318ef6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the spectrograms\n",
    "\n",
    "if species not in ['MU', 'MZ']: #if there are two clusters, generate spectrograms separately for each one\n",
    "    \n",
    "    label_0_files = [os.path.join(clips_dir,i) for i in df_umap['source_file'].loc[df_umap['label'] == 0]]\n",
    "    label_1_files = [os.path.join(clips_dir,i) for i in df_umap['source_file'].loc[df_umap['label'] == 1]]\n",
    "    \n",
    "    #get the label 0 specs\n",
    "    label_0_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                            noise_floors_path=noise_floors_path ,\n",
    "                                                            species = None, \n",
    "                                                            filtered_clips = label_0_files,\n",
    "                                                            noise_floor = None,\n",
    "                                                            spec_params=spec_params, \n",
    "                                                            num_to_process = 'all')\n",
    "\n",
    "    #get the label 1 specs\n",
    "    label_1_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                            noise_floors_path=noise_floors_path ,\n",
    "                                                            species = None, \n",
    "                                                            filtered_clips = label_1_files,\n",
    "                                                            noise_floor = None,\n",
    "                                                            spec_params=spec_params, \n",
    "                                                            num_to_process = 'all')\n",
    "\n",
    "\n",
    "else:\n",
    "    #get all the specs\n",
    "    all_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                        noise_floors_path=noise_floors_path, \n",
    "                                                        species = None, \n",
    "                                                        filtered_clips = all_files,\n",
    "                                                        noise_floor = None,\n",
    "                                                        spec_params=spec_params, \n",
    "                                                        num_to_process = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65a48a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "if species not in ['MU', 'MZ']: \n",
    "    \n",
    "    #average the spectrograms you just generated\n",
    "    label0_spec_avg = spectrogramming.spec_avg_from_list(label_0_specs_list)\n",
    "    label1_spec_avg = spectrogramming.spec_avg_from_list(label_1_specs_list)\n",
    "\n",
    "    #set up the figure\n",
    "    fig, axes = plt.subplots(nrows=2, \n",
    "                             ncols=1, \n",
    "                             figsize=(5,10))\n",
    "    \n",
    "    \n",
    "    #plot label 0 average\n",
    "    axes[0].set_axis_off()\n",
    "    axes[0].get_xaxis().set_visible(False)\n",
    "    axes[0].get_yaxis().set_visible(False)\n",
    "    axes[0].matshow(label0_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "    \n",
    "    #plot label 1 average\n",
    "    axes[1].set_axis_off()\n",
    "    axes[1].get_xaxis().set_visible(False)\n",
    "    axes[1].get_yaxis().set_visible(False)\n",
    "    axes[1].matshow(label1_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "    \n",
    "    #save \n",
    "    if save:\n",
    "        save_dir = ''\n",
    "        save_name = ('_').join([species,'_HDBSCAN_all_vocs_common_duration.jpeg'])\n",
    "        plt.savefig(fname=save_dir+save_name, dpi=600)\n",
    "    \n",
    "else: \n",
    "    \n",
    "    #average the spectrograms you just generated\n",
    "    all_spec_avg = spectrogramming.spec_avg_from_list(all_specs_list)\n",
    "\n",
    "    #set up the figure\n",
    "    fig, axes = plt.subplots(nrows=1, \n",
    "                             ncols=1, \n",
    "                             figsize=(5,5))\n",
    "    \n",
    "    \n",
    "    #plot label 0 average\n",
    "    axes.set_axis_off()\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "    axes.matshow(all_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "\n",
    "    #save \n",
    "    if save:\n",
    "        save_dir = ''\n",
    "        save_name = ('_').join([species,'_HDBSCAN_all_vocs_common_duration.jpeg'])\n",
    "        plt.savefig(fname=save_dir+save_name, dpi=600)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4646837",
   "metadata": {},
   "source": [
    "## PCA of acoustic features (Figure 1 panel D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19c34f",
   "metadata": {},
   "source": [
    "### set color palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_color_dict = params_dict['figure_1_panel_d']['species_color_dict']\n",
    "genus_color_dict = params_dict['figure_1_panel_d']['genus_color_dict']\n",
    "voc_name_color_dict = params_dict['figure_1_panel_d']['voc_name_color_dict']\n",
    "HDBSCAN_color_dict = params_dict['figure_1_panel_d']['HDBSCAN_color_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08a6f6",
   "metadata": {},
   "source": [
    "### get/clean the features and UMAP cluster labels for all species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34594eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting umap HDBSCAN labels...\n",
      "getting features...\n",
      "merging...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "#get umap cluster labels\n",
    "print('getting umap HDBSCAN labels...')\n",
    "umap_labels = os.path.join(amplitude_umap_HDBSCAN_labeled,'all_species_HDBSCAN_labels.csv')\n",
    "umap_df = pd.read_csv(umap_labels)\n",
    "\n",
    "#get features\n",
    "print('getting features...')\n",
    "features_path = os.path.join(amplitude_acoustic_features,'all_species_warbler_features.csv')\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "#remove 85 MU \"vocalizations\" that are recording artefacts (not in umap embedding)\n",
    "dropped = features_df['source_file'].loc[~features_df['source_file'].isin(umap_df['source_file'])]\n",
    "features_df = features_df.loc[~features_df['source_file'].isin(dropped)]\n",
    "\n",
    "#remove vocalizations that have nan for at least one acoustic feature (all of these are from MU)\n",
    "nans = features_df['source_file'].iloc[pd.isnull(features_df).any(1).to_numpy().nonzero()]\n",
    "features_df = features_df.loc[~features_df['source_file'].isin(nans)].sort_values(by='source_file').reset_index(drop=True)\n",
    "umap_df = umap_df.loc[~umap_df['source_file'].isin(nans)].sort_values(by='source_file').reset_index(drop=True)\n",
    "\n",
    "#make sure the source files are identical and merge\n",
    "print('merging...')\n",
    "merged_df = features_df.merge(umap_df, how='left', on='source_file')\n",
    "assert merged_df['source_file'].equals(features_df['source_file'])\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f90e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get raw audio files\n",
    "development_recordings = [i for i in os.listdir(all_wav_development) if not i.startswith('.')]\n",
    "raw_audio_df = pd.DataFrame()\n",
    "raw_audio_df['source_file'] = development_recordings\n",
    "raw_audio_df['pup'] = [i.split('_clip')[0] for i in raw_audio_df['source_file']]\n",
    "raw_audio_df['species'] = [i.split('_')[0] for i in raw_audio_df['source_file']]\n",
    "raw_audio_df['age'] = [i.split('_')[-3] if not i.split('_')[0] in ['MZ'] else i.split('_')[10] if 'R' not in i else i.split('_')[10] for i in raw_audio_df['source_file']]\n",
    "raw_audio_df['litter'] = [('_').join(i.split('_')[:3]+i.split('_')[-3:-1]) if not i.split('_')[0] in ['MZ'] else ('_').join([i.split('_')[0], i.split('_')[10],  i.split('_')[9]]) for i in raw_audio_df['source_file']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22b07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = ['BW', 'SW', 'BK', 'NB', 'PO', 'LO', 'GO', 'LL', 'MU', 'MZ']\n",
    "ages = ['p1', 'p3', 'p4', 'p5', 'p7', 'p9', 'p11', 'p13']\n",
    "\n",
    "print('Total vocalizations detected:', len(umap_df))\n",
    "print('Total pups recorded:', len(raw_audio_df))\n",
    "print('Total litters recorded:', len(raw_audio_df['litter'].unique()))\n",
    "print('Total Peromyscus pups recorded:', len(raw_audio_df['pup'].loc[~raw_audio_df['species'].isin(['MU', 'MZ'])].unique()))\n",
    "\n",
    "print('Pups per species:')\n",
    "for species in species_list:\n",
    "    print('\\t',species,':', len(raw_audio_df['pup'].loc[raw_audio_df['species'] == species].unique()))\n",
    "print('Vocalizations per species:')\n",
    "for species in species_list:\n",
    "    print('\\t',species,':', len(umap_df.loc[umap_df['species'] == species]))\n",
    "print('Litters per age per species:')\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    for age in ages:\n",
    "        print('\\t',age,':', len(raw_audio_df['pup'].loc[raw_audio_df['species'] == species].loc[raw_audio_df['age'] == age].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96235f",
   "metadata": {},
   "source": [
    "### do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f79079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#get the features\n",
    "features_set = params_dict['figure_1_panel_d']['features']\n",
    "pca_df = features_df[features_set]\n",
    "\n",
    "# Standardizing the features\n",
    "standardized_features = StandardScaler().fit_transform(pca_df)\n",
    "\n",
    "#2 component PCA\n",
    "print('doing PCA...')\n",
    "pca = PCA(n_components = 2)\n",
    "pca_embedding = pca.fit_transform(standardized_features)\n",
    "print('\\texplained variance by PC:', pca.explained_variance_ratio_)\n",
    "print('\\ttotal variance explained:', sum(pca.explained_variance_ratio_))\n",
    "\n",
    "#add to features_df\n",
    "features_df['pc_1'] = pca_embedding[:,0]\n",
    "features_df['pc_2'] = pca_embedding[:,1]\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37178b51",
   "metadata": {},
   "source": [
    "### get PCA loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8f9fdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_.T, columns=['pc1', 'pc2'], index=pca_df.columns)\n",
    "loadings['pc1'] = np.abs(loadings['pc1'])\n",
    "loadings['pc2'] = np.abs(loadings['pc2'])\n",
    "loadings = loadings.sort_values(by = ['pc1'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad62cf",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34029fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save = False\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = [4,4], dpi= 200)\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "colors = features_df['species'].map(genus_color_dict)\n",
    "\n",
    "#plot Peromyscus\n",
    "ax1.scatter(features_df['pc_1'].loc[~features_df['species'].isin(['MU', 'MZ'])], \n",
    "            features_df['pc_2'].loc[~features_df['species'].isin(['MU', 'MZ'])],\n",
    "            c= features_df['species'].loc[~features_df['species'].isin(['MU', 'MZ'])].map(genus_color_dict),\n",
    "            alpha = .0075, \n",
    "            s=.5)\n",
    "\n",
    "#plot Mus\n",
    "ax1.scatter(features_df['pc_1'].loc[features_df['species'].isin(['MU', 'MZ'])], \n",
    "            features_df['pc_2'].loc[features_df['species'].isin(['MU', 'MZ'])],\n",
    "            c= features_df['species'].loc[features_df['species'].isin(['MU', 'MZ'])].map(genus_color_dict),\n",
    "            alpha = .015, \n",
    "            s=.5)\n",
    "\n",
    "ax1.set_xlim([-10,10])\n",
    "ax1.set_ylim([-10,10])\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "if save:\n",
    "    save_dir = ''\n",
    "    save_name = 'figure1_PCA.jpeg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name), dpi=600)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8ed48",
   "metadata": {},
   "source": [
    "## Violin Plots of Acoustic Features by Species (Figure 1E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cdfb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set up the figures\n",
    "fig = plt.figure(figsize = [5,5], constrained_layout=True, dpi=200)\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "order = ['BK', 'NB', 'SW', 'BW', 'PO', 'LO', 'GO', 'LL', 'MU', 'MZ']\n",
    "\n",
    "#plot the dots\n",
    "sns.stripplot(data = features_df, ax = ax1, y='duration', x='species', hue = 'species', alpha=.01, s=.75, jitter=.075, order=order, palette = species_color_dict)\n",
    "sns.stripplot(data = features_df, ax = ax2, y='meanfreq',x='species', hue = 'species', alpha=.01, s=.75, jitter=.075, order=order, palette = species_color_dict)\n",
    "\n",
    "#plot the violins\n",
    "sns.violinplot(data = features_df, ax = ax1, y='duration', x='species', scale = 'width', width = .75, alpha=1, order=order, inner=None, color = 'lightgray', linewidth=0)\n",
    "sns.violinplot(data = features_df, ax = ax2, y='meanfreq', x='species', scale = 'width', width = .75, alpha=1, order=order, inner=None, color = 'lightgray', linewidth=0)\n",
    "\n",
    "#remove the unecessary boundaries and legends\n",
    "sns.despine()\n",
    "ax1.legend([],[], frameon = False)\n",
    "ax2.legend([],[], frameon = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714b156",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering of Acoustic Features (Figure 1 panel F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f8a85",
   "metadata": {},
   "source": [
    "### sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "795c77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#set seed for reproducible sampling\n",
    "seed = params_dict['figure_1_panel_f']['sampling_seed']\n",
    "\n",
    "#get the features\n",
    "features_set = params_dict['figure_1_panel_f']['features']\n",
    "\n",
    "#get how many vocs to sample\n",
    "num_to_sample = params_dict['figure_1_panel_f']['num_to_sample']\n",
    "\n",
    "#sample \n",
    "cluster_df = merged_df.sample(n=num_to_sample, random_state=seed)\n",
    "print('sampled',len(cluster_df),'vocalizations...')\n",
    "\n",
    "#scale the columns \n",
    "print('scaling...')\n",
    "standardized_features = pd.DataFrame(MinMaxScaler().fit_transform(cluster_df[features_set]), columns = features_set, index=cluster_df.index)\n",
    "standardized_features = standardized_features[features_set]\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f881208",
   "metadata": {},
   "source": [
    "### cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0615279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "#define the distance below which leaves will not be merged\n",
    "distance_threshold = params_dict['figure_1_panel_f']['distance_threshold']\n",
    "\n",
    "#define the clustering model to use\n",
    "clusterer = AgglomerativeClustering(n_clusters=None, \n",
    "                                    affinity='euclidean', \n",
    "                                    linkage='ward', \n",
    "                                    compute_full_tree=True, \n",
    "                                    distance_threshold = distance_threshold)\n",
    "#use it\n",
    "print('clustering...')\n",
    "clusterer.fit_predict(standardized_features)\n",
    "\n",
    "#get clustering results\n",
    "cluster_labels = clusterer.labels_\n",
    "num_clusters = clusterer.n_clusters_\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e8e48",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8be5afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may take a few minutes\n",
    "\n",
    "#set the colors\n",
    "voc_colmap = cluster_df['label'].astype(str).map(HDBSCAN_color_dict)\n",
    "species_colmap = cluster_df['species_x'].map(species_color_dict)\n",
    "genus_colmap = cluster_df['species_x'].map(genus_color_dict)\n",
    "row_colors = [voc_colmap, species_colmap, genus_colmap]\n",
    "\n",
    "#plot\n",
    "sns.clustermap(standardized_features,\n",
    "               method = 'ward', \n",
    "               cmap = 'viridis', \n",
    "               col_cluster=False,\n",
    "               figsize = (15, 12),\n",
    "               cbar_pos=(0, .2, .03, .1),\n",
    "               row_colors = row_colors, \n",
    "               mask = None, \n",
    "               colors_ratio=.03, \n",
    "               tree_kws={'linewidth':2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530631e",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c713c",
   "metadata": {},
   "source": [
    "## get model to predict vocalization types (cry, USV, non-vocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9013d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model ID\n",
    "model_ID = '20230203_044016'\n",
    "\n",
    "#path to model for labeling amplitude segmented vocalizations as cry, scratch or USV\n",
    "voc_type_model_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_voc_type_model.pkl')\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_params')\n",
    "\n",
    "#load the model\n",
    "voc_type_model = pickle.load(open(voc_type_model_path, 'rb'))\n",
    "\n",
    "#load the training parameters\n",
    "model_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                               save_name=os.path.split(voc_type_params_path)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcb080",
   "metadata": {},
   "source": [
    "## get/clean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f8165b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get umap cluster labels\n",
    "print('getting umap HDBSCAN labels...')\n",
    "umap_labels = os.path.join(amplitude_umap_HDBSCAN_labeled,'all_species_HDBSCAN_labels.csv')\n",
    "umap_df = pd.read_csv(umap_labels)\n",
    "\n",
    "#get features\n",
    "print('getting features...')\n",
    "features_path = os.path.join(amplitude_acoustic_features,'all_species_warbler_features.csv')\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "#remove 85 MU \"vocalizations\" that are recording artefacts (not in umap embedding)\n",
    "dropped = features_df['source_file'].loc[~features_df['source_file'].isin(umap_df['source_file'])]\n",
    "features_df = features_df.loc[~features_df['source_file'].isin(dropped)]\n",
    "\n",
    "#add clipping information\n",
    "clipping_df = pd.read_csv(clipping_path)\n",
    "features_df = features_df.merge(clipping_df, how='left', on='source_file')\n",
    "\n",
    "#remove vocalizations that have nan for at least one acoustic feature (all of these are from MU)\n",
    "nans = features_df['source_file'].iloc[pd.isnull(features_df).any(1).to_numpy().nonzero()]\n",
    "features_df = features_df.loc[~features_df['source_file'].isin(nans)].sort_values(by='source_file').reset_index(drop=True)\n",
    "umap_df = umap_df.loc[~umap_df['source_file'].isin(nans)].sort_values(by='source_file').reset_index(drop=True)\n",
    "\n",
    "#make sure the source files are identical and merge\n",
    "print('merging...')\n",
    "assert umap_df['source_file'].equals(features_df['source_file'])\n",
    "merged_df = features_df.merge(umap_df, on='source_file')\n",
    "assert(merged_df['species_x'].equals(merged_df['species_y']))\n",
    "merged_df = merged_df.rename(columns={'species_x':'species'}).drop(columns=['species_y'])\n",
    "merged_df['pup'] = [i.split('_clip')[0]+'.wav' for i in merged_df['source_file']]\n",
    "\n",
    "#drop pups whose recordings were found to contain artefacts (vertical lines caused by data transfer issue) during annotation\n",
    "pups_to_drop = params_dict['excluded_pups']['development']\n",
    "print('dropping', len(merged_df.loc[merged_df['pup'].isin(pups_to_drop)]), 'vocs because they come from bad recordings...')\n",
    "merged_df = merged_df.loc[~merged_df['pup'].isin(pups_to_drop)]\n",
    "assert len(merged_df.loc[merged_df['pup'].isin(pups_to_drop)]) == 0\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d738ca",
   "metadata": {},
   "source": [
    "## predict vocalization labels from features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437bae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home10/jourjine/.conda/envs/manuscript/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "#this takes a few minutes \n",
    "\n",
    "#get the features you trained on\n",
    "for_prediction = merged_df[model_params['feature_set']]\n",
    "\n",
    "#predict vocalization type from those features\n",
    "predicted_labels = voc_type_model.predict(for_prediction)\n",
    "\n",
    "#add them to the dataset\n",
    "merged_df['predicted_label'] = predicted_labels\n",
    "assert set(merged_df['predicted_label'].unique()) == set(['cry', 'USV', 'scratch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1853766",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "save_dir = ''\n",
    "all_predicted_save_name = 'figure2_all_development_vocs_with_predictions.csv'\n",
    "\n",
    "if save:\n",
    "    merged_df.to_csv(os.path.join(save_dir, all_predicted_save_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff0bbc",
   "metadata": {},
   "source": [
    "## general average spectrograms by label (Supplemental Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "save_dir = ''\n",
    "all_predicted_save_name = 'figure2_all_development_vocs_with_predictions.csv'\n",
    "merged_df = pd.read_csv(os.path.join(os.path.join(save_dir, all_predicted_save_name)))\n",
    "\n",
    "#make the spectrograms using a common max duration for all vocalizations and species (.5 seconds)\n",
    "species_list = ['GO'] \n",
    "\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    max_dur = 0.5\n",
    "    clips_dir = os.path.join(amplitude_voc_clips, species)\n",
    "    cry_files = [os.path.join(clips_dir,i) for i in merged_df['source_file'].loc[merged_df['predicted_label'] == 'cry'].loc[merged_df['species'] == species]]\n",
    "    USV_files = [os.path.join(clips_dir,i) for i in merged_df['source_file'].loc[merged_df['predicted_label'] == 'USV'].loc[merged_df['species'] == species]]\n",
    "    scratch_files = [os.path.join(clips_dir,i) for i in merged_df['source_file'].loc[merged_df['predicted_label'] == 'scratch'].loc[merged_df['species'] == species]]\n",
    "\n",
    "    spec_params = {\n",
    "        'min_freq': 5000, # minimum frequency\n",
    "        'max_freq': 125000, # maximum frequency\n",
    "        'nperseg': 512, # FFT\n",
    "        'noverlap': 512 // 4, # FFT\n",
    "        'spec_min_val': 0.5, # minimum log-spectrogram value - update from noise floors dataframe if noise_floors_path provided\n",
    "        'fs': 250000, # audio samplerate\n",
    "        'downsample_by':2, #2 means take every other pixel from the original spectrogram - only usedin interpolate = False\n",
    "        'log_resize_scaling_factor':None, \n",
    "        'fill_value': .5,\n",
    "        'max_duration':max_dur,\n",
    "        'num_time_bins':256,\n",
    "        'num_freq_bins':256,\n",
    "        'spec_max_val':10\n",
    "    }\n",
    "\n",
    "    print('getting cry specs...')\n",
    "    cry_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                            noise_floors_path=noise_floors_path ,\n",
    "                                                            species = None, \n",
    "                                                            filtered_clips = cry_files,\n",
    "                                                            noise_floor = None,\n",
    "                                                            spec_params=spec_params, \n",
    "                                                            num_to_process = 'all')\n",
    "\n",
    "    print('getting USV specs...')\n",
    "    USV_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                            noise_floors_path=noise_floors_path ,\n",
    "                                                            species = None, \n",
    "                                                            filtered_clips = USV_files,\n",
    "                                                            noise_floor = None,\n",
    "                                                            spec_params=spec_params, \n",
    "                                                            num_to_process = 'all')\n",
    "\n",
    "    print('getting scratch specs...')\n",
    "    scratch_specs_list, _ = spectrogramming.specs_from_wavs(clips_dir = clips_dir, \n",
    "                                                            noise_floors_path=noise_floors_path ,\n",
    "                                                            species = None, \n",
    "                                                            filtered_clips = scratch_files,\n",
    "                                                            noise_floor = None,\n",
    "                                                            spec_params=spec_params, \n",
    "                                                            num_to_process = 'all')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd00b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the specs\n",
    "save = False\n",
    "\n",
    "cry_spec_avg = spectrogramming.spec_avg_from_list(cry_specs_list)\n",
    "USV_spec_avg = spectrogramming.spec_avg_from_list(USV_specs_list)\n",
    "scratch_spec_avg = spectrogramming.spec_avg_from_list(scratch_specs_list)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "print('making cry spec...')\n",
    "ax1.set_axis_off()\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "ax1.set_title(species+' predicted cry n='+str(len(cry_specs_list)))\n",
    "ax1.matshow(cry_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "\n",
    "print('making USV spec...')\n",
    "ax2.set_axis_off()\n",
    "ax2.get_xaxis().set_visible(False)\n",
    "ax2.get_yaxis().set_visible(False)\n",
    "ax2.set_title(species+' predicted USV n='+str(len(USV_specs_list)))\n",
    "ax2.matshow(USV_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "\n",
    "print('making scratch spec...')\n",
    "ax3.set_axis_off()\n",
    "ax3.get_xaxis().set_visible(False)\n",
    "ax3.get_yaxis().set_visible(False)\n",
    "ax3.set_title(species+' predicted nonvocal n='+str(len(scratch_specs_list)))\n",
    "ax3.matshow(scratch_spec_avg, interpolation=\"none\", aspect=\"auto\", cmap='viridis', origin=\"lower\")\n",
    "\n",
    "#save it\n",
    "if save:\n",
    "    save_dir = ''\n",
    "    save_name = ('_').join([species,'RF_predicted_labels_20230205.svg'])\n",
    "    plt.savefig(fname=os.path.join(save_dir,save_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82932060",
   "metadata": {},
   "source": [
    "## aggregate labeled vocalizations by pup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6a53047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features to aggregate\n",
    "\n",
    "#path to params (and features) for labeling amplitude segmented vocalizations as cry, scratch or USV\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', '20230116_084254', 'random_forest_20230116_084254_params')\n",
    "\n",
    "#load the training parameters\n",
    "model_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                               save_name=os.path.split(voc_type_params_path)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9b722c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = ''\n",
    "all_predicted_save_name = 'figure2_all_development_vocs_with_predictions.csv'\n",
    "merged_df = pd.read_csv(os.path.join(os.path.join(save_dir, all_predicted_save_name)))\n",
    "\n",
    "#choose the dataset\n",
    "\n",
    "dataset='development'\n",
    "raw_wavs_root = all_wav_development    \n",
    "\n",
    "#get the pups that you want to aggregate features from\n",
    "print('getting pups...')\n",
    "\n",
    "#exclude pups that have recording artefacts\n",
    "pups_to_drop = params_dict['excluded_pups']['development']\n",
    "\n",
    "#get the pups to aggregate \n",
    "merged_df['pup'] = [i.split('.')[0] for i in merged_df['pup']]\n",
    "source_list = [i for i in merged_df['pup'].unique() if i not in pups_to_drop and i.split('_')[0] not in ['MZ', 'MU', 'IS']]\n",
    "\n",
    "#get the features \n",
    "features_to_aggregate = model_params['feature_set']\n",
    "\n",
    "#aggregate the data - this takes a few minutes\n",
    "print('getting aggregate features for each pup...')\n",
    "all_pup_features, all_pup_metadata = features.aggregate_all_pups(source_list = source_list, \n",
    "                                                                 dataset = dataset, \n",
    "                                                                 features = features_to_aggregate, \n",
    "                                                                 features_df = merged_df, \n",
    "                                                                 drop_clipped=True) \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72737abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge metadata and acoustic features\n",
    "assert(all_pup_features['pup'].equals(all_pup_metadata['pup']))\n",
    "vocal_pups = all_pup_features.merge(all_pup_metadata, on='pup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "443d861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the pups that made no detected sounds \n",
    "\n",
    "print('adding nonvocal pups...')\n",
    "#find them\n",
    "non_vocal_pups = [i.split('.')[0] for i in sorted(os.listdir(all_wav_development)) if i.split('.')[0] not in sorted(vocal_pups['pup']) and i.split('_')[0] not in ['MU', 'MZ', 'IS']]\n",
    "\n",
    "#get their metadata\n",
    "non_vocal_meta_data_list = []\n",
    "for pup in non_vocal_pups:\n",
    "    pup_metadata = features.get_pup_metadata(source_path = pup, dataset = 'development')\n",
    "    non_vocal_meta_data_list.append(pup_metadata)\n",
    "non_vocal_meta_data_df = pd.DataFrame(non_vocal_meta_data_list)\n",
    "\n",
    "#make columns for their features (which will be all nan)\n",
    "non_vocal_features_df = pd.DataFrame(columns =all_pup_features.columns)\n",
    "non_vocal_features_df['pup'] = non_vocal_meta_data_df['pup']\n",
    "non_vocal_data_df = non_vocal_features_df.merge(non_vocal_meta_data_df, on='pup')\n",
    "\n",
    "#add to the vocalizing pups\n",
    "assert sorted(vocal_pups.columns) == sorted(non_vocal_data_df.columns)\n",
    "all_pups = pd.concat([vocal_pups, non_vocal_data_df])\n",
    "all_pups = all_pups.sort_values(by='pup').reset_index(drop=True)\n",
    "\n",
    "#make sure you have all the pups now\n",
    "assert len(all_pups) == len([i for i in os.listdir(all_wav_development) if i.split('_')[0] not in ['MU', 'MZ', 'IS']])\n",
    "\n",
    "#get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "print('getting recording lengths...')\n",
    "rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'development_recording_lengths')\n",
    "all_pups['pup'] = [('.').join([i, 'wav']) for i in all_pups['pup']]\n",
    "rec_length_df = pd.DataFrame()\n",
    "rec_length_df['pup'] = rec_length_dict.keys()\n",
    "rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "    \n",
    "#replace count nan with 0 and add some useful columns\n",
    "print('finalizing dataset...')\n",
    "all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "all_pups['removal_flag'] = [int(i[2:]) if 'fr' in i else float('NaN') for i in all_pups['removal_flag']]\n",
    "all_pups['start_temp'] = [float(i.split('_')[7])/10 for i in all_pups['pup']]\n",
    "all_pups['end_temp'] = [float(i.split('_')[8])/10 for i in all_pups['pup']]\n",
    "all_pups['temp_loss'] = all_pups['end_temp'] - all_pups['start_temp']\n",
    "all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bcf42",
   "metadata": {},
   "source": [
    "## get annotated vocalizations for species comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef51fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect the annotations\n",
    "species_list = ['BW', 'BK', 'NB', 'SW', 'PO', 'LO', 'GO', 'LL']\n",
    "annotated_files = []\n",
    "\n",
    "#get the annotations\n",
    "print('getting annotations...')\n",
    "all_combined = annotation.get(annotations_root, species_list)\n",
    "    \n",
    "#get features\n",
    "print('getting features...')\n",
    "features_path = os.path.join(amplitude_acoustic_features,'all_species_warbler_features.csv')\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "#merge keeping only source files that have been annotated\n",
    "print('merging...')\n",
    "annotated_df = features_df.merge(all_combined, how='right',on='source_file').reset_index(drop=True)\n",
    "assert(annotated_df['species_x'].equals(annotated_df['species_y']))\n",
    "annotated_df = annotated_df.rename(columns={'species_x':'species'}).drop(columns=['species_y'])\n",
    "annotated_df['pup'] = [i.split('_clip')[0] for i in annotated_df['source_file']]\n",
    "\n",
    "#add columns for useful info from file names\n",
    "annotated_df['litter'] = [i.split('_')[1]+'_'+i.split('_')[-2] for i in annotated_df['pup']]\n",
    "annotated_df['voc_number'] = [int(i.split('_')[-1].split('.')[0]) for i in annotated_df['source_file']]\n",
    "annotated_df['sex'] = [i.split('_')[6] for i in annotated_df['pup']]\n",
    "annotated_df['start_temp'] = [float(i.split('_')[7])/10 for i in annotated_df['source_file']]\n",
    "annotated_df['end_temp'] = [float(i.split('_')[8])/10 for i in annotated_df['source_file']]\n",
    "annotated_df['temp_loss'] = all_pups['end_temp'] - annotated_df['start_temp']\n",
    "annotated_df['weight_mg'] = [float(i.split('_')[5]) for i in annotated_df['source_file']]\n",
    "annotated_df['age'] = [float(i.split('_')[-5][1:]) if not i.split('_')[-5] == 'nan' else float('NaN') for i in annotated_df['source_file']]\n",
    "annotated_df['removal_flag'] = [float(i.split('_')[-6][-1]) if not i.split('_')[-6] == 'nan' else float('NaN') for i in annotated_df['source_file']]\n",
    "annotated_df['duration'] = [i*1000 for i in annotated_df['duration']]\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "744bc8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are pups are those with a lot fo artefacts (big vertical lines) identified during annotations for das\n",
    "pups_to_drop = params_dict['excluded_pups']['development']\n",
    "\n",
    "#make sure they are dropped (should have happened above)\n",
    "assert len(annotated_df.loc[annotated_df['pup'].isin(pups_to_drop)]) == 0\n",
    "print(len(annotated_df))\n",
    "\n",
    "#drop clipped vocalizations\n",
    "print('dropping clipped vocalizations...')\n",
    "clipping_df = pd.read_csv(clipping_path)\n",
    "annotated_df_not_clipped = annotated_df.merge(clipping_df, how='left', on='source_file')\n",
    "annotated_df_not_clipped = annotated_df_not_clipped.loc[annotated_df_not_clipped['percent_clipped']==0]\n",
    "print(len(annotated_df_not_clipped))\n",
    "\n",
    "# drop forcibly removed pups\n",
    "print('dropping forcibly removed pups...')\n",
    "ds_df = annotated_df_not_clipped.loc[annotated_df_not_clipped['removal_flag']==0]\n",
    "ds_df['temp_loss'] = ds_df['end_temp'] - ds_df['start_temp']\n",
    "print(len(ds_df))\n",
    "all_pups_for_plotting = all_pups.loc[all_pups['removal_flag']==0]\n",
    "\n",
    "assert sum(all_pups_for_plotting['removal_flag']) == 0\n",
    "assert sum(ds_df['removal_flag']) == 0\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2550101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "save = False\n",
    "\n",
    "save_dir = ''\n",
    "vocs_save_name = 'figure2CD_vocs_data.csv'\n",
    "pups_save_name = 'figure2CD_pups_data.csv'\n",
    "\n",
    "if save:\n",
    "    ds_df.to_csv(os.path.join(save_dir, vocs_save_name), index=False)\n",
    "    all_pups_for_plotting.to_csv(os.path.join(save_dir, pups_save_name), index=False)\n",
    "    all_pups.to_csv(os.path.join(save_dir, 'all_pups_unfiltered.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5c504",
   "metadata": {},
   "source": [
    "## plot counts and features (Figure 2 C/D )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca7e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2CD_vocs_data_path = \"\"\n",
    "fig2CD_pups_data_path = \"\"\n",
    "\n",
    "#get the data\n",
    "fig2CD_vocs_data = pd.read_csv(fig2CD_vocs_data_path)\n",
    "fig2CD_pups_data = pd.read_csv(fig2CD_pups_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7f0f4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "order = ['BK', 'NB', 'SW', 'BW', 'PO', 'LO', 'GO', 'LL']\n",
    "\n",
    "fontsize = 9\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "\n",
    "rate_dots_jitter = 0.1\n",
    "rate_dots_size = 2\n",
    "rate_dots_alpha = 0.5\n",
    "\n",
    "feature_dots_jitter = 0.05\n",
    "feature_dots_size = 1\n",
    "feature_dots_alpha = .25\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "#get ages\n",
    "cry_df = fig2CD_vocs_data.loc[fig2CD_vocs_data['human_label'] == 'cry']\n",
    "USV_df = fig2CD_vocs_data.loc[fig2CD_vocs_data['human_label'] == 'USV']\n",
    "all_pups_for_plotting = fig2CD_pups_data\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig, axes = plt.subplot_mosaic(mosaic=\"ACBBDD;EGFFHH;IKJJLL\", \n",
    "                               figsize=[8,4], \n",
    "                               constrained_layout=True, \n",
    "                               dpi=600)\n",
    "\n",
    "# # get the colors\n",
    "# colors = pd.Series(data['species'].unique()).map(species_color_dict)\n",
    "\n",
    "#define the colors\n",
    "species_color_dict = {'BW':'steelblue',\n",
    "                      'NB': 'dodgerblue',\n",
    "                      'BK': 'darkblue',\n",
    "                      'SW': 'blue',\n",
    "                      'PO': 'orange',\n",
    "                      'LO': 'gold',\n",
    "                      'GO': 'green',\n",
    "                      'LL': 'forestgreen',\n",
    "                      'MU': 'mediumspringgreen', \n",
    "                      'MZ': 'turquoise'}\n",
    "\n",
    "\n",
    "# to iterate through\n",
    "features_to_plot = ['duration', 'meanfreq']\n",
    "cry_axes = ['A', 'E']\n",
    "whis_axes = ['C', 'G']\n",
    "\n",
    "#plot the cry acoustic features_to_plot at p9\n",
    "for cry_axis, feature in zip(cry_axes, features_to_plot):\n",
    "    \n",
    "    #remove outliers\n",
    "    data = cry_df.loc[cry_df['age'] == 9]\n",
    "    \n",
    "    #cry dots\n",
    "    box = sns.stripplot(ax=axes[cry_axis], \n",
    "          data=data,\n",
    "          x='species',\n",
    "          jitter=feature_dots_jitter,\n",
    "          y= feature, \n",
    "          color='black', \n",
    "          order=order, \n",
    "          dodge=True, \n",
    "          s=feature_dots_size, \n",
    "          alpha = feature_dots_alpha)\n",
    "\n",
    "    #cry boxes\n",
    "    box = sns.violinplot(ax=axes[cry_axis], \n",
    "                data=data,\n",
    "                x='species', \n",
    "                y=feature, \n",
    "                order=order, \n",
    "                linewidth=0.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5, \n",
    "                inner='quartile')\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    if feature == 'duration':\n",
    "        axes[cry_axis].set_ylabel(ylabel = 'duration (ms)', fontsize = fontsize)\n",
    "    elif feature == 'meanfreq':\n",
    "        axes[cry_axis].set_ylabel(ylabel = 'mean freq. (kHz)', fontsize = fontsize)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[cry_axis].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axes[cry_axis].get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    axes[cry_axis].set_xticks([], [])\n",
    "    \n",
    "    axes[cry_axis].get_xaxis().set_visible(False)\n",
    "    axes[cry_axis].yaxis.set_tick_params(width=.5, rotation = 0, length=ytick_length, pad = ytick_pad)\n",
    "    axes[cry_axis].xaxis.set_tick_params(width=.5, rotation = 0, length=ytick_length, pad = ytick_pad)\n",
    "    if feature == 'meanfreq':\n",
    "        axes[cry_axis].set_ylim([0,100])\n",
    "        axes[cry_axis].set_yticks([0,25,50,75,100])\n",
    "    if feature == 'duration':\n",
    "        axes[cry_axis].set_ylim([0,400])\n",
    "        axes[cry_axis].set_yticks([0,100,200,300,400])\n",
    "    sns.despine()\n",
    "        \n",
    "# plot the USV acoustic features_to_plot at p9\n",
    "for USV_axis, feature in zip(whis_axes, features_to_plot):\n",
    "        \n",
    "    #remove outliers\n",
    "    data = USV_df.loc[USV_df['age'] == 9]\n",
    "\n",
    "    #USV boxes\n",
    "    box = sns.violinplot(ax=axes[USV_axis], \n",
    "                data=data,\n",
    "                x='species', \n",
    "                y=feature, \n",
    "                order=order, \n",
    "                linewidth=0.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5, \n",
    "                inner='quartile')\n",
    "\n",
    "    #USV dots\n",
    "    box = sns.stripplot(ax=axes[USV_axis], \n",
    "          data=data,\n",
    "          x='species', \n",
    "          jitter=feature_dots_jitter,\n",
    "          y=feature, \n",
    "          color='black', \n",
    "          order=order, \n",
    "          dodge=True, \n",
    "          s=feature_dots_size, \n",
    "          alpha = feature_dots_alpha)\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[USV_axis].set_ylabel('')\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[USV_axis].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axes[USV_axis].get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    axes[USV_axis].set_xticks([], [])\n",
    "    axes[USV_axis].get_xaxis().set_visible(False)\n",
    "    axes[USV_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].set_yticks([0,25,50,75,100])\n",
    "    if feature == 'meanfreq':\n",
    "        axes[USV_axis].set_ylim([0,100])\n",
    "    if feature == 'duration':\n",
    "        axes[USV_axis].set_ylim([0,100])\n",
    "    sns.despine()\n",
    "        \n",
    "#plot the cry rates at p9\n",
    "cry_axes = ['I']\n",
    "whis_axes = ['K']\n",
    "feeatures = ['_per_sec']\n",
    "\n",
    "for cry_axis in cry_axes:\n",
    "    \n",
    "    data = all_pups_for_plotting.loc[all_pups_for_plotting['age'] == 9]\n",
    "\n",
    "    #cry dots\n",
    "    box = sns.stripplot(ax=axes[cry_axis], \n",
    "          data=data,\n",
    "          jitter = rate_dots_jitter,\n",
    "          x='species', \n",
    "          y= 'cry_per_sec', \n",
    "          color='black', \n",
    "          order=order, \n",
    "          dodge=True, \n",
    "          s=rate_dots_size, \n",
    "          alpha = rate_dots_alpha)\n",
    "\n",
    "    #cry boxes\n",
    "    box = sns.boxplot(ax=axes[cry_axis], \n",
    "                data=data,\n",
    "                x='species', \n",
    "                y='cry_per_sec', \n",
    "                order=order, \n",
    "                linewidth=.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5, \n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[cry_axis].set_ylabel(ylabel = 'rate (vocs/s)', fontsize=fontsize)\n",
    "    axes[cry_axis].set_xlabel(xlabel = 'species (age 9 days)', fontsize=fontsize)\n",
    "    axes[cry_axis].set_yticks([0,.5,1,1.5,2])\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[cry_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[cry_axis].get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "    for label in (axes[cry_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    axes[cry_axis].set_yticks([0,0.5,1,1.5, 2])\n",
    "    axes[cry_axis].yaxis.set_tick_params(length = ytick_length, width=.5, rotation = 0, pad=ytick_pad)\n",
    "    axes[cry_axis].xaxis.set_tick_params(length = ytick_length, width=.5, rotation = 0, pad=ytick_pad)\n",
    "    axes[cry_axis].set_ylim([0,2])\n",
    "    sns.despine()\n",
    "       \n",
    "        \n",
    "#plot the USV rates at p9        \n",
    "for USV_axis in whis_axes:\n",
    "        \n",
    "    data = all_pups_for_plotting.loc[all_pups_for_plotting['age'] == 9]\n",
    "\n",
    "    #USV boxes\n",
    "    box = sns.boxplot(ax=axes[USV_axis], \n",
    "                data=data,\n",
    "                x='species', \n",
    "                y='USV_per_sec', \n",
    "                order=order, \n",
    "                linewidth=.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5, \n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "\n",
    "    #USV dots\n",
    "    box = sns.stripplot(ax=axes[USV_axis], \n",
    "          data=data,\n",
    "          jitter = rate_dots_jitter,\n",
    "          x='species', \n",
    "          y='USV_per_sec',\n",
    "          color='black', \n",
    "          order=order, \n",
    "          dodge=True, \n",
    "          s=rate_dots_size, \n",
    "          alpha = rate_dots_alpha)\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[USV_axis].set_ylabel('')\n",
    "    axes[USV_axis].set_xlabel(xlabel = 'species (age 9 days)', fontsize=fontsize)\n",
    "    axes[USV_axis].set_yticks([0,.5,1,1.5,2])\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[USV_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[USV_axis].get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "    for label in (axes[USV_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    #axes[USV_axis].set_xticks([], [])\n",
    "    axes[USV_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].set_ylim([0,2])\n",
    "    sns.despine()\n",
    "\n",
    "\n",
    "#add time courses    \n",
    "bwpo_cry_axes = ['B', 'F', 'J']\n",
    "bwpo_USV_axes = ['D', 'H', 'L']\n",
    "\n",
    "#get data for BW and PO\n",
    "cry_bwpo_df = cry_df.loc[(cry_df['species'] == 'BW' ) | (cry_df['species'] == 'PO' )]\n",
    "USV_bwpo_df = USV_df.loc[(USV_df['species'] == 'BW' ) | (USV_df['species'] == 'PO' )]\n",
    "\n",
    "#plot the BW_PO time course for the cry acoustic features_to_plot\n",
    "for cry_axis, feature in zip(bwpo_cry_axes, features_to_plot):\n",
    "    \n",
    "    #cry dots\n",
    "    data = cry_bwpo_df\n",
    "\n",
    "    box = sns.stripplot(ax=axes[cry_axis], \n",
    "          data=data,\n",
    "          jitter=feature_dots_jitter,\n",
    "          x='age', \n",
    "          y=feature, \n",
    "          hue='species', \n",
    "          color='black', \n",
    "          dodge=True, \n",
    "          s=feature_dots_size, \n",
    "          alpha = feature_dots_alpha)\n",
    "\n",
    "    #cry boxes\n",
    "    box = sns.violinplot(ax=axes[cry_axis], \n",
    "                data=data,\n",
    "                x='age', \n",
    "                y=feature, \n",
    "                dodge=True, \n",
    "                hue='species',\n",
    "                linewidth=0.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5, \n",
    "                inner='quartile')\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[cry_axis].set_ylabel('')\n",
    "    axes[cry_axis].set_xlabel('')\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[cry_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[cry_axis].get_yticklabels()+axes[cry_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    #axes[cry_axis].set_xticks([], [])\n",
    "    axes[cry_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[cry_axis].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    if feature == 'meanfreq':\n",
    "        axes[cry_axis].set_ylim([0,100])\n",
    "        axes[cry_axis].set_yticks([0,25,50,75,100])\n",
    "    if feature == 'duration':\n",
    "        axes[cry_axis].set_ylim([0,400])\n",
    "        axes[cry_axis].set_yticks([0,100,200,300,400])\n",
    "    sns.despine()\n",
    "\n",
    "#plot the BW_PO time course for the USV acoustic features_to_plot       \n",
    "for USV_axis, feature in zip(bwpo_USV_axes, features_to_plot):\n",
    "    \n",
    "    data = USV_bwpo_df.loc[~USV_bwpo_df['age'].isin([1, 3])] #drop ages where PO makes no wbistls\n",
    "    \n",
    "    #USV dots\n",
    "    box = sns.stripplot(ax=axes[USV_axis], \n",
    "          data=data,\n",
    "          jitter=feature_dots_jitter,\n",
    "          x='age', \n",
    "          y=feature, \n",
    "          hue='species', \n",
    "          color='black', \n",
    "          dodge=True, \n",
    "          s=feature_dots_size, \n",
    "          alpha = feature_dots_alpha)\n",
    "\n",
    "    #USV boxes\n",
    "    box = sns.violinplot(ax=axes[USV_axis], \n",
    "                      data=data,\n",
    "                      x='age', \n",
    "                      y=feature,  \n",
    "                      dodge=True, \n",
    "                      hue='species',\n",
    "                      linewidth=0.5, \n",
    "                      palette=species_color_dict, \n",
    "                      saturation=.5, \n",
    "                      inner='quartile')\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[USV_axis].set_ylabel('')\n",
    "    axes[USV_axis].set_xlabel('')\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[USV_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[USV_axis].get_yticklabels() + axes[USV_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    #axes[USV_axis].set_xticks([], [])\n",
    "    axes[USV_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].set_yticks([0,25,50,75,100])\n",
    "    if feature == 'meanfreq':\n",
    "        axes[USV_axis].set_ylim([0,100])\n",
    "    if feature == 'duration':\n",
    "        axes[USV_axis].set_ylim([0,100])\n",
    "    sns.despine()\n",
    "    \n",
    "#plot the rates for the BW PO time course \n",
    "bwpo_cry_axes = ['J']\n",
    "bwpo_USV_axes = ['L']\n",
    "\n",
    "#get data for BW and PO\n",
    "bwpo_df = all_pups_for_plotting.loc[(all_pups_for_plotting['species'] == 'BW' ) | (all_pups_for_plotting['species'] == 'PO' )]\n",
    "colors = bwpo_df['species'].map(species_color_dict)\n",
    "\n",
    "#plot the rates for the BW PO time course cries\n",
    "for cry_axis in bwpo_cry_axes:\n",
    "    #cry dots\n",
    "    \n",
    "    data = bwpo_df\n",
    "    \n",
    "    #cry dots\n",
    "    box = sns.stripplot(ax=axes[cry_axis], \n",
    "          data=data,\n",
    "          jitter = rate_dots_jitter,\n",
    "          x='age', \n",
    "          y='cry_per_sec', \n",
    "          hue='species', \n",
    "          color='grey', \n",
    "          dodge=True, \n",
    "          s=rate_dots_size, \n",
    "          alpha = rate_dots_alpha)\n",
    "\n",
    "    #cry boxes\n",
    "    box = sns.boxplot(ax=axes[cry_axis], \n",
    "                data=data,\n",
    "                x='age', \n",
    "                y='cry_per_sec', \n",
    "                dodge=True, \n",
    "                hue='species',\n",
    "                linewidth=.5, \n",
    "                palette=species_color_dict, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[cry_axis].set_ylabel('')\n",
    "    axes[cry_axis].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[cry_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[cry_axis].get_yticklabels() + axes[cry_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "    \n",
    "    axes[cry_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[cry_axis].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[cry_axis].set_yticks([0,.5,1,1.5,2])\n",
    "    axes[cry_axis].spines.bottom.set_visible(False)\n",
    "    axes[cry_axis].set_ylim([0,2])\n",
    "    sns.despine()\n",
    "        \n",
    "        \n",
    "#plot the rates for the BW PO time course USVs\n",
    "for USV_axis, feature in zip(bwpo_USV_axes, features_to_plot):\n",
    "    \n",
    "    data = bwpo_df\n",
    "    \n",
    "    #USV dots\n",
    "    box = sns.stripplot(ax=axes[USV_axis], \n",
    "          data=data,\n",
    "          jitter = rate_dots_jitter,\n",
    "          x='age', \n",
    "          y='USV_per_sec', \n",
    "          hue='species', \n",
    "          color='grey', \n",
    "          dodge=True, \n",
    "          s=rate_dots_size, \n",
    "          alpha = rate_dots_alpha)\n",
    "\n",
    "    #USV boxes\n",
    "    box = sns.boxplot(ax=axes[USV_axis], \n",
    "                      data=data,\n",
    "                      x='age', \n",
    "                      y='USV_per_sec',  \n",
    "                      dodge=True, \n",
    "                      hue='species',\n",
    "                      linewidth=.5, \n",
    "                      palette=species_color_dict, \n",
    "                      saturation=.5, \n",
    "                      whis=1.5, \n",
    "                      showfliers = False, \n",
    "                      flierprops={\"marker\": \"\"},\n",
    "                      medianprops={\"color\": \"black\"},\n",
    "                      boxprops={\"linewidth\": .00001}, \n",
    "                      width = .75, \n",
    "                      showcaps=False)\n",
    "    \n",
    "\n",
    "    #prettify\n",
    "    box.legend([],[], frameon=False)\n",
    "    axes[USV_axis].set_ylabel('')\n",
    "    axes[USV_axis].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[USV_axis].spines[axis].set_linewidth(.5)\n",
    "    for label in (axes[USV_axis].get_yticklabels() + axes[USV_axis].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    axes[USV_axis].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[USV_axis].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    axes[USV_axis].spines.bottom.set_visible(False)\n",
    "    #axes[USV_axis].set_xticks([], [])\n",
    "    axes[USV_axis].set_yticks([0,.5,1,1.5,2])\n",
    "    axes[USV_axis].set_ylim([0,2])\n",
    "    sns.despine()\n",
    "    \n",
    "if save == True:\n",
    "    save_dir = ''\n",
    "    save_name = 'figure2CD_no_clipping_no_forcibly-removed'\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.svg')\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.jpeg', dpi=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a74bd",
   "metadata": {},
   "source": [
    "### statistics (use R kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63d27b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "library(lme4)\n",
    "library(emmeans)\n",
    "library(lmerTest)\n",
    "library(pbkrtest)\n",
    "library(tidyr)\n",
    "\n",
    "#paths to data\n",
    "fig2CD_vocs_data_path <- \"\"\n",
    "fig2CD_pups_data_path <- \"\"\n",
    "\n",
    "#get the data\n",
    "fig2CD_vocs_data <- read.csv(fig2CD_vocs_data_path)\n",
    "fig2CD_pups_data <- read.csv(fig2CD_pups_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4a711",
   "metadata": {},
   "source": [
    "#### all species p9 cry duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "355ab6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_vocs_data, human_label == \"cry\" & age == 9)\n",
    "\n",
    "#scale the data\n",
    "data[[\"duration\"]] <- scale(data[[\"duration\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "#fit model\n",
    "p9cry.duration.lme <- lmer(duration ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(p9cry.duration.lme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d249c",
   "metadata": {},
   "source": [
    "#### all species p9 cry mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "933fa475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_vocs_data, human_label == \"cry\" & age == 9)\n",
    "\n",
    "#scale the data\n",
    "data[[\"meanfreq\"]] <- scale(data[[\"meanfreq\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "#fit model\n",
    "p9cry.meanfreq.lme <- lmer(meanfreq ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(p9cry.meanfreq.lme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df99afc",
   "metadata": {},
   "source": [
    "#### all species p9 cry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fb3b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_pups_data, age == 9)\n",
    "\n",
    "#scale the data\n",
    "data[[\"cry_per_sec\"]] <- scale(data[[\"cry_per_sec\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "#fit model\n",
    "p9cry.cryrate.lme <- lmer(cry_per_sec ~ species + sex + (1|temp_loss), data = data)\n",
    "summary(p9cry.cryrate.lme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9489514",
   "metadata": {},
   "source": [
    "#### all species p9 USV duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b9d77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_vocs_data, human_label == \"USV\" & age == 9)\n",
    "\n",
    "#scale the data\n",
    "data[[\"duration\"]] <- scale(data[[\"duration\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "#fit model\n",
    "p9USV.duration.lme <- lmer(duration ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(p9USV.duration.lme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409829c8",
   "metadata": {},
   "source": [
    "#### all species p9 USV mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16ab3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_vocs_data, human_label == \"USV\" & age == 9)\n",
    "\n",
    "\n",
    "#scale the data\n",
    "data[[\"meanfreq\"]] <- scale(data[[\"meanfreq\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "#fit model\n",
    "p9USV.meanfreq.lme <- lmer(meanfreq ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(p9USV.meanfreq.lme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c46ae",
   "metadata": {},
   "source": [
    "#### all species p9 USV rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06a58e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data\n",
    "data <- subset(fig2CD_pups_data, age == 9)\n",
    "\n",
    "#fit model\n",
    "p9USV.USVrate.lme <- lmer(USV_per_sec ~ species + sex + (1|temp_loss), data = data)\n",
    "summary(p9USV.USVrate.lme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60c716",
   "metadata": {},
   "source": [
    "#### BW and PO all ages cry duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ee45204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_vocs_data, human_label == \"cry\" & (species == \"BW\" | species == \"PO\"))\n",
    "\n",
    "#scale the data\n",
    "data[[\"duration\"]] <- scale(data[[\"duration\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "bwpo.cry.duration.lm <- lmer(duration ~ species + age + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(bwpo.cry.duration.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce445f",
   "metadata": {},
   "source": [
    "#### BW and PO all ages cry mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41709433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_vocs_data, human_label == \"cry\" & (species == \"BW\" | species == \"PO\"))\n",
    "\n",
    "#scale the data\n",
    "data[[\"meanfreq\"]] <- scale(data[[\"meanfreq\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "bwpo.cry.meanfreq.lm <- lmer(meanfreq ~ species + sex + age +  (1|pup) + (1|temp_loss) , data = data)\n",
    "summary(bwpo.cry.meanfreq.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b630d",
   "metadata": {},
   "source": [
    "#### BW and PO all ages cry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99fc57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_pups_data, (species == \"BW\" | species == \"PO\"))\n",
    "\n",
    "bwpo.cry.rate.lm <- lmer(cry_per_sec ~ species + age + sex + (1|temp_loss), data = data)\n",
    "summary(bwpo.cry.rate.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367387ec",
   "metadata": {},
   "source": [
    "#### BW and PO all ages USV duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63696c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_vocs_data, human_label == \"USV\" & (species == \"BW\" | species == \"PO\"))\n",
    "\n",
    "#scale the data\n",
    "data[[\"duration\"]] <- scale(data[[\"duration\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "bwpo.USV.duration.lm <- lmer(duration ~ species + age + sex   + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(bwpo.USV.duration.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ea245",
   "metadata": {},
   "source": [
    "#### BW and PO all ages USV mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06754354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_vocs_data, human_label == \"USV\" & (species == \"BW\" | species == \"PO\"))\n",
    "\n",
    "#scale the data\n",
    "data[[\"meanfreq\"]] <- scale(data[[\"meanfreq\"]], center = TRUE, scale = TRUE)\n",
    "\n",
    "bwpo.USV.meanfreq.lm <- lmer(meanfreq ~ species + age + sex +  (1|pup) + (1|temp_loss), data = data)\n",
    "summary(bwpo.USV.meanfreq.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eefdb1",
   "metadata": {},
   "source": [
    "#### BW and PO all ages USV rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48d0e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = subset(fig2CD_pups_data, (species == \"BW\" | species == \"PO\"))\n",
    "bwpo.USV.rate.lm <- lmer(USV_per_sec ~ species + age + sex + (1|temp_loss), data = data)\n",
    "summary(bwpo.USV.rate.lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbb94b",
   "metadata": {},
   "source": [
    "## PCA on features of annotated vocalizations (Supplemental Figure 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58d5a5",
   "metadata": {},
   "source": [
    "### collect annotated vocalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "296fd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect the annotations\n",
    "species_list = ['BW', 'BK', 'NB', 'SW', 'PO', 'LO', 'GO', 'LL']\n",
    "annotated_files = []\n",
    "\n",
    "#get the annotations\n",
    "print('getting annotations...')\n",
    "all_combined = annotation.get(annotations_root, species_list)\n",
    "    \n",
    "#get features\n",
    "print('getting features...')\n",
    "features_path = os.path.join(amplitude_acoustic_features,'all_species_warbler_features.csv')\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "#merge keeping only source files that have been annotated\n",
    "print('merging...')\n",
    "annotated_df = features_df.merge(all_combined, how='right',on='source_file').reset_index(drop=True)\n",
    "assert(annotated_df['species_x'].equals(annotated_df['species_y']))\n",
    "annotated_df = annotated_df.rename(columns={'species_x':'species'}).drop(columns=['species_y'])\n",
    "annotated_df['pup'] = [i.split('_clip')[0] for i in annotated_df['source_file']]\n",
    "\n",
    "#add columns for useful info from file names\n",
    "annotated_df['litter'] = [i.split('_')[1]+'_'+i.split('_')[-2] for i in annotated_df['pup']]\n",
    "annotated_df['voc_number'] = [int(i.split('_')[-1].split('.')[0]) for i in annotated_df['source_file']]\n",
    "annotated_df['sex'] = [i.split('_')[6] for i in annotated_df['pup']]\n",
    "annotated_df['start_temp'] = [float(i.split('_')[7])/10 for i in annotated_df['source_file']]\n",
    "annotated_df['end_temp'] = [float(i.split('_')[8])/10 for i in annotated_df['source_file']]\n",
    "annotated_df['temp_loss'] = annotated_df['end_temp'] - annotated_df['start_temp']\n",
    "annotated_df['weight_mg'] = [float(i.split('_')[5]) for i in annotated_df['source_file']]\n",
    "annotated_df['age'] = [float(i.split('_')[-5][1:]) if not i.split('_')[-5] == 'nan' else float('NaN') for i in annotated_df['source_file']]\n",
    "annotated_df['removal_flag'] = [float(i.split('_')[-6][-1]) if not i.split('_')[-6] == 'nan' else float('NaN') for i in annotated_df['source_file']]\n",
    "annotated_df['duration'] = [i*1000 for i in annotated_df['duration']]\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76de61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are pups are those with a lot fo artefacts (big vertical lines) identified during annotations for das\n",
    "pups_to_drop = params_dict['excluded_pups']['development']\n",
    "\n",
    "#make sure they are dropped (should have happened above)\n",
    "assert len(annotated_df.loc[annotated_df['pup'].isin(pups_to_drop)]) == 0\n",
    "\n",
    "#drop clipped vocalizations\n",
    "print('dropping clipped vocalizations...')\n",
    "clipping_df = pd.read_csv(clipping_path)\n",
    "annotated_df = annotated_df.merge(clipping_df, how='left', on='source_file')\n",
    "annotated_df = annotated_df.loc[annotated_df['percent_clipped']==0]\n",
    "\n",
    "# drop forcibly removed pups\n",
    "print('dropping forcibly removed pups...')\n",
    "annotated_df = annotated_df.loc[annotated_df['removal_flag']==0]\n",
    "annotated_df['temp_loss'] = annotated_df['end_temp'] - annotated_df['start_temp']\n",
    "\n",
    "assert sum(annotated_df['removal_flag']) == 0\n",
    "assert sum(annotated_df['percent_clipped']) == 0\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd04b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of annotations for each species and label\n",
    "print('total label counts:\\n\\t')\n",
    "print('cry:', len(annotated_df.loc[annotated_df['human_label'] == 'cry']))\n",
    "print('USV:', len(annotated_df.loc[annotated_df['human_label'] == 'USV']))\n",
    "print('scratch:', len(annotated_df.loc[annotated_df['human_label'] == 'scratch']))\n",
    "\n",
    "print('\\n')\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    print('\\tcry:', len(annotated_df.loc[annotated_df['human_label'] == 'cry'].loc[annotated_df['species'] == species]))\n",
    "    print('\\tUSV:', len(annotated_df.loc[annotated_df['human_label'] == 'USV'].loc[annotated_df['species'] == species]))\n",
    "    print('\\tscratch:', len(annotated_df.loc[annotated_df['human_label'] == 'scratch'].loc[annotated_df['species'] == species]))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aea9004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 333333\n",
    "num_to_sample = 500\n",
    "\n",
    "#sample\n",
    "print('sampling...')\n",
    "ds_df = annotation.sample(seed, num_to_sample, annotated_df)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb38d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of annotations for each species and label\n",
    "print('total label counts:\\n\\t')\n",
    "print('cry:', len(ds_df.loc[ds_df['human_label'] == 'cry']))\n",
    "print('USV:', len(ds_df.loc[ds_df['human_label'] == 'USV']))\n",
    "print('scratch:', len(ds_df.loc[ds_df['human_label'] == 'scratch']))\n",
    "\n",
    "print('\\n')\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    print('\\tcry:', len(ds_df.loc[ds_df['human_label'] == 'cry'].loc[ds_df['species'] == species]))\n",
    "    print('\\tUSV:', len(ds_df.loc[ds_df['human_label'] == 'USV'].loc[ds_df['species'] == species]))\n",
    "    print('\\tscratch:', len(ds_df.loc[ds_df['human_label'] == 'scratch'].loc[ds_df['species'] == species]))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4862c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate cries and USVs for separate PCAs\n",
    "cry_df = ds_df.loc[ds_df['human_label'] == 'cry']\n",
    "USV_df = ds_df.loc[ds_df['human_label'] == 'USV']\n",
    "\n",
    "assert set(cry_df['human_label'].unique())== set(['cry'])\n",
    "assert set(USV_df['human_label'].unique()) == set(['USV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a9e8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features for PCA that are \"biologically interesting\", ie that we might be able to map genetically\n",
    "features_for_pca = params_dict['supp_figure_2']['features']\n",
    "cry_df_pca = cry_df[features_for_pca]\n",
    "USV_df_pca = USV_df[features_for_pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c624b",
   "metadata": {},
   "source": [
    "### do the pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36aff396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "n_components = 3\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# Standardize the cry features\n",
    "cry_sf = StandardScaler().fit_transform(cry_df_pca)\n",
    "\n",
    "# Standardize the USV features\n",
    "USV_sf = StandardScaler().fit_transform(USV_df_pca)\n",
    "\n",
    "#cry PCA\n",
    "print('doing PCA on cry features...')\n",
    "cry_pca = PCA(n_components = n_components)\n",
    "cry_pca_embedding = cry_pca.fit_transform(cry_sf)\n",
    "print('\\tdone.')\n",
    "print('\\texplained variance by PC:', cry_pca.explained_variance_ratio_)\n",
    "print('\\ttotal variance explained:', sum(cry_pca.explained_variance_ratio_))\n",
    "\n",
    "#add for plotting\n",
    "cry_df['pc1'] = cry_pca_embedding[:,0]\n",
    "cry_df['pc2'] = cry_pca_embedding[:,1]\n",
    "cry_df['pc3'] = cry_pca_embedding[:,2]\n",
    "\n",
    "print('getting loadings for cry pcs...')\n",
    "cry_loadings = pd.DataFrame(cry_pca.components_.T, columns=['pc1', 'pc2', 'pc3'], index=cry_df_pca.columns)\n",
    "cry_loadings['pc1'] = np.abs(cry_loadings['pc1'])\n",
    "cry_loadings['pc2'] = np.abs(cry_loadings['pc2'])\n",
    "cry_loadings['pc3'] = np.abs(cry_loadings['pc3'])\n",
    "cry_loadings['feature'] = cry_loadings.index\n",
    "cry_loadings_pc1 = pd.DataFrame(cry_loadings.sort_values(by = ['pc1'], ascending=False))\n",
    "cry_loadings_pc2 = pd.DataFrame(cry_loadings.sort_values(by = ['pc2'], ascending=False))\n",
    "cry_loadings_pc3 = pd.DataFrame(cry_loadings.sort_values(by = ['pc3'], ascending=False))\n",
    "print('\\tdone.')\n",
    "\n",
    "\n",
    "#USV PCA\n",
    "print('doing PCA on USV features...')\n",
    "USV_pca = PCA(n_components = n_components)\n",
    "USV_pca_embedding = USV_pca.fit_transform(USV_sf)\n",
    "print('\\tdone.')\n",
    "print('\\texplained variance by PC:', USV_pca.explained_variance_ratio_)\n",
    "print('\\ttotal variance explained:', sum(USV_pca.explained_variance_ratio_))\n",
    "\n",
    "#add for plotting\n",
    "USV_df['pc1'] = USV_pca_embedding[:,0]\n",
    "USV_df['pc2'] = USV_pca_embedding[:,1]\n",
    "USV_df['pc3'] = USV_pca_embedding[:,2]\n",
    "\n",
    "print('getting loadings for USV pcs...')\n",
    "USV_loadings = pd.DataFrame(USV_pca.components_.T, \n",
    "                                columns=['pc1', 'pc2', 'pc3'], \n",
    "                                index=USV_df_pca.columns)\n",
    "\n",
    "USV_loadings['pc1'] = np.abs(USV_loadings['pc1'])\n",
    "USV_loadings['pc2'] = np.abs(USV_loadings['pc2'])\n",
    "USV_loadings['pc3'] = np.abs(USV_loadings['pc3'])\n",
    "USV_loadings['feature'] = USV_loadings.index\n",
    "USV_loadings_pc1 = pd.DataFrame(USV_loadings.sort_values(by = ['pc1'], ascending=False))\n",
    "USV_loadings_pc2 = pd.DataFrame(USV_loadings.sort_values(by = ['pc2'], ascending=False))\n",
    "USV_loadings_pc3 = pd.DataFrame(USV_loadings.sort_values(by = ['pc3'], ascending=False))\n",
    "print('\\tdone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1522e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(cry_df))\n",
    "print(len(USV_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c30e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "save = False\n",
    "\n",
    "vocs_save_dir = ''\n",
    "cry_vocs_save_name = 'supplement_figure2_cry_vocs_data.csv'\n",
    "USV_vocs_save_name = 'supplement_figure2_USV_vocs_data.csv'\n",
    "\n",
    "if save:\n",
    "    cry_df.to_csv(os.path.join(vocs_save_dir, cry_vocs_save_name), index=False)\n",
    "    USV_df.to_csv(os.path.join(vocs_save_dir, USV_vocs_save_name), index=False)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9239a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# plot cry PCS\n",
    "vocs_save_dir = ''\n",
    "cry_vocs_save_name = 'supplement_figure2_cry_vocs_data.csv'\n",
    "USV_vocs_save_name = 'supplement_figure2_USV_vocs_data.csv'\n",
    "cry_df = pd.read_csv(os.path.join(vocs_save_dir, cry_vocs_save_name))\n",
    "USV_df = pd.read_csv(os.path.join(vocs_save_dir, USV_vocs_save_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4236a",
   "metadata": {},
   "source": [
    "### plot the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7b1c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save= False\n",
    "\n",
    "print(len(cry_df))\n",
    "print(len(USV_df))\n",
    "fig, axes = plt.subplot_mosaic(mosaic = \"AAAABBBB;AAAABBBB;IIWWJJXX;KKYYLLZZ;MMEENNFF\", \n",
    "                            figsize=[8.5,8], \n",
    "                            constrained_layout=True, \n",
    "                            dpi=600)\n",
    "\n",
    "violin_dot_size = .5\n",
    "violin_alpha = .3\n",
    "fontsize = 9\n",
    "\n",
    "species_color_dict = params_dict['figure_1_panel_d']['species_color_dict']\n",
    "genus_color_dict = params_dict['figure_1_panel_d']['genus_color_dict']\n",
    "voc_name_color_dict = params_dict['figure_1_panel_d']['voc_name_color_dict']\n",
    "HDBSCAN_color_dict = params_dict['figure_1_panel_d']['HDBSCAN_color_dict']\n",
    "\n",
    "cry_pc_violin_axes = [\"I\", \"K\", \"M\"]\n",
    "pcs = ['pc1', 'pc2', 'pc3']\n",
    "for ax, pc in zip(cry_pc_violin_axes, pcs):\n",
    "    data = cry_df\n",
    "\n",
    "    swarm_plot = sns.stripplot(x='species', \n",
    "                               y=pc, \n",
    "                               data=data, \n",
    "                               alpha = violin_alpha, \n",
    "                               size=violin_dot_size, \n",
    "                               jitter = True,  \n",
    "                               color = 'black',\n",
    "                               dodge = False, \n",
    "                               ax = axes[ax])\n",
    "\n",
    "    swarm_plot = sns.violinplot(x='species', \n",
    "                             y=pc,\n",
    "                             showcaps=False,\n",
    "                             saturation=0.5,\n",
    "                             data=data,\n",
    "                             width = .5, \n",
    "                             linewidth = 0.5, \n",
    "                             inner='quartile',\n",
    "                             dodge = False, \n",
    "                             palette = species_color_dict,\n",
    "                             hue = 'species',  \n",
    "                             ax = axes[ax])\n",
    "\n",
    "    swarm_plot.legend([],[], frameon=False)\n",
    "    \n",
    "    if pc == 'pc1':\n",
    "        axes[ax].set_ylabel(\"PC1 (\"+str(round(cry_pca.explained_variance_ratio_[0], 3)*100)+\") %\", fontsize=9)\n",
    "    elif pc == 'pc2':\n",
    "        axes[ax].set_ylabel(\"PC2 (\"+str(round(cry_pca.explained_variance_ratio_[1], 3)*100)[:4]+\") %\", fontsize=9)\n",
    "    elif pc == 'pc3':\n",
    "        axes[ax].set_ylabel(\"PC3 (\"+str(round(cry_pca.explained_variance_ratio_[2], 3)*100)+\") %\", fontsize=9)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[ax].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    axes[ax].set_xticks([],[])  \n",
    "    axes[ax].xaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    axes[ax].yaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    sns.despine()\n",
    "    \n",
    "#plot USV PCS\n",
    "USV_pc_violin_axes = [\"J\", \"L\", \"N\"]\n",
    "pcs = ['pc1', 'pc2', 'pc3']\n",
    "for ax, pc in zip(USV_pc_violin_axes, pcs):\n",
    "    data = USV_df\n",
    "\n",
    "    swarm_plot = sns.stripplot(x='species', \n",
    "                               y=pc, \n",
    "                               data=data, \n",
    "                               alpha = violin_alpha, \n",
    "                               size= violin_dot_size, \n",
    "                               jitter = True,  \n",
    "                               color = 'black', \n",
    "                               dodge = False, \n",
    "                               ax = axes[ax])\n",
    "\n",
    "    swarm_plot = sns.violinplot(x='species', \n",
    "                             y=pc,\n",
    "                             showcaps=False,\n",
    "                             saturation=0.5,\n",
    "                             data=data,\n",
    "                             width = .5, \n",
    "                             linewidth = 0.5, \n",
    "                             inner='quartile',\n",
    "                             dodge = False, \n",
    "                             palette = species_color_dict,\n",
    "                             hue = 'species',  \n",
    "                             ax = axes[ax])\n",
    "    \n",
    "    if pc == 'pc1':\n",
    "        axes[ax].set_ylabel(\"PC1 (\"+str(round(USV_pca.explained_variance_ratio_[0], 3)*100)[:4]+\") %\", fontsize=9)\n",
    "    elif pc == 'pc2':\n",
    "        axes[ax].set_ylabel(\"PC2 (\"+str(round(USV_pca.explained_variance_ratio_[1], 3)*100)[:4]+\") %\", fontsize=9)\n",
    "    elif pc == 'pc3':\n",
    "        axes[ax].set_ylabel(\"PC3 (\"+str(round(USV_pca.explained_variance_ratio_[2], 3)*100)+\") %\", fontsize=9)\n",
    "\n",
    "    swarm_plot.legend([],[], frameon=False)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[ax].spines[axis].set_linewidth(.5)\n",
    "\n",
    "    axes[ax].set_xticks([],[])    \n",
    "    axes[ax].yaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    sns.despine()\n",
    "    \n",
    "    \n",
    "# plot loadings for cries\n",
    "\n",
    "cry_loadings_axes = [\"W\", \"Y\", \"E\"]\n",
    "cry_pc_dfs = [cry_loadings_pc1, cry_loadings_pc2, cry_loadings_pc3]\n",
    "pcs = ['pc1', 'pc2', 'pc3']\n",
    "\n",
    "for ax,df,pc in zip(cry_loadings_axes, cry_pc_dfs, pcs):\n",
    "    sns.barplot(ax = axes[ax], \n",
    "                x =df['feature'], \n",
    "                y=df[pc], \n",
    "                color = 'grey')\n",
    "    \n",
    "    axes[ax].set_xticks(ticks = range(len(df.index)), labels = df.index, rotation=90, fontsize=9)\n",
    "    axes[ax].set_xlabel('')\n",
    "    axes[ax].set_ylabel('')\n",
    "\n",
    "#plot loadings for USVs\n",
    "USV_loadings_axes = [\"X\", \"Z\", \"F\"]\n",
    "USV_pc_dfs = [USV_loadings_pc1, USV_loadings_pc2, USV_loadings_pc3]\n",
    "pcs = ['pc1', 'pc2', 'pc3']\n",
    "\n",
    "for ax,df,pc in zip(USV_loadings_axes, USV_pc_dfs, pcs):\n",
    "    sns.barplot(ax = axes[ax], \n",
    "                x =df['feature'], \n",
    "                y=df[pc], \n",
    "                color = 'grey')\n",
    "    \n",
    "    axes[ax].set_xticks(ticks = range(len(df.index)), labels = df.index, rotation=90, fontsize=9)\n",
    "    axes[ax].set_xlabel('')\n",
    "    axes[ax].set_ylabel('')\n",
    "    \n",
    "    \n",
    "#plot cry PCA scatter\n",
    "axes[\"A\"].scatter(x=cry_df[\"pc1\"], \n",
    "                  y=cry_df[\"pc2\"], \n",
    "                  c=cry_df['species'].map(species_color_dict), \n",
    "                  s = violin_dot_size+2, \n",
    "                  alpha = violin_alpha \n",
    ")\n",
    "axes[\"B\"].scatter(x=USV_df[\"pc1\"], \n",
    "                  y=USV_df[\"pc2\"], \n",
    "                  c=USV_df['species'].map(species_color_dict),\n",
    "                  s = violin_dot_size+2, \n",
    "                  alpha = violin_alpha \n",
    ")\n",
    "plt.xticks(fontsize=9, rotation=90)\n",
    "axes[\"A\"].set_xlabel(\"PC1 (\"+str(round(cry_pca.explained_variance_ratio_[0], 3)*100)+\") %\", fontsize=9)\n",
    "axes[\"A\"].set_ylabel(\"PC2 (\"+str(round(cry_pca.explained_variance_ratio_[1], 3)*100)[:4]+\") %\", fontsize=9)\n",
    "axes[\"B\"].set_xlabel(\"PC1 (\"+str(round(USV_pca.explained_variance_ratio_[0], 3)*100)[:4]+\") %\", fontsize=9)\n",
    "axes[\"B\"].set_ylabel(\"PC2 (\"+str(round(USV_pca.explained_variance_ratio_[1], 3)*100)+\") %\", fontsize=9)\n",
    "\n",
    "print(len(cry_df))\n",
    "print(len(USV_df))\n",
    "\n",
    "if save:\n",
    "    save_name = '20230207_supplement_figure_2.jpeg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name), dpi=600)\n",
    "    \n",
    "    save_name = '20230207_supplement_figure_2.svg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95053bdf",
   "metadata": {},
   "source": [
    "### statistics (use R kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b83a8",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "634efa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "library(lme4)\n",
    "library(emmeans)\n",
    "library(lmerTest)\n",
    "library(pbkrtest)\n",
    "\n",
    "#paths to data\n",
    "supplement_fig2_cry_vocs_data_path <- \"\"\n",
    "supplement_fig2_USV_vocs_data_path <- \"\"\n",
    "\n",
    "#get the data\n",
    "supplement_fig2_cry_vocs <- read.csv(supplement_fig2_cry_vocs_data_path)\n",
    "supplement_fig2_USV_vocs <- read.csv(supplement_fig2_USV_vocs_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f75a34",
   "metadata": {},
   "source": [
    "#### cry PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89fef164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC1\n",
    "data = supplement_fig2_cry_vocs\n",
    "pc1.cry <- lmer(pc1 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc1.cry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c89e17",
   "metadata": {},
   "source": [
    "#### cry PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9c6114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC2\n",
    "data = supplement_fig2_cry_vocs\n",
    "pc2.cry <- lmer(pc2 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc2.cry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b8e7d",
   "metadata": {},
   "source": [
    "#### cry PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "945663e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC3\n",
    "data = supplement_fig2_cry_vocs\n",
    "pc3.cry <- lmer(pc3 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc3.cry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b71e52",
   "metadata": {},
   "source": [
    "#### USV PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0cd19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC1\n",
    "data = supplement_fig2_USV_vocs\n",
    "pc1.USV <- lmer(pc1 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc1.USV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414cea7",
   "metadata": {},
   "source": [
    "#### USV PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5852d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC2\n",
    "data = supplement_fig2_USV_vocs\n",
    "\n",
    "pc2.USV <- lmer(pc2 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc2.USV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1cfacc",
   "metadata": {},
   "source": [
    "#### USV PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fb7729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PC3\n",
    "data = supplement_fig2_USV_vocs\n",
    "pc3.USV <- lmer(pc3 ~ species + sex + (1|pup) + (1|temp_loss), data = data)\n",
    "summary(pc3.USV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f72e7c",
   "metadata": {},
   "source": [
    "# Figure 4 (cross foster experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da8bb4",
   "metadata": {},
   "source": [
    "## collect the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0058d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths for warbleR features and parameters\n",
    "cf_features_dir = os.path.join(acoustic_features_root, 'bw_po_cf','20230206_050454', '20230206_051147')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf9c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: 0\n",
      "there are 2 rows with nan values.\n",
      "dropping these rows...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "#combine features across conditions into a single dataframe\n",
    "\n",
    "dataset = 'bw_po_cf'\n",
    "\n",
    "#these are the files to combine\n",
    "feature_files = ['BWwarbler_features.csv', \n",
    "                 'POwarbler_features.csv', \n",
    "                 'CF-BWwarbler_features.csv', \n",
    "                 'CF-POwarbler_features.csv']\n",
    "\n",
    "#combine them\n",
    "all_combined = []\n",
    "for file in [i for i in glob.glob(os.path.join(cf_features_dir,'*.csv')) if i.split('/')[-1] in feature_files]:\n",
    "    temp = pd.read_csv(file)\n",
    "    all_combined.append(temp)\n",
    "    \n",
    "#add useful information\n",
    "bw_po_cf_features = pd.concat(all_combined)\n",
    "bw_po_cf_features['species'] = [i.split('_')[0] for i in bw_po_cf_features['source_file']]\n",
    "bw_po_cf_features['dataset'] = dataset\n",
    "bw_po_cf_features['segmentation_iteration'] = cf_features_dir.split('/')[-1]\n",
    "bw_po_cf_features['pup'] = [i.split('_clip')[0] for i in bw_po_cf_features['source_file']]\n",
    "bw_po_cf_features = bw_po_cf_features.reset_index(drop=True)\n",
    "print('duplicates:', bw_po_cf_features.duplicated().sum())\n",
    "\n",
    "#check for NaNs and drop those vocalizations\n",
    "print('there are', len(bw_po_cf_features[bw_po_cf_features.isna().any(axis=1)]), 'rows with nan values.')\n",
    "print('dropping these rows...')\n",
    "bw_po_cf_features = bw_po_cf_features.dropna(axis=0,how='any')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a134e6c",
   "metadata": {},
   "source": [
    "## predict vocalization type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "326cc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the dataset to process --- this is the only line you need to change to label a new dataset\n",
    "dataset = 'bw_po_cf'\n",
    "\n",
    "#model ID\n",
    "model_ID = '20230203_044016'\n",
    "\n",
    "#path to model for labeling amplitude segmented vocalizations as cry, scratch or USV\n",
    "voc_type_model_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_voc_type_model.pkl')\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_params')\n",
    "\n",
    "#load the model\n",
    "voc_type_model = pickle.load(open(voc_type_model_path, 'rb'))\n",
    "\n",
    "#load the training parameters\n",
    "model_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                               save_name=os.path.split(voc_type_params_path)[1])\n",
    "\n",
    "print('predicting', dataset, 'vocalizations')\n",
    "df = bw_po_cf_features\n",
    "\n",
    "#make sure NaNs are dropped (can't predict labels from them)\n",
    "assert len(df[df.isna().any(axis=1)]) == 0\n",
    "\n",
    "#check for duplicates (there should be none)\n",
    "assert df.duplicated().sum() == 0\n",
    "\n",
    "#get the features needed to predict vocalization type\n",
    "for_prediction = df[model_params['feature_set']]\n",
    "\n",
    "#predict vocalization types - takes about 10 minutes \n",
    "print('predicting vocalization labels...')\n",
    "predicted_labels = voc_type_model.predict(for_prediction)\n",
    "assert set(predicted_labels) == set(['cry', 'scratch', 'USV'])\n",
    "assert len(predicted_labels) == len(df)\n",
    "\n",
    "#give some info\n",
    "print(\"total predicted 'cry':\", len([i for i in predicted_labels if i=='cry']))\n",
    "print(\"total predicted 'USV':\", len([i for i in predicted_labels if i=='USV']))\n",
    "print(\"total predicted 'scratch':\", len([i for i in predicted_labels if i=='scratch']))\n",
    "\n",
    "#add to the dataframe\n",
    "bw_po_cf_features['predicted_label'] = predicted_labels\n",
    "\n",
    "#drop the non-vocal sound\n",
    "bw_po_cf_features = bw_po_cf_features.loc[bw_po_cf_features['predicted_label'] != 'scratch']\n",
    "assert set(bw_po_cf_features['predicted_label']) == set(['cry', 'USV'])\n",
    "print('done.')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdb930",
   "metadata": {},
   "source": [
    "## aggregate pups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ca1637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pups that you want to aggregate features from\n",
    "\n",
    "#clipping information\n",
    "bw_po_cf_clipping =  pd.read_csv(clipping_path_bw_po_cf)\n",
    "\n",
    "print('getting pups...')\n",
    "#get data\n",
    "df_to_aggregate = bw_po_cf_features.merge(bw_po_cf_clipping, how='left', on='source_file')\n",
    "source_list = [i+'.wav' for i in sorted(bw_po_cf_features['pup'].unique())]\n",
    "assert len(set([len(i.split('_')) for i in source_list])) == 1 \n",
    "\n",
    "#choose the features to aggregate\n",
    "metadata_columns = ['source_file', 'species', 'pup', 'predicted_label', 'dataset', 'segmentation_type', 'segmentation_iteration']\n",
    "features_to_aggregate = model_params['feature_set']\n",
    "\n",
    "#aggregate the data - this takes a few minutes\n",
    "print('getting aggregate features for each pup...')\n",
    "all_pup_features, all_pup_metadata = features.aggregate_all_pups(source_list = source_list, \n",
    "                                                                 dataset = 'bw_po_cf', \n",
    "                                                                 features = features_to_aggregate, \n",
    "                                                                 features_df = df_to_aggregate, \n",
    "                                                                 drop_clipped=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28a13a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge metadata and acoustic features\n",
    "assert(all_pup_features['pup'].equals(all_pup_metadata['pup']))\n",
    "vocal_pups = all_pup_features.merge(all_pup_metadata, on='pup')\n",
    "\n",
    "#add the pups that made no detected sounds \n",
    "#find them\n",
    "non_vocal_pups = [i for i in sorted(os.listdir(all_wav_bw_po_cf)) if i not in sorted(vocal_pups['pup']) and not i.startswith('.')]\n",
    "\n",
    "if len(non_vocal_pups) != 0:\n",
    "    print('adding nonvocal pups...')\n",
    "    \n",
    "    #get their metadata\n",
    "    non_vocal_meta_data_list = []\n",
    "    for pup in non_vocal_pups:\n",
    "        pup_metadata = features.get_pup_metadata(source_path = pup, dataset = dataset)\n",
    "        non_vocal_meta_data_list.append(pup_metadata)\n",
    "    non_vocal_meta_data_df = pd.DataFrame(non_vocal_meta_data_list)\n",
    "\n",
    "    #make columns for their features (which will be all nan)\n",
    "    non_vocal_features_df = pd.DataFrame(columns =all_pup_features.columns)\n",
    "    non_vocal_features_df['pup'] = non_vocal_meta_data_df['pup']\n",
    "    non_vocal_data_df = non_vocal_features_df.merge(non_vocal_meta_data_df, on='pup')\n",
    "\n",
    "    #add to the vocalizing pups\n",
    "    assert sorted(vocal_pups.columns) == sorted(non_vocal_data_df.columns)\n",
    "    all_pups = pd.concat([vocal_pups, non_vocal_data_df])\n",
    "    all_pups = all_pups.sort_values(by='pup').reset_index(drop=True)\n",
    "\n",
    "    #make sure you have all the pups now\n",
    "    #assert len(all_pups) == len([i for i in os.listdir(raw_wavs_root) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_cf_recording_lengths')\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "\n",
    "    print('done.')\n",
    "    \n",
    "else:\n",
    "    print('no non-vocal pups to add...')\n",
    "    all_pups = vocal_pups.copy()\n",
    "    \n",
    "    #make sure you have all the pups now\n",
    "    #assert len(all_pups) == len([i for i in os.listdir(all_wav_bw_po_cf) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_cf_recording_lengths')\n",
    "#     rec_length_dict = {}\n",
    "#     for pup in all_pups['pup'].unique():\n",
    "#         path = os.path.join(all_wav_bw_po_cf, pup)\n",
    "#         fs, wav = wavfile.read(path)\n",
    "#         length = len(wav)/fs\n",
    "#         rec_length_dict[pup] = length\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "    all_pups = all_pups.drop(columns = [i for i in all_pups.columns if 'scratch' in i]) #ignore columns about non-vocal sound\n",
    "    to_drop = params_dict['excluded_pups']['bw_po_cf']\n",
    "    all_pups = all_pups.loc[~all_pups['pup'].isin(to_drop)]\n",
    "    print('done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd622d",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b02fd7",
   "metadata": {},
   "source": [
    "### preprocess for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af7f0af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "\n",
    "#features for PCA\n",
    "cry_features = params_dict['figure_4']['cry_features']\n",
    "USV_features = params_dict['figure_4']['USV_features']\n",
    "\n",
    "#pups to exclude \n",
    "to_drop = params_dict['excluded_pups']['bw_po_cf']\n",
    "all_pups_pca = all_pups.copy()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "#cry PCA preprocessing\n",
    "cry_df_filtered = all_pups_pca[cry_features]\n",
    "cry_df_filtered = cry_df_filtered.dropna()\n",
    "cry_df_filtered = cry_df_filtered.loc[cry_df_filtered['cry_count'] > 1]\n",
    "cry_df_pca = cry_df_filtered.drop(columns=['species', 'pup', 'cry_count'])\n",
    "cry_standardized_features = StandardScaler().fit_transform(cry_df_pca)\n",
    "\n",
    "#USV PCA preprocessing\n",
    "USV_df_filtered = all_pups_pca[USV_features]\n",
    "USV_df_filtered = USV_df_filtered.dropna()\n",
    "USV_df_filtered = USV_df_filtered.loc[USV_df_filtered['USV_count'] > 1]\n",
    "USV_df_pca = USV_df_filtered.drop(columns=['species', 'pup', 'USV_count'])\n",
    "USV_standardized_features = StandardScaler().fit_transform(USV_df_pca)\n",
    "\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e5398",
   "metadata": {},
   "source": [
    "### do the  PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e32d08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cry PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#make a projection objects and connect them to the data\n",
    "cry_pca = PCA(n_components = 2)\n",
    "cry_pca_embedding = cry_pca.fit_transform(cry_standardized_features)\n",
    "print('explained variance by PC:', cry_pca.explained_variance_ratio_)\n",
    "print('total variance explained:', sum(cry_pca.explained_variance_ratio_))\n",
    "\n",
    "loadings = pd.DataFrame(cry_pca.components_.T, columns=['pc1', 'pc2'], index=cry_df_pca.columns)\n",
    "loadings['pc1'] = np.abs(loadings['pc1'])\n",
    "loadings['pc2'] = np.abs(loadings['pc2'])\n",
    "\n",
    "loadings = loadings.sort_values(by = ['pc1'], ascending=False)\n",
    "print(len(loadings))\n",
    "print(loadings)\n",
    "\n",
    "# USV PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#make a projection objects and connect them to the data\n",
    "USV_pca = PCA(n_components = 2)\n",
    "USV_pca_embedding = USV_pca.fit_transform(USV_standardized_features)\n",
    "print('explained variance by PC:', USV_pca.explained_variance_ratio_)\n",
    "print('total variance explained:', sum(USV_pca.explained_variance_ratio_))\n",
    "\n",
    "loadings = pd.DataFrame(USV_pca.components_.T, columns=['pc1', 'pc2'], index=USV_df_pca.columns)\n",
    "loadings['pc1'] = np.abs(loadings['pc1'])\n",
    "loadings['pc2'] = np.abs(loadings['pc2'])\n",
    "\n",
    "loadings = loadings.sort_values(by = ['pc1'], ascending=False)\n",
    "print(len(loadings))\n",
    "print(loadings)\n",
    "\n",
    "USV_df_filtered['pc1'] = USV_pca_embedding[:, 0]\n",
    "USV_df_filtered['pc2'] = USV_pca_embedding[:, 1]\n",
    "\n",
    "cry_df_filtered['pc1'] = cry_pca_embedding[:, 0]\n",
    "cry_df_filtered['pc2'] = cry_pca_embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d71dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "\n",
    "save = False\n",
    "\n",
    "bw_po_cf_pups_save_dir = ''\n",
    "bw_po_cf_pups_save_name = 'figure4_pups_data.csv'\n",
    "cry_pca_save_name = 'figure4_pups_cry_pca.csv'\n",
    "USV_pca_save_name = 'figure4_pups_USV_pca.csv'\n",
    "\n",
    "if save:\n",
    "    all_pups.to_csv(os.path.join(bw_po_cf_pups_save_dir, bw_po_cf_pups_save_name), index=False)\n",
    "    cry_df_filtered.to_csv(os.path.join(bw_po_cf_pups_save_dir, cry_pca_save_name), index=False)\n",
    "    USV_df_filtered.to_csv(os.path.join(bw_po_cf_pups_save_dir, USV_pca_save_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d60e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "bw_po_cf_pups_save_dir = ''\n",
    "bw_po_cf_pups_save_name = 'figure4_pups_data.csv'\n",
    "cry_pca_save_name = 'figure4_pups_cry_pca.csv'\n",
    "USV_pca_save_name = 'figure4_pups_USV_pca.csv'\n",
    "\n",
    "all_pups = pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, bw_po_cf_pups_save_name))\n",
    "cry_df_filtered = pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, cry_pca_save_name))\n",
    "USV_df_filtered = pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, USV_pca_save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031a0ff",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9a8c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_color_dict = {'BW':'steelblue', \n",
    "                 'CF-BW':'lightblue',\n",
    "                 'PO': 'orange',\n",
    "                 'CF-PO': 'wheat'\n",
    "                }\n",
    "\n",
    "#choose to save or not\n",
    "save = False\n",
    "\n",
    "\n",
    "fontsize = 11\n",
    "dot_alpha = 0.7\n",
    "dot_size = 3\n",
    "all_pups=pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, bw_po_cf_pups_save_name))\n",
    "cry_df_filtered=pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, cry_pca_save_name))\n",
    "USV_df_filtered=pd.read_csv(os.path.join(bw_po_cf_pups_save_dir, USV_pca_save_name))\n",
    "\n",
    "#convert to ms\n",
    "all_pups['cry_duration_med'] = all_pups['cry_duration_med']*1000\n",
    "all_pups['USV_duration_med'] = all_pups['USV_duration_med']*1000\n",
    "\n",
    "features_to_plot = ['_per_sec', \n",
    "            '_meanfreq_med', \n",
    "             '_duration_med']\n",
    "\n",
    "voc_types = ['cry', 'USV']\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig, ax = plt.subplots(nrows=2,\n",
    "                       ncols=4,\n",
    "                       figsize=[12,8], \n",
    "                       sharex = False, \n",
    "                       constrained_layout = False)\n",
    "\n",
    "# get the colors\n",
    "colors = pd.Series(all_pups['species'].unique()).map(cf_color_dict)\n",
    "\n",
    "#plot the box plots\n",
    "#for each row\n",
    "for i, voc_type in zip(range(ax.shape[0]), voc_types):\n",
    "    \n",
    "    #plot each column\n",
    "    for j, feature in zip(range(ax.shape[1]), features_to_plot):\n",
    "\n",
    "        if 'per_sec' in feature:\n",
    "            data = all_pups\n",
    "        else:\n",
    "            data = all_pups\n",
    "            \n",
    "        ax[0,j].set_title(feature)\n",
    "        \n",
    "        swarm_plot = sns.stripplot(x='species', \n",
    "                                   y=voc_type+feature, \n",
    "                                   data=data, \n",
    "                                   alpha = dot_alpha, \n",
    "                                   size=dot_size, \n",
    "                                   jitter = True,  \n",
    "                                   color = 'black', \n",
    "                                   dodge = False, \n",
    "                                   hue = 'species', \n",
    "                                   ax = ax[i,j])\n",
    "        \n",
    "        swarm_plot = sns.boxplot(x='species', \n",
    "                                 y=voc_type+feature,\n",
    "                                 showcaps=False,\n",
    "                                 saturation=0.5,\n",
    "                                 data=data, \n",
    "                                 flierprops={\"marker\": \"\"},\n",
    "                                 medianprops={\"color\": \"black\"},\n",
    "                                 whis=1.5,\n",
    "                                 width = .4, \n",
    "                                 linewidth = .5, \n",
    "                                 boxprops={\"linewidth\": .00001},\n",
    "                                 dodge = False, \n",
    "                                 palette = colors,\n",
    "                                 hue = 'species',  \n",
    "                                 ax = ax[i,j])\n",
    "        \n",
    "        #prettify\n",
    "        swarm_plot.legend([],[], frameon=False)\n",
    "        ax[i,j].set_ylabel('')\n",
    "        \n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax[i,j].spines[axis].set_linewidth(.5)\n",
    "    \n",
    "        ax[i,j].xaxis.set_tick_params(width=.5)\n",
    "        ax[i,j].yaxis.set_tick_params(width=.5)\n",
    "        \n",
    "        sns.despine()\n",
    "        \n",
    "        \n",
    "#plot the PCs\n",
    "\n",
    "#cry PCA\n",
    "\n",
    "cry_pca_colors = cry_df_filtered['species'].map(cf_color_dict)\n",
    "USV_pca_colors = USV_df_filtered['species'].map(cf_color_dict)\n",
    "\n",
    "#cry PCA\n",
    "ax[0,3].scatter(cry_df_filtered['pc1'], \n",
    "                cry_df_filtered['pc2'],\n",
    "                c= cry_pca_colors,\n",
    "                linewidth = 0,\n",
    "                alpha = .8, \n",
    "                s=50)\n",
    "\n",
    "#USV PCA\n",
    "ax[1,3].scatter(USV_df_filtered['pc1'], \n",
    "                USV_df_filtered['pc2'],\n",
    "                c= USV_pca_colors,\n",
    "                linewidth = 0,\n",
    "                alpha = .8, \n",
    "                s=50)\n",
    "\n",
    "#prettify\n",
    "swarm_plot.legend([],[], frameon=False)\n",
    "ax[0,3].set_ylabel('')\n",
    "ax[1,3].set_ylabel('')\n",
    "\n",
    "ax[0,3].set_ylabel('')\n",
    "ax[1,3].set_ylabel('')\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax[0,3].spines[axis].set_linewidth(.5)\n",
    "    ax[1,3].spines[axis].set_linewidth(.5)\n",
    "\n",
    "for label in (ax[i,j].get_yticklabels() + ax[i,j].get_xticklabels()):\n",
    "    label.set_fontname('Arial')\n",
    "    label.set_fontsize(fontsize)\n",
    "            \n",
    "    \n",
    "ax[0,3].xaxis.set_tick_params(width=.5)\n",
    "ax[0,3].yaxis.set_tick_params(width=.5)\n",
    "ax[0,3].set_yticks([10, 5, 0, -5, -10])\n",
    "ax[1,3].xaxis.set_tick_params(width=.5)\n",
    "ax[1,3].yaxis.set_tick_params(width=.5)\n",
    "ax[1,3].set_yticks([10, 5, 0, -5, -10])\n",
    "sns.despine()\n",
    "\n",
    "if save:\n",
    "    save_dir = ''\n",
    "    save_name = 'figure_4_all_panels.svg'\n",
    "    plt.savefig(os.path.join(save_dir, save_name))\n",
    "    plt.savefig(os.path.join(save_dir, save_name+'.jpeg'), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "caa728be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('cry rate sample size:')\n",
    "print('\\tBW:', len(all_pups['cry_per_sec'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['cry_per_sec'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['cry_per_sec'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['cry_per_sec'].loc[all_pups['species'] == 'PO'].dropna()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4ac543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' cry mean freq size:')\n",
    "print('\\tBW:', len(all_pups['cry_meanfreq_med'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['cry_meanfreq_med'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['cry_meanfreq_med'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['cry_meanfreq_med'].loc[all_pups['species'] == 'PO'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d64c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' cry duration size:')\n",
    "print('\\tBW:', len(all_pups['cry_duration_med'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['cry_duration_med'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['cry_duration_med'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['cry_duration_med'].loc[all_pups['species'] == 'PO'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94161075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV rate sample size:')\n",
    "print('\\tBW:', len(all_pups['USV_per_sec'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['USV_per_sec'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['USV_per_sec'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['USV_per_sec'].loc[all_pups['species'] == 'PO'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6cfd5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV mean freq sample size:')\n",
    "print('\\tBW:', len(all_pups['USV_meanfreq_med'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['USV_meanfreq_med'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['USV_meanfreq_med'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['USV_meanfreq_med'].loc[all_pups['species'] == 'PO'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "950f6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV duration sample size:')\n",
    "print('\\tBW:', len(all_pups['USV_duration_med'].loc[all_pups['species'] == 'BW'].dropna()))\n",
    "print('\\tCF-BW:', len(all_pups['USV_duration_med'].loc[all_pups['species'] == 'CF-BW'].dropna()))\n",
    "print('\\tCF-PO:', len(all_pups['USV_duration_med'].loc[all_pups['species'] == 'CF-PO'].dropna()))\n",
    "print('\\tPO:', len(all_pups['USV_duration_med'].loc[all_pups['species'] == 'PO'].dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace66f83",
   "metadata": {},
   "source": [
    "## statistics (use R kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f5532",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09a9e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "library(lme4)\n",
    "library(emmeans)\n",
    "library(lmerTest)\n",
    "library(pbkrtest)\n",
    "\n",
    "#paths to data\n",
    "fig4_pups_data_path <- \"\"\n",
    "\n",
    "#get the data\n",
    "fig4_pups_data <- read.csv(fig4_pups_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b02126",
   "metadata": {},
   "source": [
    "### cry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "225b619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry.rate.one.way <- aov(cry_per_sec ~ species , data = fig4_pups_data)\n",
    "summary(cry.rate.one.way)\n",
    "cry.rate.tukey.one.way <- TukeyHSD(cry.rate.one.way)\n",
    "cry.rate.tukey.one.way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30780416",
   "metadata": {},
   "source": [
    "### cry mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1642226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry.meanfreq.one.way <- aov(cry_meanfreq_med ~ species , data = fig4_pups_data)\n",
    "summary(cry.meanfreq.one.way)\n",
    "TukeyHSD(cry.meanfreq.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b44ac",
   "metadata": {},
   "source": [
    "### cry duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08e5daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry.duration.one.way <- aov(cry_duration_med ~ species , data = fig4_pups_data)\n",
    "summary(cry.duration.one.way)\n",
    "TukeyHSD(cry.duration.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc77ccd",
   "metadata": {},
   "source": [
    "### USV rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f41be674",
   "metadata": {},
   "outputs": [],
   "source": [
    "USV.rate.one.way <- aov(USV_per_sec ~ species , data = fig4_pups_data)\n",
    "summary(USV.rate.one.way)\n",
    "USV.rate.tukey.one.way <- TukeyHSD(USV.rate.one.way)\n",
    "USV.rate.tukey.one.way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad4ba2",
   "metadata": {},
   "source": [
    "### USV mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57b0b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "USV.meanfreq.one.way <- aov(USV_meanfreq_med ~ species , data = fig4_pups_data)\n",
    "summary(USV.meanfreq.one.way)\n",
    "TukeyHSD(USV.meanfreq.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fede3f",
   "metadata": {},
   "source": [
    "### USV duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b53bfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "USV.duration.one.way <- aov(USV_duration_med ~ species , data = fig4_pups_data)\n",
    "summary(USV.duration.one.way)\n",
    "TukeyHSD(USV.duration.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bee0e",
   "metadata": {},
   "source": [
    "# Figure 5 (F1 and F2 experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d275d3e",
   "metadata": {},
   "source": [
    "## collect raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772e6e3",
   "metadata": {},
   "source": [
    "### collect the F1 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5350fa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: 0\n",
      "there are 2 rows with nan values.\n",
      "dropping these rows...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "#combine features across conditions into a single dataframe\n",
    "f1_features_dir = os.path.join(acoustic_features_root, 'bw_po_f1','20220926_073134')\n",
    "\n",
    "#these are the files to combine\n",
    "feature_files = ['BW-PO-cross-F1warbler_features.csv', 'cross-BWwarbler_features.csv', 'cross-POwarbler_features.csv']\n",
    "\n",
    "#combine them\n",
    "all_combined = []\n",
    "for file in [i for i in glob.glob(os.path.join(f1_features_dir,'*.csv')) if i.split('/')[-1] in feature_files]:\n",
    "    temp = pd.read_csv(file)\n",
    "    all_combined.append(temp)\n",
    "    \n",
    "#add useful information\n",
    "bw_po_f1_features = pd.concat(all_combined)\n",
    "bw_po_f1_features['species'] = [i.split('_')[0] for i in bw_po_f1_features['source_file']]\n",
    "bw_po_f1_features['dataset'] = 'bw_po_f1'\n",
    "bw_po_f1_features['segmentation_iteration'] = f1_features_dir.split('/')[-1]\n",
    "bw_po_f1_features['pup'] = [i.split('_clip')[0] for i in bw_po_f1_features['source_file']]\n",
    "bw_po_f1_features = bw_po_f1_features.reset_index(drop=True)\n",
    "print('duplicates:', bw_po_f1_features.duplicated().sum())\n",
    "\n",
    "#check for NaNs and drop those vocalizations\n",
    "print('there are', len(bw_po_f1_features[bw_po_f1_features.isna().any(axis=1)]), 'rows with nan values.')\n",
    "print('dropping these rows...')\n",
    "bw_po_f1_features = bw_po_f1_features.dropna(axis=0,how='any')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d597e4f",
   "metadata": {},
   "source": [
    "### prediction vocalization types from F1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50b4077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the dataset to process --- this is the only line you need to change to label a new dataset\n",
    "dataset = 'bw_po_f1'\n",
    "\n",
    "#model ID\n",
    "model_ID = '20230203_044016'\n",
    "\n",
    "#path to model for labeling amplitude segmented vocalizations as cry, scratch or USV\n",
    "voc_type_model_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_voc_type_model.pkl')\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_params')\n",
    "\n",
    "#load the model\n",
    "voc_type_model = pickle.load(open(voc_type_model_path, 'rb'))\n",
    "\n",
    "#load the training parameters\n",
    "model_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                               save_name=os.path.split(voc_type_params_path)[1])\n",
    "\n",
    "print('predicting', dataset, 'vocalizations')\n",
    "df = bw_po_f1_features\n",
    "\n",
    "#make sure NaNs are dropped (can't predict labels from them)\n",
    "assert len(df[df.isna().any(axis=1)]) == 0\n",
    "\n",
    "#check for duplicates (there should be none)\n",
    "assert df.duplicated().sum() == 0\n",
    "\n",
    "#get the features needed to predict vocalization type\n",
    "for_prediction = df[model_params['feature_set']]\n",
    "\n",
    "#predict vocalization types - takes about 10 minutes \n",
    "print('predicting vocalization labels...')\n",
    "predicted_labels = voc_type_model.predict(for_prediction)\n",
    "assert set(predicted_labels) == set(['cry', 'scratch', 'USV'])\n",
    "assert len(predicted_labels) == len(df)\n",
    "\n",
    "#give some info\n",
    "print(\"total predicted 'cry':\", len([i for i in predicted_labels if i=='cry']))\n",
    "print(\"total predicted 'USV':\", len([i for i in predicted_labels if i=='USV']))\n",
    "print(\"total predicted 'scratch':\", len([i for i in predicted_labels if i=='scratch']))\n",
    "\n",
    "#add to the dataframe\n",
    "bw_po_f1_features['predicted_label'] = predicted_labels\n",
    "\n",
    "#drop the non-vocal sound\n",
    "bw_po_f1_features = bw_po_f1_features.loc[bw_po_f1_features['predicted_label'] != 'scratch']\n",
    "assert set(bw_po_f1_features['predicted_label'].unique()) == set(['cry', 'USV'])\n",
    "\n",
    "print('done.')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe5a97",
   "metadata": {},
   "source": [
    "### aggregate the F1 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a5c8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add clipping data and get the pups that you want to aggregate features from\n",
    "bw_po_f1_clipping =  pd.read_csv(clipping_path_bw_po_f1)\n",
    "df_to_aggregate = bw_po_f1_features.merge(bw_po_f1_clipping, how='left', on='source_file')\n",
    "\n",
    "print('getting pups...')\n",
    "source_list = [i+'.wav' for i in sorted(bw_po_f1_features['pup'].unique())]\n",
    "\n",
    "#choose the features to aggregate\n",
    "metadata_columns = ['source_file', 'species', 'pup', 'predicted_label', 'dataset', 'segmentation_type', 'segmentation_iteration']\n",
    "features_to_aggregate = model_params['feature_set']\n",
    "\n",
    "#aggregate the data - this takes a few minutes\n",
    "print('getting aggregate features for each pup...')\n",
    "all_pup_features, all_pup_metadata = features.aggregate_all_pups(source_list = source_list, \n",
    "                                                                 dataset = 'bw_po_f1', \n",
    "                                                                 features = features_to_aggregate, \n",
    "                                                                 features_df = df_to_aggregate, \n",
    "                                                                 drop_clipped=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1df07d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge metadata and acoustic features\n",
    "assert(all_pup_features['pup'].equals(all_pup_metadata['pup']))\n",
    "vocal_pups = all_pup_features.merge(all_pup_metadata, on='pup')\n",
    "\n",
    "#add the pups that made no detected sounds \n",
    "#find them\n",
    "non_vocal_pups = [i for i in sorted(os.listdir(all_wav_bw_po_f1)) if i not in sorted(vocal_pups['pup']) and not i.startswith('.')]\n",
    "\n",
    "if len(non_vocal_pups) != 0:\n",
    "    print('adding nonvocal pups...')\n",
    "    \n",
    "    #get their metadata\n",
    "    non_vocal_meta_data_list = []\n",
    "    for pup in non_vocal_pups:\n",
    "        pup_metadata = features.get_pup_metadata(source_path = pup, dataset = dataset)\n",
    "        non_vocal_meta_data_list.append(pup_metadata)\n",
    "    non_vocal_meta_data_df = pd.DataFrame(non_vocal_meta_data_list)\n",
    "\n",
    "    #make columns for their features (which will be all nan)\n",
    "    non_vocal_features_df = pd.DataFrame(columns =all_pup_features.columns)\n",
    "    non_vocal_features_df['pup'] = non_vocal_meta_data_df['pup']\n",
    "    non_vocal_data_df = non_vocal_features_df.merge(non_vocal_meta_data_df, on='pup')\n",
    "\n",
    "    #add to the vocalizing pups\n",
    "    assert sorted(vocal_pups.columns) == sorted(non_vocal_data_df.columns)\n",
    "    all_pups = pd.concat([vocal_pups, non_vocal_data_df])\n",
    "    all_pups = all_pups.sort_values(by='pup').reset_index(drop=True)\n",
    "\n",
    "    #make sure you have all the pups now\n",
    "    assert len(all_pups) == len([i for i in os.listdir(raw_wavs_root) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_f1_recording_lengths')\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['removal_flag'] = [int(i[2:]) if 'fr' in i else float('NaN') for i in all_pups['removal_flag']]\n",
    "    all_pups['start_temp'] = [float(i.split('_')[7])/10 for i in all_pups['pup']]\n",
    "    all_pups['end_temp'] = [float(i.split('_')[8])/10 for i in all_pups['pup']]\n",
    "    all_pups['temp_loss'] = all_pups['end_temp'] - all_pups['start_temp']\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "    f1_df = all_pups.drop(columns = [i for i in all_pups.columns if 'scratch' in i]) #ignore columns about non-vocal sound\n",
    "    \n",
    "else:\n",
    "    print('no non-vocal pups to add...')\n",
    "    all_pups = vocal_pups.copy()\n",
    "    \n",
    "    #make sure you have all the pups now\n",
    "    assert len(all_pups) == len([i for i in os.listdir(all_wav_bw_po_f1) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_f1_recording_lengths')\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['removal_flag'] = [int(i[2:]) if 'fr' in i else float('NaN') for i in all_pups['removal_flag']]\n",
    "    all_pups['start_temp'] = [float(i.split('_')[7])/10 for i in all_pups['pup']]\n",
    "    all_pups['end_temp'] = [float(i.split('_')[8])/10 for i in all_pups['pup']]\n",
    "    all_pups['temp_loss'] = all_pups['end_temp'] - all_pups['start_temp']\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "    f1_df = all_pups.drop(columns = [i for i in all_pups.columns if 'scratch' in i]) #ignore columns about non-vocal sound\n",
    "\n",
    "    print('done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a679f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df.to_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e53058",
   "metadata": {},
   "source": [
    "### collect the F2 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee4e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_features_dir = os.path.join(acoustic_features_root, 'bw_po_f2','20220926_101549')\n",
    "\n",
    "#these are the files to combine\n",
    "feature_files = ['ch1warbler_features.csv', \n",
    "                 'ch2warbler_features.csv', \n",
    "                 'ch3warbler_features.csv', \n",
    "                 'ch4warbler_features.csv', \n",
    "                 'ch5warbler_features.csv', \n",
    "                 'ch6warbler_features.csv', \n",
    "                 'ch7warbler_features.csv', \n",
    "                 'ch8warbler_features.cs']\n",
    "\n",
    "#combine them\n",
    "all_combined = []\n",
    "for file in [i for i in glob.glob(os.path.join(f2_features_dir,'*.csv')) if i.split('/')[-1] in feature_files]:\n",
    "    temp = pd.read_csv(file)\n",
    "    all_combined.append(temp)\n",
    "    \n",
    "#add useful information\n",
    "bw_po_f2_features = pd.concat(all_combined)\n",
    "bw_po_f2_features['species'] = [i.split('_')[0] for i in bw_po_f2_features['source_file']]\n",
    "bw_po_f2_features['dataset'] = 'bw_po_f2'\n",
    "bw_po_f2_features['segmentation_iteration'] = f2_features_dir.split('/')[-1]\n",
    "bw_po_f2_features['pup'] = [i.split('_clip')[0] for i in bw_po_f2_features['source_file']]\n",
    "bw_po_f2_features = bw_po_f2_features.reset_index(drop=True)\n",
    "print('duplicates:', bw_po_f2_features.duplicated().sum())\n",
    "\n",
    "#check for NaNs and drop those vocalizations\n",
    "print('there are', len(bw_po_f2_features[bw_po_f2_features.isna().any(axis=1)]), 'rows with nan values.')\n",
    "print('dropping these rows...')\n",
    "bw_po_f2_features = bw_po_f2_features.dropna(axis=0,how='any')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc27556",
   "metadata": {},
   "source": [
    "### predict vocalization types from F2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8844b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#choose the dataset to process --- this is the only line you need to change to label a new dataset\n",
    "dataset = 'bw_po_f2'\n",
    "\n",
    "#model ID\n",
    "model_ID = '20230203_044016'\n",
    "\n",
    "#path to model for labeling amplitude segmented vocalizations as cry, scratch or USV\n",
    "voc_type_model_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_voc_type_model.pkl')\n",
    "voc_type_params_path = os.path.join(models_root,'voc_type_classifiers', model_ID, 'random_forest_'+model_ID+'_params')\n",
    "\n",
    "#load the model\n",
    "voc_type_model = pickle.load(open(voc_type_model_path, 'rb'))\n",
    "\n",
    "#load the training parameters\n",
    "model_params = parameters.load(save_dir=os.path.split(voc_type_params_path)[0], \n",
    "                               save_name=os.path.split(voc_type_params_path)[1])\n",
    "\n",
    "print('predicting', dataset, 'vocalizations')\n",
    "df = bw_po_f2_features\n",
    "\n",
    "#make sure NaNs are dropped (can't predict labels from them)\n",
    "assert len(df[df.isna().any(axis=1)]) == 0\n",
    "\n",
    "#check for duplicates (there should be none)\n",
    "assert df.duplicated().sum() == 0\n",
    "\n",
    "#get the features needed to predict vocalization type\n",
    "for_prediction = df[model_params['feature_set']]\n",
    "\n",
    "#predict vocalization types - takes about 10 minutes \n",
    "print('predicting vocalization labels...')\n",
    "predicted_labels = voc_type_model.predict(for_prediction)\n",
    "assert set(predicted_labels) == set(['cry', 'scratch', 'USV'])\n",
    "assert len(predicted_labels) == len(df)\n",
    "\n",
    "#give some info\n",
    "print(\"total predicted 'cry':\", len([i for i in predicted_labels if i=='cry']))\n",
    "print(\"total predicted 'USV':\", len([i for i in predicted_labels if i=='USV']))\n",
    "print(\"total predicted 'scratch':\", len([i for i in predicted_labels if i=='scratch']))\n",
    "\n",
    "#add to the dataframe\n",
    "bw_po_f2_features['predicted_label'] = predicted_labels\n",
    "\n",
    "#drop the non-vocal sound\n",
    "bw_po_f2_features = bw_po_f2_features.loc[bw_po_f2_features['predicted_label'] != 'scratch']\n",
    "assert(set(bw_po_f2_features['predicted_label'].unique())) == set(['cry', 'USV'])\n",
    "print('done.')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c299b7",
   "metadata": {},
   "source": [
    "### aggregate F2 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pups that you want to aggregate features from\n",
    "#add clipping data and get the pups that you want to aggregate features from\n",
    "bw_po_f2_clipping =  pd.read_csv(clipping_path_bw_po_f2)\n",
    "df_to_aggregate = bw_po_f2_features.merge(bw_po_f2_clipping, how='left', on='source_file')\n",
    "\n",
    "print('getting pups...')\n",
    "source_list = [i+'.wav' for i in sorted(bw_po_f2_features['pup'].unique()) if 'p9' in i] #just get p9 pups\n",
    "\n",
    "#choose the features to aggregate\n",
    "metadata_columns = ['source_file', 'species', 'pup', 'predicted_label', 'dataset', 'segmentation_type', 'segmentation_iteration']\n",
    "features_to_aggregate = model_params['feature_set']\n",
    "\n",
    "#aggregate the data - this takes a few minutes\n",
    "print('getting aggregate features for each pup...')\n",
    "all_pup_features, all_pup_metadata = features.aggregate_all_pups(source_list = source_list, \n",
    "                                                                 dataset = 'bw_po_f2', \n",
    "                                                                 features = features_to_aggregate, \n",
    "                                                                 features_df = df_to_aggregate, \n",
    "                                                                 drop_clipped=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5036c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge metadata and acoustic features\n",
    "assert(all_pup_features['pup'].equals(all_pup_metadata['pup']))\n",
    "vocal_pups = all_pup_features.merge(all_pup_metadata, on='pup')\n",
    "\n",
    "#add the pups that made no detected sounds \n",
    "#find them\n",
    "non_vocal_pups = [i for i in sorted(os.listdir(all_wav_bw_po_f2)) if i not in sorted(vocal_pups['pup']) and not i.startswith('.')]\n",
    "\n",
    "if len(non_vocal_pups) != 0:\n",
    "    print('adding nonvocal pups...')\n",
    "    \n",
    "    #get their metadata\n",
    "    non_vocal_meta_data_list = []\n",
    "    for pup in non_vocal_pups:\n",
    "        pup_metadata = features.get_pup_metadata(source_path = pup, dataset = dataset)\n",
    "        non_vocal_meta_data_list.append(pup_metadata)\n",
    "    non_vocal_meta_data_df = pd.DataFrame(non_vocal_meta_data_list)\n",
    "\n",
    "    #make columns for their features (which will be all nan)\n",
    "    non_vocal_features_df = pd.DataFrame(columns =all_pup_features.columns)\n",
    "    non_vocal_features_df['pup'] = non_vocal_meta_data_df['pup']\n",
    "    non_vocal_data_df = non_vocal_features_df.merge(non_vocal_meta_data_df, on='pup')\n",
    "\n",
    "    #add to the vocalizing pups\n",
    "    assert sorted(vocal_pups.columns) == sorted(non_vocal_data_df.columns)\n",
    "    all_pups = pd.concat([vocal_pups, non_vocal_data_df])\n",
    "    all_pups = all_pups.sort_values(by='pup').reset_index(drop=True)\n",
    "\n",
    "    #make sure you have all the pups now\n",
    "    assert len(all_pups) == len([i for i in os.listdir(all_wav_bw_po_f2) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_f2_recording_lengths')\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['removal_flag'] = [int(i[2:]) if 'fr' in i else float('NaN') for i in all_pups['removal_flag']]\n",
    "    all_pups['start_temp'] = [float(i.split('_')[9])/10 for i in all_pups['pup']]\n",
    "    all_pups['end_temp'] = [float(i.split('_')[10])/10 for i in all_pups['pup']]\n",
    "    all_pups['temp_loss'] = all_pups['end_temp'] - all_pups['start_temp']\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "    f2_df = all_pups.drop(columns = [i for i in all_pups.columns if 'scratch' in i]) #ignore columns about non-vocal sound\n",
    "    f2_df = f2_df.rename(columns={'cross_prefix':'species'}) #so you can color f1 and f2 using columns with the same name\n",
    "    f2_df = f2_df.drop(columns = ['family', 'mic_chanel_prefix']) #so you can concat with f1 and f2 columns\n",
    "    \n",
    "    print('done.')\n",
    "    \n",
    "else:\n",
    "    print('no non-vocal pups to add...')\n",
    "    all_pups = vocal_pups.copy()\n",
    "    \n",
    "    #make sure you have all the pups now\n",
    "    assert len(all_pups) == len([i for i in os.listdir(all_wav_bw_po_f2) if not i.startswith('.')])\n",
    "\n",
    "    #get the recording lengths so you can calculate vocalizations per second - this takes about 10 minutes\n",
    "    print('getting recording lengths...')\n",
    "    rec_length_dict = parameters.load(save_dir = recording_lengths_dir, save_name = 'bw_po_f2_recording_lengths')\n",
    "    rec_length_df = pd.DataFrame()\n",
    "    rec_length_df['pup'] = rec_length_dict.keys()\n",
    "    rec_length_df['recording_length'] = rec_length_dict.values()\n",
    "    rec_length_df['recording_length'] = rec_length_df['recording_length'].astype(float)\n",
    "    all_pups = all_pups.merge(rec_length_df, on='pup')\n",
    "\n",
    "    #replace count nan with 0 and add some useful columns\n",
    "    print('finalizing dataset...')\n",
    "    all_pups['age'] = [int(i[1:]) for i in all_pups['age'] if 'p' in i]\n",
    "    all_pups['removal_flag'] = [int(i[2:]) if 'fr' in i else float('NaN') for i in all_pups['removal_flag']]\n",
    "    all_pups['start_temp'] = [float(i.split('_')[9])/10 for i in all_pups['pup']]\n",
    "    all_pups['end_temp'] = [float(i.split('_')[10])/10 for i in all_pups['pup']]\n",
    "    all_pups['temp_loss'] = all_pups['end_temp'] - all_pups['start_temp']\n",
    "    all_pups['cry_count'] = all_pups['cry_count'].replace(np.nan, 0)\n",
    "    all_pups['USV_count'] = all_pups['USV_count'].replace(np.nan, 0)\n",
    "    all_pups['cry_per_sec'] = all_pups['cry_count']/all_pups['recording_length']\n",
    "    all_pups['USV_per_sec'] = all_pups['USV_count']/all_pups['recording_length']\n",
    "    all_pups['vocs_per_sec'] = all_pups['total_vocalizations_detected']/all_pups['recording_length']\n",
    "    f2_df = all_pups.drop(columns = [i for i in all_pups.columns if 'scratch' in i]) #ignore columns about non-vocal sound\n",
    "    \n",
    "    f2_df = f2_df.rename(columns={'cross_prefix':'species'}) #so you can color f1 and f2 using columns with the same name\n",
    "    f2_df = f2_df.drop(columns = ['family', 'mic_chanel_prefix']) #so you can concat with f1 and f2 columns\n",
    "    print('done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96df6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_df.to_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a79eb6",
   "metadata": {},
   "source": [
    "## PCA of F1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35855f6c",
   "metadata": {},
   "source": [
    "### get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d787a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_df = pd.read_csv('')\n",
    "\n",
    "#get the parameters\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "\n",
    "#features for PCA\n",
    "cry_features = params_dict['figure_5']['cry_features']\n",
    "USV_features = params_dict['figure_5']['USV_features']\n",
    "\n",
    "#pups to exclude\n",
    "to_drop = params_dict['excluded_pups']['bw_po_f1']\n",
    "f1_df = f1_df.loc[~f1_df['pup'].isin(to_drop)]\n",
    "\n",
    "#drop forcibly removed pups\n",
    "f1_df = f1_df.loc[f1_df['removal_flag'] == 0].loc[f1_df['age'] == 9]\n",
    "assert(f1_df['removal_flag'].sum() == 0)\n",
    "assert(set(f1_df['age'].unique()) == set([9]))\n",
    "\n",
    "################################################################################\n",
    "\n",
    "#cry PCA preprocessing\n",
    "cry_df_filtered = f1_df[cry_features]\n",
    "cry_df_filtered= cry_df_filtered.dropna()\n",
    "cry_df_filtered= cry_df_filtered.dropna()\n",
    "cry_df_pca = cry_df_filtered.drop(columns=['species', 'pup', 'cry_count'])\n",
    "cry_standardized_features = StandardScaler().fit_transform(cry_df_pca)\n",
    "\n",
    "#USV PCA preprocessing\n",
    "USV_df_filtered = f1_df[USV_features]\n",
    "USV_df_filtered = USV_df_filtered.dropna()\n",
    "USV_df_filtered= USV_df_filtered.dropna()\n",
    "USV_df_pca = USV_df_filtered.drop(columns=['species', 'pup', 'USV_count'])\n",
    "USV_standardized_features = StandardScaler().fit_transform(USV_df_pca)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebc458",
   "metadata": {},
   "source": [
    "### do the pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ffd5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cry PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#make a projection objects and connect them to the data\n",
    "cry_pca = PCA(n_components = 2)\n",
    "cry_pca_embedding = cry_pca.fit_transform(cry_standardized_features)\n",
    "print('explained variance by PC:', cry_pca.explained_variance_ratio_)\n",
    "print('total variance explained:', sum(cry_pca.explained_variance_ratio_))\n",
    "\n",
    "loadings = pd.DataFrame(cry_pca.components_.T, columns=['pc1', 'pc2'], index=cry_df_pca.columns)\n",
    "loadings['pc1'] = np.abs(loadings['pc1'])\n",
    "loadings['pc2'] = np.abs(loadings['pc2'])\n",
    "\n",
    "loadings = loadings.sort_values(by = ['pc1'], ascending=False)\n",
    "print(len(loadings))\n",
    "print(loadings)\n",
    "\n",
    "# USV PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#make a projection objects and connect them to the data\n",
    "USV_pca = PCA(n_components = 2)\n",
    "USV_pca_embedding = USV_pca.fit_transform(USV_standardized_features)\n",
    "print('explained variance by PC:', USV_pca.explained_variance_ratio_)\n",
    "print('total variance explained:', sum(USV_pca.explained_variance_ratio_))\n",
    "\n",
    "loadings = pd.DataFrame(USV_pca.components_.T, columns=['pc1', 'pc2'], index=USV_df_pca.columns)\n",
    "loadings['pc1'] = np.abs(loadings['pc1'])\n",
    "loadings['pc2'] = np.abs(loadings['pc2'])\n",
    "\n",
    "loadings = loadings.sort_values(by = ['pc1'], ascending=False)\n",
    "print(len(loadings))\n",
    "print(loadings)\n",
    "\n",
    "\n",
    "USV_df_filtered['pc1'] = USV_pca_embedding[:, 0]\n",
    "USV_df_filtered['pc2'] = USV_pca_embedding[:, 1]\n",
    "\n",
    "cry_df_filtered['pc1'] = cry_pca_embedding[:, 0]\n",
    "cry_df_filtered['pc2'] = cry_pca_embedding[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7829",
   "metadata": {},
   "source": [
    "## merge F1 and F2 for plotting and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74994e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes\n",
    "f1_df = pd.read_csv('')\n",
    "f2_df = pd.read_csv('')\n",
    "\n",
    "#drop forcibly removed pups and get p9 pups\n",
    "f1_df_p9 = f1_df.loc[f1_df['age'] == 9].loc[f1_df['removal_flag'] == 0]\n",
    "\n",
    "#drop forcibly removed pups and get p9 pups\n",
    "f2_df_p9 = f2_df.loc[f2_df['age'] == 9].loc[f2_df['removal_flag'] == 0]\n",
    "\n",
    "#concatenate for plotting\n",
    "assert sorted(f1_df_p9.columns) == sorted(f2_df_p9.columns)\n",
    "f1_f2_df_p9 = pd.concat([f1_df_p9, f2_df_p9]).reset_index(drop=True)\n",
    "assert set(f1_f2_df_p9['age'].unique()) == set([9])\n",
    "assert f1_f2_df_p9['removal_flag'].sum() == 0\n",
    "\n",
    "#convert to seconds\n",
    "f1_f2_df_p9['cry_duration_med'] = [i*1000 for i in f1_f2_df_p9['cry_duration_med']]\n",
    "f1_f2_df_p9['USV_duration_med'] = [i*1000 for i in f1_f2_df_p9['USV_duration_med']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b8e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes\n",
    "f1_df = pd.read_csv('')\n",
    "f2_df = pd.read_csv('')\n",
    "\n",
    "#drop forcibly removed pups and get p9 pups\n",
    "f1_df_p9 = f1_df.loc[f1_df['age'] == 9].loc[f1_df['removal_flag'] == 0]\n",
    "\n",
    "#drop forcibly removed pups and get p9 pups\n",
    "f2_df_p9 = f2_df.loc[f2_df['age'] == 9].loc[f2_df['removal_flag'] == 0]\n",
    "\n",
    "#concatenate for plotting\n",
    "assert sorted(f1_df_p9.columns) == sorted(f2_df_p9.columns)\n",
    "f1_f2_df_p9 = pd.concat([f1_df_p9, f2_df_p9]).reset_index(drop=True)\n",
    "#assert f1_f2_df_p9['removal_flag'].sum() == 0\n",
    "assert set(f1_f2_df_p9['age'].unique()) == set([9])\n",
    "assert f1_f2_df_p9['removal_flag'].sum() == 0\n",
    "\n",
    "#convert to seconds\n",
    "f1_f2_df_p9['cry_duration_med'] = [i*1000 for i in f1_f2_df_p9['cry_duration_med']]\n",
    "f1_f2_df_p9['USV_duration_med'] = [i*1000 for i in f1_f2_df_p9['USV_duration_med']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7343b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "save = False\n",
    "\n",
    "f1_f2_save_dir = ''\n",
    "#f1_f2_df_p9_save_name = 'figure5_f1_f2_data_for_plotting.csv'\n",
    "cry_df_filtered_save_name = 'figure5_f1_cry_PCA_data_for_plotting_20230213.csv'\n",
    "USV_df_filtered_save_name = 'figure5_f1_USV_PCA_data_for_plotting_20230213.csv'\n",
    "\n",
    "if save:\n",
    "    #f1_f2_df_p9.to_csv(os.path.join(f1_f2_save_dir, f1_f2_df_p9_save_name ), index=False)\n",
    "    cry_df_filtered.to_csv(os.path.join(f1_f2_save_dir, cry_df_filtered_save_name), index=False) \n",
    "    USV_df_filtered.to_csv(os.path.join(f1_f2_save_dir,USV_df_filtered_save_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6026f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "\n",
    "f1_f2_save_dir = ''\n",
    "f1_f2_df_p9_save_name = 'figure5_f1_f2_data_for_plotting.csv'\n",
    "cry_df_filtered_save_name = 'figure5_f1_cry_PCA_data_for_plotting_20230213.csv'\n",
    "USV_df_filtered_save_name = 'figure5_f1_USV_PCA_data_for_plotting_20230213.csv'\n",
    "\n",
    "data = pd.read_csv(os.path.join(f1_f2_save_dir, f1_f2_df_p9_save_name))\n",
    "cry_df_filtered = pd.read_csv(os.path.join(f1_f2_save_dir, cry_df_filtered_save_name))\n",
    "USV_df_filtered = pd.read_csv(os.path.join(f1_f2_save_dir, USV_df_filtered_save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37a68b",
   "metadata": {},
   "source": [
    "## plot violin and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56480bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all together\n",
    "\n",
    "#choose to save or not\n",
    "save = False\n",
    "\n",
    "#plotting parameters\n",
    "\n",
    "alpha = 0.5\n",
    "jitter = 0.05\n",
    "linewidth = 0\n",
    "s = 2\n",
    "fontsize = 9\n",
    "linewidth = 0.5\n",
    "violinwidth = 0.5\n",
    "\n",
    "pca_s = 10\n",
    "pca_alpha = .6\n",
    "\n",
    "fontsize = 9\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "\n",
    "cross_color_dict = {'cross-BW': 'steelblue', \n",
    "                    'cross-PO': 'orange', \n",
    "                    'BW-PO-cross-F1': 'grey', \n",
    "                    'BWxPO-cross-F2': 'grey'}\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig, ax = plt.subplots(nrows=2,\n",
    "                       ncols=4,\n",
    "                       figsize=[5.5,3.5], \n",
    "                       sharex = False, \n",
    "                       sharey = False, \n",
    "                       constrained_layout=True, \n",
    "                       dpi=600)\n",
    "\n",
    "#set rows (features) columns (genotype) and voc_type\n",
    "features = ['_per_sec', '_meanfreq_med', '_duration_med']\n",
    "voc_types = ['cry', 'USV']\n",
    "\n",
    "#sepaarete tos you can plto separateekyt \n",
    "cry_pca_data_bwpo = cry_df_filtered.loc[~cry_df_filtered['species'].isin(['BW-PO-cross-F1'])]\n",
    "cry_pca_data_f1 = cry_df_filtered.loc[cry_df_filtered['species'].isin(['BW-PO-cross-F1'])]\n",
    "\n",
    "USV_pca_data_bwpo = USV_df_filtered.loc[~USV_df_filtered['species'].isin(['BW-PO-cross-F1'])]\n",
    "USV_pca_data_f1 = USV_df_filtered.loc[USV_df_filtered['species'].isin(['BW-PO-cross-F1'])]\n",
    "\n",
    "# get the colors\n",
    "colors = pd.Series(data['species'].unique()).map(cross_color_dict)\n",
    "\n",
    "#for each row\n",
    "for i, voc_type in zip(range(ax.shape[0]), voc_types):\n",
    "    \n",
    "    #plot each column\n",
    "    for j, feature in zip(range(ax.shape[1]), features):\n",
    "\n",
    "        plot = sns.stripplot(x='species', \n",
    "                            y=voc_type+feature, \n",
    "                            data=data, \n",
    "                            alpha = alpha, \n",
    "                            size= s, \n",
    "                            jitter = jitter,  \n",
    "                            color = 'black', \n",
    "                            dodge = False, \n",
    "                            hue = 'species', \n",
    "                            ax = ax[i,j], order=['cross-BW', 'cross-PO', 'BW-PO-cross-F1', 'BWxPO-cross-F2'])\n",
    "\n",
    "        plot = sns.violinplot(x='species', \n",
    "                              y=voc_type+feature,\n",
    "                              saturation=0.5,\n",
    "                              data=data,       \n",
    "                              width = violinwidth, \n",
    "                              linewidth = linewidth,   \n",
    "                              dodge = False, \n",
    "                              palette = colors,\n",
    "                              hue = 'species',  \n",
    "                              inner = 'quartile',\n",
    "                              ax = ax[i,j], order=['cross-BW', 'cross-PO', 'BW-PO-cross-F1', 'BWxPO-cross-F2'])\n",
    "        \n",
    "        #prettify\n",
    "        plot.legend([],[], frameon=False)\n",
    "        ax[i,j].set_ylabel('')\n",
    "        ax[i,j].set_xlabel('')\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax[i,j].spines[axis].set_linewidth(.5)\n",
    "        for label in (ax[i,j].get_yticklabels() + ax[i,j].get_xticklabels()):\n",
    "            label.set_fontname('Arial')\n",
    "            label.set_fontsize(fontsize)\n",
    "\n",
    "        ax[i,j].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "        ax[i,j].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "        ax[i,j].spines.bottom.set_visible(False)\n",
    "        ax[i,j].set_xticks([], [])\n",
    "        if feature == '_rate':\n",
    "            ax[i,j].set_yticks([0,.5,1,1.5,2])\n",
    "            ax[i,j].set_ylim([0,2])\n",
    "        elif feature == '_meanfreq_med' and voc_type == 'cry':\n",
    "            ax[i,j].set_yticks([20,30,40,50,60, 70])\n",
    "            ax[i,j].set_ylim([20,70])\n",
    "        elif feature == '_meanfreq_med' and voc_type == 'USV':\n",
    "            ax[i,j].set_yticks([40,50,60,70,80, 90])\n",
    "            ax[i,j].set_ylim([40,90])\n",
    "        elif feature == '_duration_med' and voc_type == 'cry':\n",
    "            ax[i,j].set_yticks([0,100,200,300])\n",
    "            ax[i,j].set_ylim([0,300])\n",
    "        elif feature == '_duration_med' and voc_type == 'USV':\n",
    "            ax[i,j].set_yticks([0,20,40,60, 80, 100])\n",
    "            ax[i,j].set_ylim([0,100])\n",
    "#             ax[i,j].set_yticks([0,15,30,45, 60])\n",
    "#             ax[i,j].set_ylim([0,60])\n",
    "        sns.despine()\n",
    "        \n",
    "pca_cross_color_dict = {'cross-BW': 'steelblue', \n",
    "                    'cross-PO': 'orange', \n",
    "                    'BW-PO-cross-F1': 'black'}\n",
    "        \n",
    "        \n",
    "#cry PCA\n",
    "cry_pca_colors = cry_df_filtered['species'].map(pca_cross_color_dict)\n",
    "USV_pca_colors = USV_df_filtered['species'].map(pca_cross_color_dict)       \n",
    "        \n",
    "        \n",
    "#cry PCA - bw po\n",
    "ax[0,3].scatter(cry_pca_data_bwpo['pc1'], \n",
    "                cry_pca_data_bwpo['pc2'],\n",
    "                c= cry_pca_data_bwpo['species'].map(pca_cross_color_dict),\n",
    "                alpha = pca_alpha, \n",
    "                linewidth = linewidth,\n",
    "                s=pca_s)\n",
    "\n",
    "#cry PCA - f1\n",
    "ax[0,3].scatter(cry_pca_data_f1['pc1'], \n",
    "                cry_pca_data_f1['pc2'],\n",
    "                c= cry_pca_data_f1['species'].map(pca_cross_color_dict),\n",
    "                alpha = pca_alpha, \n",
    "                linewidth = linewidth,\n",
    "                s=pca_s)\n",
    "\n",
    "#USV PCA - bw po\n",
    "ax[1,3].scatter(USV_pca_data_bwpo['pc1'], \n",
    "                USV_pca_data_bwpo['pc2'],\n",
    "                c= USV_pca_data_bwpo['species'].map(pca_cross_color_dict),\n",
    "                alpha = pca_alpha, \n",
    "                linewidth = linewidth,\n",
    "                s=pca_s)\n",
    "\n",
    "#USV PCA - f1\n",
    "ax[1,3].scatter(USV_pca_data_f1['pc1'], \n",
    "                USV_pca_data_f1['pc2'],\n",
    "                c= USV_pca_data_f1['species'].map(pca_cross_color_dict),\n",
    "                alpha = pca_alpha, \n",
    "                linewidth = linewidth,\n",
    "                s=pca_s)\n",
    "\n",
    "#prettify\n",
    "#swarm_plot.legend([],[], frameon=False)\n",
    "ax[0,3].set_ylabel('')\n",
    "ax[1,3].set_ylabel('')\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax[0,3].spines[axis].set_linewidth(.5)\n",
    "    ax[1,3].spines[axis].set_linewidth(.5)\n",
    "    \n",
    "for label in (ax[0,3].get_yticklabels() + ax[0,3].get_xticklabels()):\n",
    "            label.set_fontname('Arial')\n",
    "            label.set_fontsize(fontsize)\n",
    "            \n",
    "for label in (ax[1,3].get_yticklabels() + ax[1,3].get_xticklabels()):\n",
    "            label.set_fontname('Arial')\n",
    "            label.set_fontsize(fontsize)\n",
    "    \n",
    "ax[0,3].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "ax[0,3].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "ax[0,3].set_yticks([-10,-5,0,5,10])\n",
    "ax[0,3].set_ylim([-10,10])\n",
    "\n",
    "ax[0,3].set_xticks([-10,-5,0,5,10])\n",
    "ax[0,3].set_xlim([-10,11])\n",
    "\n",
    "ax[1,3].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "ax[1,3].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "ax[1,3].set_yticks([-10,-10,-5,0,5,10, 15])\n",
    "ax[1,3].set_ylim([-10,15])\n",
    "\n",
    "ax[1,3].set_xticks([-10,-5,0,5,10])\n",
    "ax[1,3].set_xlim([-10,10])\n",
    "\n",
    "\n",
    "if save:\n",
    "\n",
    "    save_dir = ''\n",
    "    save_name = 'Figure_5_top_panels_p9_PCA_only_final_post_resubmission_20230213'\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.jpeg', dpi=600)\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.svg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b1d6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cry rate sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['cry_per_sec'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['cry_per_sec'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['cry_per_sec'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['cry_per_sec'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19f2bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cry meanfreq sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['cry_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['cry_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['cry_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['cry_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99fe7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cry duration sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['cry_duration_med'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['cry_duration_med'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['cry_duration_med'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['cry_duration_med'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04359774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV rate sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['USV_per_sec'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['USV_per_sec'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['USV_per_sec'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['USV_per_sec'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6c18c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV meanfrew sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['USV_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['USV_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['USV_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['USV_meanfreq_med'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d8f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('USV duration sample sizes:')\n",
    "print('BW:', len(f1_f2_df_p9['USV_duration_med'].loc[f1_f2_df_p9['species'] == 'cross-BW'].dropna()))\n",
    "print('PO:', len(f1_f2_df_p9['USV_duration_med'].loc[f1_f2_df_p9['species'] == 'cross-PO'].dropna()))\n",
    "print('F1:', len(f1_f2_df_p9['USV_duration_med'].loc[f1_f2_df_p9['species'] == 'BW-PO-cross-F1'].dropna()))\n",
    "print('F2:', len(f1_f2_df_p9['USV_duration_med'].loc[f1_f2_df_p9['species'] == 'BWxPO-cross-F2'].dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2085a3",
   "metadata": {},
   "source": [
    "## plot F2 trait correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b68b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all together\n",
    "\n",
    "#choose to save or not\n",
    "save = False\n",
    "\n",
    "#set the dataframe\n",
    "data = pd.read_csv(os.path.join(f1_f2_save_dir, f1_f2_df_p9_save_name))\n",
    "pca_data = all_pca\n",
    "########################################################################################################################\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2,\n",
    "                       ncols=5, \n",
    "                       figsize=[8,4], \n",
    "                       sharey=False, \n",
    "                       dpi=600)\n",
    "\n",
    "alpha = 0.2\n",
    "linewidth = 0\n",
    "s = 10\n",
    "\n",
    "#get the data\n",
    "parental_data = data.loc[~data['species'].isin(['BW-PO-cross-F1', 'BWxPO-cross-F2'])]\n",
    "F2_data = data.loc[data['species'].isin(['BWxPO-cross-F2'])]\n",
    "\n",
    "parental_colors = parental_data['species'].map(cross_color_dict)\n",
    "F2_colors = F2_data['species'].map(cross_color_dict)\n",
    "\n",
    "#set rows (features_to_plot) columns (genotype) and voc_type\n",
    "features_to_plot = ['per_sec', 'meanfreq_med', 'duration_med', 'pc1', 'pc2']\n",
    "datasets = ['parental', 'F2']\n",
    "\n",
    "#plot\n",
    "for j, feature in zip(range(ax.shape[1]), features_to_plot):\n",
    "    print(j,':', feature)\n",
    "        \n",
    "    ax[0,j].scatter(parental_data['cry_'+feature], \n",
    "                    parental_data['USV_'+feature], \n",
    "                    color = parental_colors,\n",
    "                    s = s)\n",
    "    \n",
    "    sns.regplot(data = parental_data,\n",
    "                x = 'cry_'+feature,\n",
    "                y = 'USV_'+feature,\n",
    "                fit_reg = True,\n",
    "                color = 'grey',\n",
    "                scatter_kws = {'s':0}, \n",
    "                line_kws = {'color':'black'},\n",
    "                ax = ax[0,j]\n",
    "               )\n",
    "\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "            ax[0,j].spines[axis].set_linewidth(.5)\n",
    "\n",
    "    sns.regplot(data = F2_data,\n",
    "                x = 'cry_'+feature,\n",
    "                y = 'USV_'+feature,\n",
    "                fit_reg = True,\n",
    "                ci=95,\n",
    "                color = 'grey',\n",
    "                scatter_kws = {'alpha': alpha, 'linewidth':linewidth, 's':s}, \n",
    "                line_kws = {'color':'black'},\n",
    "                ax = ax[1,j]\n",
    "               )\n",
    "\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "            ax[1,j].spines[axis].set_linewidth(.5)\n",
    "\n",
    "    #prettify\n",
    "    ax[0,j].xaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    ax[0,j].yaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    ax[1,j].xaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    ax[1,j].yaxis.set_tick_params(width=.5, rotation = 90)\n",
    "    sns.despine()\n",
    "\n",
    "    #prettify\n",
    "    plot.legend([],[], frameon=False)\n",
    "    ax[0,j].set_ylabel('')\n",
    "    ax[0,j].set_xlabel('')\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax[0,j].spines[axis].set_linewidth(.5)\n",
    "    for label in (ax[0,j].get_yticklabels() + ax[0,j].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    ax[0,j].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    ax[0,j].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "    ax[1,j].set_ylabel('')\n",
    "    ax[1,j].set_xlabel('')\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax[1,j].spines[axis].set_linewidth(.5)\n",
    "    for label in (ax[1,j].get_yticklabels() + ax[1,j].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "    ax[1,j].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    ax[1,j].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "\n",
    "            \n",
    "if save:\n",
    "\n",
    "    save_dir = ''\n",
    "    save_name = 'Figure_5_bottom_panels'\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.jpeg', dpi=600)\n",
    "    plt.savefig(os.path.join(save_dir, save_name)+'.svg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891a665",
   "metadata": {},
   "source": [
    "## statistics (use R kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f118d2",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "873c4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "library(lme4)\n",
    "library(emmeans)\n",
    "library(lmerTest)\n",
    "library(pbkrtest)\n",
    "library(tidyr)\n",
    "\n",
    "#paths to data\n",
    "f1_f2_df_p9_data_path <- \"\"\n",
    "\n",
    "#get the data\n",
    "f1_f2_df_p9_data <- read.csv(f1_f2_df_p9_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baadefa",
   "metadata": {},
   "source": [
    "### cry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b484a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "\n",
    "cry.rate.one.way <- aov(cry_per_sec ~ species, data = data)\n",
    "summary(cry.rate.one.way)\n",
    "TukeyHSD(cry.rate.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de7701",
   "metadata": {},
   "source": [
    "### cry mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fd333d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "cry.meanfreq.one.way <- aov(cry_meanfreq_med ~ species, data = data)\n",
    "summary(cry.meanfreq.one.way)\n",
    "TukeyHSD(cry.meanfreq.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71024bfc",
   "metadata": {},
   "source": [
    "### cry duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c56275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "cry.duration.one.way <- aov(cry_duration_med ~ species, data = data)\n",
    "summary(cry.duration.one.way)\n",
    "TukeyHSD(cry.duration.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102156f6",
   "metadata": {},
   "source": [
    "### USV rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89ef9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "USV.rate.one.way <- aov(USV_per_sec ~ species, data = data)\n",
    "summary(USV.rate.one.way)\n",
    "TukeyHSD(USV.rate.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b5f8b",
   "metadata": {},
   "source": [
    "### USV mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "769553cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "USV.meanfreq.one.way <- aov(USV_meanfreq_med ~ species, data = data)\n",
    "summary(USV.meanfreq.one.way)\n",
    "TukeyHSD(USV.meanfreq.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d889c7",
   "metadata": {},
   "source": [
    "### USV duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ea95c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species != \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "USV.duration.one.way <- aov(USV_duration_med ~ species, data = data)\n",
    "summary(USV.duration.one.way)\n",
    "TukeyHSD(USV.duration.one.way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da4a4b",
   "metadata": {},
   "source": [
    "### F2 rate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d66fa646",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- subset(f1_f2_df_p9_data, species == \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "cor.test(data$cry_per_sec, data$USV_per_sec, method = \"spearman\", exact=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cc269",
   "metadata": {},
   "source": [
    "### F2 mean frequency correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8af04798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data\n",
    "data <- subset(f1_f2_df_p9_data, species == \"BWxPO-cross-F2\")\n",
    "unique(data$species)\n",
    "#the test\n",
    "cor.test(data$cry_meanfreq_med, data$USV_meanfreq_med, method = \"spearman\", exact=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c43479",
   "metadata": {},
   "source": [
    "### F2 duration correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "493290bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data\n",
    "data <- subset(f1_f2_df_p9_data, species == \"BWxPO-cross-F2\")\n",
    "\n",
    "#the test\n",
    "cor.test(data$cry_duration_med, data$USV_duration_med, method = \"spearman\", exact=FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3106ae5",
   "metadata": {},
   "source": [
    "# Supplemental Figure 1 (colored UMAPs; UMAP examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310b9b4",
   "metadata": {},
   "source": [
    "## color UMAP by acoustic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e35c2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the directories you want and print the embeddings you have available to plot\n",
    "coords_dir = amplitude_umap_HDBSCAN_labeled\n",
    "coords_list = os.listdir(coords_dir)\n",
    "\n",
    "print('coordinates available...')\n",
    "for i in coords_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "877bd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save = False\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "#choose species decide what to save\n",
    "species_list = ['BW', 'BK', 'NB', 'SW', 'PO', 'LO', 'GO', 'LL', 'MU', 'MZ'][0:1]\n",
    "\n",
    "for species in species_list:\n",
    "    print(species)\n",
    "    \n",
    "    save_umap_plot = False\n",
    "    save_labeled_umap_plot = False\n",
    "    save_label_verification = False\n",
    "\n",
    "    #load the coordinates\n",
    "    coords_path = glob.glob(coords_dir+species+'/*labeled.feather')[0]\n",
    "    df_umap = pd.read_feather(coords_path)\n",
    "\n",
    "    #just get the coordinates\n",
    "    df_umap_small = df_umap[['umap1', 'umap2', 'source_file', 'label']]\n",
    "\n",
    "    #merge UMAP coordinates with acoustic features\n",
    "\n",
    "    #the features csv for this species\n",
    "    features = pd.read_csv(glob.glob(os.path.join(amplitude_acoustic_features, species+'*'))[0])\n",
    "\n",
    "    #get the SPL values for this species\n",
    "    spl_df = pd.read_csv(spl_path)\n",
    "    assert(sorted(spl_df['source_file'].loc[spl_df['species'] == species]) == sorted(features['source_file']))\n",
    "    features = features.merge(spl_df, on='source_file')\n",
    "\n",
    "    #merge\n",
    "    df_umap_features = df_umap_small.merge(features, on= 'source_file')\n",
    "    df_umap_features['pup'] = [i.split('_clip')[0] for i in df_umap_features['source_file']]\n",
    "    df_umap_features['pupID'] = pd.factorize(df_umap_features['pup'])[0]\n",
    "    df_umap_features['age'] = [int(i.split('_')[10][1:]) for i in  df_umap_features['pup']]\n",
    "    df_umap_features['duration'] = df_umap_features['duration']*1000\n",
    "\n",
    "    #plot\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "    save_dir = ''\n",
    "\n",
    "\n",
    "    colors = ['pupID', 'duration', 'meanfreq', 'age', 'SPL']\n",
    "    df = df_umap_features\n",
    "    fontsize= 9\n",
    "\n",
    "    fig, axes = plt.subplots(nrows =1, \n",
    "                             ncols = len(colors), \n",
    "                             figsize = [9,1.5], \n",
    "                             constrained_layout=True, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             dpi=600)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "\n",
    "    for color, i in zip(colors, range(len(colors))):\n",
    "\n",
    "        if color == 'pupID':\n",
    "            cmap = 'gist_ncar'\n",
    "        else:\n",
    "            cmap = 'viridis'\n",
    "\n",
    "        divider = make_axes_locatable(axes[i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "    #     axes[i].set_xlim([-17,13])\n",
    "    #     axes[i].set_ylim([-10,20])\n",
    "\n",
    "        im = axes[i].scatter(\n",
    "            df['umap1'],\n",
    "            df['umap2'],\n",
    "            c = df[color],\n",
    "            s = .1,\n",
    "            alpha = .25, \n",
    "            cmap=cmap)\n",
    "\n",
    "        for label in (axes[i].get_yticklabels() + axes[i].get_xticklabels()):\n",
    "            label.set_fontname('Arial')\n",
    "            label.set_fontsize(fontsize)\n",
    "\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            axes[i].spines[axis].set_linewidth(.5)\n",
    "\n",
    "\n",
    "        if color == 'pupID':\n",
    "            bounds = np.linspace(0, np.max(df['pupID']), np.max(df['pupID'])+1)\n",
    "            cmap = cm.get_cmap('gist_ncar')\n",
    "            norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "            cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                         cax=cax, \n",
    "                         orientation='vertical')\n",
    "            cbar.ax.tick_params(labelsize=9) \n",
    "\n",
    "        elif color == 'age':\n",
    "            bounds = np.linspace(0, 13, 7)\n",
    "            cmap = cm.get_cmap('viridis')\n",
    "            norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "            cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                         cax=cax, \n",
    "                         orientation='vertical', \n",
    "                         ticks = [1, 3, 5, 7, 9, 11, 13])\n",
    "            cbar.ax.tick_params(labelsize=9) \n",
    "\n",
    "        elif color == 'SPL':\n",
    "            bounds = np.round(np.linspace(np.min(df[color]), np.max(df[color])), decimals=0)\n",
    "            cmap = cm.get_cmap('viridis')\n",
    "            norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "            cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                         cax=cax, \n",
    "                         orientation='vertical')\n",
    "            cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "\n",
    "        elif color == 'duration':\n",
    "            bounds = np.round(np.linspace(np.min(df[color]), np.max(df[color])), decimals=0)\n",
    "            cmap = cm.get_cmap('viridis')\n",
    "            norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "            cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                         cax=cax, \n",
    "                         orientation='vertical')\n",
    "            cbar.ax.tick_params(labelsize=9) \n",
    "\n",
    "        elif color == 'meanfreq':\n",
    "            bounds = np.round(np.linspace(np.min(df[color]), np.max(df[color])), decimals=0)\n",
    "            cmap = cm.get_cmap('viridis')\n",
    "            norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "            cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                         cax=cax, \n",
    "                         orientation='vertical')\n",
    "            cbar.ax.tick_params(labelsize=9) \n",
    "\n",
    "        axes[i].xaxis.set_tick_params(width=.5)\n",
    "        axes[i].yaxis.set_tick_params(width=.5)\n",
    "        axes[i].yaxis.set_tick_params(width=.5)\n",
    "        axes[i].set_xticks([], [])\n",
    "        axes[i].set_yticks([], [])\n",
    "\n",
    "        sns.despine()\n",
    "\n",
    "    if save:\n",
    "        save_name = ('_').join([species,'colored_umap_with_spl.jpeg'])\n",
    "        plt.savefig(os.path.join(save_dir, save_name))\n",
    "        \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767175db",
   "metadata": {},
   "source": [
    "## get vocalization examples from UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "071e471f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "save=False\n",
    "\n",
    "#load the samlping parameters\n",
    "params_dict = parameters.load(save_dir = params_dict_path, save_name='parameters')\n",
    "\n",
    "if species in ['MU', 'MZ']: \n",
    "    \n",
    "    #set up the axes\n",
    "    fig, axes = plt.subplot_mosaic(mosaic=\"AAAABCDEFG;AAAAHIJKLM;AAAANOPQRS\", \n",
    "                                   figsize=[8,3], \n",
    "                                   layout='tight', \n",
    "                                   dpi=600)\n",
    "\n",
    "    # #parameters\n",
    "    label_color_dict = {1: 'deeppink', 0: 'thistle', -1:'thistle'}\n",
    "    num_freq_bins=128\n",
    "    num_time_bins=128\n",
    "    dot_size = 10\n",
    "    dot_alpha = 1\n",
    "\n",
    "    #plot the umap\n",
    "    axes[\"A\"].scatter(\n",
    "        df_umap['umap1'],\n",
    "        df_umap['umap2'],\n",
    "        c = df_umap['label'].map(label_color_dict),\n",
    "        s = .5,\n",
    "        alpha = .25, \n",
    "        cmap='viridis')\n",
    "\n",
    "    USV_spec_axes = [\"B\", \"C\", \"D\", \"H\", \"I\", \"J\", \"N\", \"O\", \"P\"]\n",
    "    cry_spec_axes = [\"E\", \"F\", \"G\", \"K\", \"L\", \"M\", \"Q\", \"R\", \"S\"]\n",
    "    all_spec_axes  = USV_spec_axes + cry_spec_axes\n",
    "\n",
    "    #remove ticks\n",
    "    axes[\"A\"].set_xticks([], [])\n",
    "    axes[\"A\"].set_yticks([], [])\n",
    "    axes[\"A\"].axis('off')\n",
    "    sns.despine()\n",
    "    for ax in all_spec_axes:\n",
    "        axes[ax].set_xticks([], [])\n",
    "        axes[ax].set_yticks([], [])\n",
    "    fig.subplots_adjust(wspace=0, hspace = 0)\n",
    "\n",
    "    #sample example vocalizations\n",
    "    USV_seed = params_dict['supp_figure_1'][species]['USV_seed']\n",
    "    USV_examples = df_umap.sample(n=18, random_state=USV_seed)\n",
    "\n",
    "    #overlay them on the umap as black dots\n",
    "    axes[\"A\"].scatter(\n",
    "        USV_examples['umap1'],\n",
    "        USV_examples['umap2'],\n",
    "        c = 'black',\n",
    "        s = dot_size,\n",
    "        alpha = dot_alpha, \n",
    "        linewidth=0)\n",
    "    \n",
    "    #label them so you know which spec goes with which axis\n",
    "    USV_examples = USV_examples.sort_values(by='umap2', ascending=False)\n",
    "    USV_examples['plot_axis'] = USV_spec_axes + cry_spec_axes\n",
    "\n",
    "    txt = [1,2,3,4,5,6,7,8,9, 10,11,12,13,14,15,16,17,18]\n",
    "    for txt, umap1, umap2 in zip(txt,USV_examples['umap1'], USV_examples['umap2']):\n",
    "        if txt == 4:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(-10,0), textcoords = 'offset points')\n",
    "        elif txt == 7:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(0,0), textcoords = 'offset points')\n",
    "        else:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(0,0), textcoords = 'offset points')\n",
    "\n",
    "    txt = [1,2,3,4,5,6,7,8,9, 10,11,12,13,14,15,16,17,18]\n",
    "    for ax,label in zip(all_spec_axes, txt):\n",
    "        columns_to_drop = ['source_file', 'umap1', 'umap2', 'label', 'plot_axis']\n",
    "\n",
    "        to_plot = USV_examples.loc[USV_examples['plot_axis'] == ax].drop(columns = columns_to_drop)\n",
    "\n",
    "        img = np.array(to_plot).reshape((num_freq_bins, num_time_bins))\n",
    "        axes[ax].axis('off')\n",
    "        axes[ax].imshow(img, origin = 'lower', extent = (num_freq_bins, 0, num_time_bins, 0 ))\n",
    "\n",
    "        axes[ax].text(118, 15, label, ha=\"center\", va=\"center\", color=\"w\", fontsize=9, fontname='Arial')\n",
    "\n",
    "else:\n",
    "    \n",
    "    #set up the axes\n",
    "    fig, axes = plt.subplot_mosaic(mosaic=\"AAAABCDEFG;AAAAHIJKLM;AAAANOPQRS\", \n",
    "                                   figsize=[8,3], \n",
    "                                   layout='tight', \n",
    "                                   dpi=600)\n",
    "\n",
    "    # #parameters\n",
    "    label_color_dict = {1: 'deeppink', 0: 'thistle', -1:'thistle'}\n",
    "\n",
    "    num_freq_bins=128\n",
    "    num_time_bins=128\n",
    "    dot_size = 10\n",
    "    dot_alpha = 1\n",
    "\n",
    "    #plot the umap\n",
    "    axes[\"A\"].scatter(\n",
    "        df_umap['umap1'],\n",
    "        df_umap['umap2'],\n",
    "        c = df_umap['label'].map(label_color_dict),\n",
    "        s = .5,\n",
    "        alpha = .25, \n",
    "        cmap='viridis')\n",
    "\n",
    "    USV_spec_axes = [\"B\", \"C\", \"D\", \"H\", \"I\", \"J\", \"N\", \"O\", \"P\"]\n",
    "    cry_spec_axes = [\"E\", \"F\", \"G\", \"K\", \"L\", \"M\", \"Q\", \"R\", \"S\"]\n",
    "    all_spec_axes  = USV_spec_axes + cry_spec_axes\n",
    "\n",
    "    #remove ticks\n",
    "    axes[\"A\"].set_xticks([], [])\n",
    "    axes[\"A\"].set_yticks([], [])\n",
    "    axes[\"A\"].axis('off')\n",
    "    sns.despine()\n",
    "    for ax in all_spec_axes:\n",
    "        axes[ax].set_xticks([], [])\n",
    "        axes[ax].set_yticks([], [])\n",
    "    fig.subplots_adjust(wspace=0, hspace = 0)\n",
    "\n",
    "    #sample example vocalizations\n",
    "    USV_seed = params_dict['supp_figure_1'][species]['USV_seed']\n",
    "    USV_examples = df_umap.loc[df_umap['label'] == 0].sample(n=9, random_state=USV_seed)\n",
    "    cry_seed = params_dict['supp_figure_1'][species]['cry_seed']\n",
    "    cry_examples = df_umap.loc[df_umap['label'] == 1].sample(n=9, random_state=cry_seed)\n",
    "\n",
    "    #overlay them on the umap as black dots\n",
    "    axes[\"A\"].scatter(\n",
    "        USV_examples['umap1'],\n",
    "        USV_examples['umap2'],\n",
    "        c = 'black',\n",
    "        s = dot_size,\n",
    "        alpha = dot_alpha, \n",
    "        linewidth=0)\n",
    "\n",
    "    axes[\"A\"].scatter(\n",
    "        cry_examples['umap1'],\n",
    "        cry_examples['umap2'],\n",
    "        c = 'black',\n",
    "        s = dot_size,\n",
    "        alpha = dot_alpha)\n",
    "\n",
    "    #label them so you know which spec goes with which axis\n",
    "    USV_examples = USV_examples.sort_values(by='umap2', ascending=False)\n",
    "    cry_examples = cry_examples.sort_values(by='umap2', ascending=False)\n",
    "    USV_examples['plot_axis'] = USV_spec_axes\n",
    "    cry_examples['plot_axis'] = cry_spec_axes\n",
    "\n",
    "    #annotate the dots with numbers and move some of them so they aren't on top of other numbers\n",
    "    txt = [1,2,3,4,5,6,7,8,9, 10,11,12,13,14,15,16,17,18]\n",
    "    for txt, umap1, umap2 in zip(txt,USV_examples['umap1'], USV_examples['umap2']):\n",
    "\n",
    "        if txt == 4:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(-10,0), textcoords = 'offset points')\n",
    "        elif txt == 7:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(0,0), textcoords = 'offset points')\n",
    "        else:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(0,0), textcoords = 'offset points')\n",
    "\n",
    "    txt = [10,11,12,13,14,15,16,17,18]\n",
    "    for txt, umap1, umap2 in zip(txt,cry_examples['umap1'], cry_examples['umap2']):\n",
    "        if txt == 17:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(-12,0), textcoords = 'offset points')\n",
    "        else:\n",
    "            axes[\"A\"].annotate(text=txt, xy=(umap1,  umap2), xytext=(0,0), textcoords = 'offset points')\n",
    "\n",
    "    #put the USV spectrograms on their axes\n",
    "    txt = [1,2,3,4,5,6,7,8,9]\n",
    "    for ax,label in zip(USV_spec_axes, txt):\n",
    "        \n",
    "        #get the spectrogram\n",
    "        columns_to_drop = ['source_file', 'umap1', 'umap2', 'label', 'plot_axis']\n",
    "        to_plot = USV_examples.loc[USV_examples['plot_axis'] == ax].drop(columns = columns_to_drop)\n",
    "        img = np.array(to_plot).reshape((num_freq_bins, num_time_bins))      \n",
    "        \n",
    "        #plot\n",
    "        axes[ax].axis('off')\n",
    "        axes[ax].imshow(img, origin = 'lower', cmap='viridis', extent = (num_freq_bins, 0, num_time_bins, 0 ))\n",
    "        axes[ax].text(118, 15, label, ha=\"center\", va=\"center\", color=\"w\", fontsize=9, fontname='Arial')\n",
    "\n",
    "    #put the cry spectrograms on their axes\n",
    "    txt = [10,11,12,13,14,15,16,17,18]\n",
    "    for ax, label in zip(cry_spec_axes, txt):\n",
    "        \n",
    "        #get the spectrogram\n",
    "        columns_to_drop = ['source_file', 'umap1', 'umap2', 'label', 'plot_axis']\n",
    "        to_plot = cry_examples.loc[cry_examples['plot_axis'] == ax].drop(columns = columns_to_drop)\n",
    "        img = np.array(to_plot).reshape((num_freq_bins, num_time_bins))\n",
    "        \n",
    "        #plot\n",
    "        axes[ax].axis('off')\n",
    "        axes[ax].imshow(img, origin = 'lower', cmap='viridis', extent = (num_freq_bins, 0, num_time_bins, 0 ))\n",
    "        axes[ax].text(115, 15, label, ha=\"center\", va=\"center\", color=\"w\", fontsize=9, fontname='Arial')\n",
    "\n",
    "if save:\n",
    "    save_dir = ''\n",
    "    save_name = ('_').join([species,'spec_examples_20230125'+'.jpeg'])\n",
    "    plt.savefig(os.path.join(save_dir,save_name),dpi=600)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7104c40",
   "metadata": {},
   "source": [
    "# Supplemental Figure 4 (development time course for all species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50b8fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to the data\n",
    "save_dir = ''\n",
    "vocs_save_name = 'figure2CD_vocs_data.csv'\n",
    "pups_save_name = 'figure2CD_pups_data.csv'\n",
    "\n",
    "#the data\n",
    "development_vocs_df = pd.read_csv(os.path.join(save_dir, vocs_save_name))\n",
    "development_vocs_df['duration'] = development_vocs_df['duration']/1000 #convert to s so you can fit the y-axis\n",
    "development_pups_df = pd.read_csv(os.path.join(save_dir, pups_save_name))\n",
    "development_pups_df['weight_g'] = development_pups_df['weight_mg']/1000\n",
    "development_pups_df['weight_g'] = development_pups_df['weight_mg']/1000\n",
    "development_pups_df['temp_change'] = development_pups_df['temp_loss']*-1\n",
    "development_pups_df['proportion_cries'] = development_pups_df['cry_count']/development_pups_df['total_vocalizations_detected']\n",
    "development_pups_df['proportion_USVs'] = development_pups_df['USV_count']/development_pups_df['total_vocalizations_detected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize for plottting and set color pallets\n",
    "\n",
    "species_voc_df = [\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'BK'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'NB'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'SW'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'BW'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'PO'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'LO'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'GO'],\n",
    "    development_vocs_df.loc[development_vocs_df['species'] == 'LL']]\n",
    "\n",
    "\n",
    "species_df = [\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'BK'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'NB'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'SW'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'BW'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'PO'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'LO'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'GO'],\n",
    "    development_pups_df.loc[development_pups_df['species'] == 'LL']]\n",
    "\n",
    "colors = ['darkblue', \n",
    "          'dodgerblue', \n",
    "          'blue', \n",
    "          'steelblue', \n",
    "          'orange', \n",
    "          'goldenrod', \n",
    "          'green', \n",
    "          'forestgreen']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c1a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "save = False\n",
    "\n",
    "\n",
    "dot_size = 2\n",
    "dot_alpha = .8\n",
    "by_voc_dot_size = .5\n",
    "by_voc_dot_alpha = .8\n",
    "num_columns = 6\n",
    "fontsize = 9\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "fig, axs = plt.subplots(nrows = len(species_df), \n",
    "                        ncols = num_columns, \n",
    "                        sharex = True, \n",
    "                        figsize = (6.5, 11), \n",
    "                        dpi = 600)\n",
    "\n",
    "plt.rcParams['font.size'] = 9\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "axs = np.array(np.concatenate(axs).flat)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for s, voc, c in zip(species_df,species_voc_df, colors):\n",
    "    \n",
    "    g1 = sns.stripplot(data = s, \n",
    "                       y = 'weight_g', \n",
    "                       x = 'age', \n",
    "                       hue = 'sex',\n",
    "                       palette = {'m':'green', 'f':'blue'}, \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 6)])\n",
    "    \n",
    "    g1 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 6)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='weight_g', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 6)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -6)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -6)].get_yticklabels() + axs[num_columns*counter + (num_columns -5)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -6)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -6)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns - 6)].set_ylim([0, 10])\n",
    "    \n",
    "    g1.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g1.set(xticklabels = [])  \n",
    "        g1.set(xlabel = None)\n",
    "        g1.tick_params(bottom = False) \n",
    "    else:\n",
    "        #g1.set(xlabel = 'age (days)')\n",
    "        axs[num_columns*counter + (num_columns -6)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -6)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        \n",
    "    ###\n",
    "    g2 = sns.stripplot(data = s, \n",
    "                       y = 'temp_change', \n",
    "                       x = 'age', \n",
    "                       hue = 'sex',\n",
    "                       palette = {'m':'green', 'f':'blue'}, \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 5)])\n",
    "    \n",
    "    g2 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 5)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='temp_change', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 5)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -5)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -5)].get_yticklabels() + axs[num_columns*counter + (num_columns -5)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -5)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -5)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)    \n",
    "    axs[num_columns*counter + (num_columns - 5)].set_ylim([-5, 15])\n",
    "    \n",
    "    g2.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g2.set(xticklabels = [])  \n",
    "        g2.set(xlabel = None)\n",
    "        g2.tick_params(bottom = False) \n",
    "    else:\n",
    "        \n",
    "        axs[num_columns*counter + (num_columns -5)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -5)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    g3 = sns.stripplot(data = s, \n",
    "                       y = 'cry_per_sec', \n",
    "                       x = 'age', \n",
    "                       color = 'deeppink', \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 4)])\n",
    "    \n",
    "    g3 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 4)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='cry_per_sec', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -4)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -4)].get_yticklabels() + axs[num_columns*counter + (num_columns -4)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -4)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -4)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 4)].set_ylim([-0.5, 2])\n",
    "    axs[num_columns*counter + (num_columns - 4)].set_ylim([-0.5, 2])\n",
    "    \n",
    "    g3.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g3.set(xticklabels = [])  \n",
    "        g3.set(xlabel = None)\n",
    "        g3.tick_params(bottom = False) \n",
    "    else:\n",
    "        \n",
    "        axs[num_columns*counter + (num_columns -4)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -4)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    \n",
    "    g4 = sns.stripplot(data = s, \n",
    "                       y = 'USV_per_sec', \n",
    "                       x = 'age', \n",
    "                       color = 'thistle',\n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 3)])\n",
    "    \n",
    "    g4 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 3)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='USV_per_sec', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -3)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -3)].get_yticklabels() + axs[num_columns*counter + (num_columns -3)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -3)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -3)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 3)].set_ylim([-0.5, 2])\n",
    "    \n",
    "    g4.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g4.set(xticklabels = [])  \n",
    "        g4.set(xlabel = None)\n",
    "        g4.tick_params(bottom = False) \n",
    "    else:\n",
    "        \n",
    "        axs[num_columns*counter + (num_columns -3)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -3)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    g5 = sns.stripplot(data = voc, \n",
    "                       y = 'meanfreq', \n",
    "                       x = 'age', \n",
    "                       hue = 'human_label',\n",
    "                       color = 'black',\n",
    "                       dodge = True,\n",
    "                       s = by_voc_dot_size,\n",
    "                       alpha = by_voc_dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns -2)])\n",
    "\n",
    "    g5 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 2)], \n",
    "                data=voc,\n",
    "                y='meanfreq',\n",
    "                x='age', \n",
    "                hue = 'human_label',   \n",
    "                palette = {'cry':'deeppink', 'USV':'thistle'},\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "\n",
    "    axs[num_columns*counter + (num_columns - 2)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -2)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -2)].get_yticklabels() + axs[num_columns*counter + (num_columns -2)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -2)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -2)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns - 2)].set_ylim([-0.5, 110])\n",
    "    \n",
    "    g5.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g5.set(xticklabels = [])  \n",
    "        g5.set(xlabel = None)\n",
    "        g5.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -2)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -2)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        \n",
    "    \n",
    "    g6 = sns.stripplot(data = voc, \n",
    "                       y = 'duration', \n",
    "                       x = 'age', \n",
    "                       hue = 'human_label',\n",
    "                       color = 'black',\n",
    "                       dodge = True,\n",
    "                       s = by_voc_dot_size,\n",
    "                       alpha = by_voc_dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13],\n",
    "                       ax = axs[num_columns*counter + (num_columns -1)])\n",
    "    \n",
    "    \n",
    "    g6 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 1)], \n",
    "            data=voc,\n",
    "            y='duration',\n",
    "            x='age', \n",
    "            hue = 'human_label',   \n",
    "            palette = {'cry':'deeppink', 'USV':'thistle'},\n",
    "            dodge=True, \n",
    "            linewidth=.5, \n",
    "            saturation=.5,\n",
    "            whis=1.5, \n",
    "            showfliers = False, \n",
    "            flierprops={\"marker\": \"\"},\n",
    "            medianprops={\"color\": \"black\"},\n",
    "            boxprops={\"linewidth\": .00001}, \n",
    "            width = .75, \n",
    "            showcaps=False)\n",
    "\n",
    "    axs[num_columns*counter + (num_columns - 1)].set_ylim([0, 0.5])\n",
    "    axs[num_columns*counter + (num_columns - 1)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -1)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -1)].get_yticklabels() + axs[num_columns*counter + (num_columns -1)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -1)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -1)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    g6.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g6.set(xticklabels = [])  \n",
    "        g6.set(xlabel = None)\n",
    "        g6.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "# Adjust the spacing between the subplots\n",
    "plt.subplots_adjust(wspace = 0.3, hspace = 0.1)\n",
    "sns.despine()\n",
    "\n",
    "save_dir = ''\n",
    "if save:\n",
    "    save_name = 'supplement_fig4_feature_panels.svg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name),dpi=600)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "860c73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "save = False\n",
    "\n",
    "dot_size = 2\n",
    "dot_alpha = .5\n",
    "by_voc_dot_size = .5\n",
    "by_voc_dot_alpha = .5\n",
    "num_columns = 6\n",
    "fontsize = 9\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "fig, axs = plt.subplots(nrows = len(species_df), \n",
    "                        ncols = num_columns, \n",
    "                        sharex = True, \n",
    "                        figsize = (6.5, 11), \n",
    "                        dpi = 600)\n",
    "\n",
    "plt.rcParams['font.size'] = 9\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "axs = np.array(np.concatenate(axs).flat)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for s, voc, c in zip(species_df,species_voc_df, colors):\n",
    "    \n",
    "    g1 = sns.stripplot(data = s, \n",
    "                       y = 'weight_g', \n",
    "                       x = 'age', \n",
    "                       hue = 'sex',\n",
    "                       palette = {'m':'green', 'f':'blue'}, \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 6)])\n",
    "    \n",
    "    g1 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 6)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='weight_g', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 6)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -6)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -6)].get_yticklabels() + axs[num_columns*counter + (num_columns -5)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -6)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -6)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)    \n",
    "    axs[num_columns*counter + (num_columns - 6)].set_ylim([0, 10])\n",
    "    \n",
    "    g1.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g1.set(xticklabels = [])  \n",
    "        g1.set(xlabel = None)\n",
    "        g1.tick_params(bottom = False) \n",
    "    else:\n",
    "\n",
    "        axs[num_columns*counter + (num_columns -6)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -6)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        \n",
    "    ###\n",
    "    g2 = sns.stripplot(data = s, \n",
    "                       y = 'temp_change', \n",
    "                       x = 'age', \n",
    "                       hue = 'sex',\n",
    "                       palette = {'m':'green', 'f':'blue'}, \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 5)])\n",
    "    \n",
    "    g2 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 5)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='temp_change', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 5)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -5)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -5)].get_yticklabels() + axs[num_columns*counter + (num_columns -5)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -5)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -5)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns - 5)].set_ylim([-5, 15])\n",
    "    \n",
    "    g2.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g2.set(xticklabels = [])  \n",
    "        g2.set(xlabel = None)\n",
    "        g2.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -5)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -5)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    g3 = sns.stripplot(data = s, \n",
    "                       y = 'cry_per_sec', \n",
    "                       x = 'age', \n",
    "                       color = 'deeppink', \n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 4)])\n",
    "    \n",
    "    g3 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 4)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='cry_per_sec', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -4)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -4)].get_yticklabels() + axs[num_columns*counter + (num_columns -4)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -4)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -4)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 4)].set_ylim([-0.5, 2])\n",
    "    axs[num_columns*counter + (num_columns - 4)].set_ylim([-0.5, 2])\n",
    "    \n",
    "    g3.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g3.set(xticklabels = [])  \n",
    "        g3.set(xlabel = None)\n",
    "        g3.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -4)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -4)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    \n",
    "    g4 = sns.stripplot(data = s, \n",
    "                       y = 'USV_per_sec', \n",
    "                       x = 'age', \n",
    "                       color = 'thistle',\n",
    "                       s = dot_size,\n",
    "                       alpha = dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns - 3)])\n",
    "    \n",
    "    g4 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 3)], \n",
    "                data=s,\n",
    "                x='age', \n",
    "                y='USV_per_sec', \n",
    "                color = 'grey',\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "    \n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -3)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -3)].get_yticklabels() + axs[num_columns*counter + (num_columns -3)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -3)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -3)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns - 3)].set_ylim([-0.5, 2])\n",
    "    \n",
    "    g4.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g4.set(xticklabels = [])  \n",
    "        g4.set(xlabel = None)\n",
    "        g4.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -3)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -3)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "    \n",
    "    g5 = sns.stripplot(data = voc, \n",
    "                       y = 'meanfreq', \n",
    "                       x = 'age', \n",
    "                       hue = 'human_label',\n",
    "                       color = 'black',\n",
    "                       dodge = True,\n",
    "                       s = by_voc_dot_size,\n",
    "                       alpha = by_voc_dot_alpha,\n",
    "                       order = [1, 3, 5, 7, 9, 11, 13], \n",
    "                       ax = axs[num_columns*counter + (num_columns -2)])\n",
    "\n",
    "    g5 = sns.boxplot(ax=axs[num_columns*counter + (num_columns - 2)], \n",
    "                data=voc,\n",
    "                y='meanfreq',\n",
    "                x='age', \n",
    "                hue = 'human_label',   \n",
    "                palette = {'cry':'deeppink', 'USV':'thistle'},\n",
    "                dodge=True, \n",
    "                linewidth=.5, \n",
    "                saturation=.5,\n",
    "                whis=1.5, \n",
    "                showfliers = False, \n",
    "                flierprops={\"marker\": \"\"},\n",
    "                medianprops={\"color\": \"black\"},\n",
    "                boxprops={\"linewidth\": .00001}, \n",
    "                width = .75, \n",
    "                showcaps=False)\n",
    "\n",
    "    axs[num_columns*counter + (num_columns - 2)].legend([],[], frameon = False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -2)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -2)].get_yticklabels() + axs[num_columns*counter + (num_columns -2)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -2)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -2)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns - 2)].set_ylim([-0.5, 110])\n",
    "    \n",
    "    g5.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g5.set(xticklabels = [])  \n",
    "        g5.set(xlabel = None)\n",
    "        g5.tick_params(bottom = False) \n",
    "    else:\n",
    "\n",
    "        axs[num_columns*counter + (num_columns -2)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -2)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        \n",
    "    g6 = sns.regplot(ax = axs[num_columns*counter + (num_columns -1)], \n",
    "            data=s, \n",
    "            scatter = True,\n",
    "            x = 'age', \n",
    "            y='proportion_cries', \n",
    "            color = 'deeppink', \n",
    "            order=2, \n",
    "            scatter_kws = {\"s\": dot_size, \"alpha\": dot_alpha},  \n",
    "            line_kws={\"linewidth\":1}, \n",
    "            truncate = True)\n",
    "    \n",
    "    axs[num_columns*counter + (num_columns -2)].set_ylim([-0.1, 1.1])\n",
    "  \n",
    "    \n",
    "    g6.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g6.set(xticklabels = [])  \n",
    "        g6.set(xlabel = None)\n",
    "        g6.tick_params(bottom = False) \n",
    "    else:\n",
    "\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "\n",
    "    axs[num_columns*counter + (num_columns -1)].set_ylim([-0.1, 1.1])\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axs[num_columns*counter + (num_columns -1)].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axs[num_columns*counter + (num_columns -1)].get_yticklabels() + axs[num_columns*counter + (num_columns -1)].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axs[num_columns*counter + (num_columns -1)].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axs[num_columns*counter + (num_columns -1)].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    g7 = sns.regplot(ax = axs[num_columns*counter + (num_columns -1)], \n",
    "            data=s, \n",
    "            scatter = True,\n",
    "            x = 'age', \n",
    "            y='proportion_USVs', \n",
    "            color = 'thistle', \n",
    "            order=2,  \n",
    "            line_kws={\"linewidth\":1},\n",
    "            scatter_kws = {\"s\": dot_size, \"alpha\": dot_alpha},\n",
    "            truncate = True)\n",
    " \n",
    "    g7.set(ylabel = None)\n",
    "    \n",
    "    if c != 'forestgreen':\n",
    "        g7.set(xticklabels = [])  \n",
    "        g7.set(xlabel = None)\n",
    "        g7.tick_params(bottom = False) \n",
    "    else:\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xlabel(xlabel = 'age (d)', fontsize=fontsize)\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xticks([1,3,5,7,9,11,13])\n",
    "        axs[num_columns*counter + (num_columns -1)].set_xticklabels([1,3,5,7,9,11,13])\n",
    "#####\n",
    "    counter += 1\n",
    "    \n",
    "# Adjust the spacing between the subplots\n",
    "plt.subplots_adjust(wspace = 0.3, hspace = 0.1)\n",
    "sns.despine()\n",
    "\n",
    "save_dir = ''\n",
    "if save:\n",
    "    save_name = 'supplement_fig4_proportions_panel.svg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name),dpi=600)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9bb72",
   "metadata": {},
   "source": [
    "# Supplemental Figure 5 (inter-onset intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4ea75",
   "metadata": {},
   "source": [
    "## get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c879d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to all_development_vocs_with_predictions.csv\n",
    "save_dir = ''\n",
    "vocs_save_name = 'all_development_vocs_with_predictions.csv'\n",
    "\n",
    "#the vocalizations\n",
    "development_vocs_df = pd.read_csv(os.path.join(save_dir, vocs_save_name))\n",
    "\n",
    "#drop the features - you don't need them\n",
    "development_vocs_df = development_vocs_df.drop(columns = [i for i in params_dict['supp_figure_3']['features'] if i!='duration'])\n",
    "\n",
    "#their start and stop times (path to all_development_vocs_with_start_stop_times.csv)\n",
    "development_vocs_times_dir = ''\n",
    "development_vocs_times_name = 'all_development_vocs_with_start_stop_times.csv'\n",
    "development_vocs_times_df = pd.read_csv(os.path.join(development_vocs_times_dir, development_vocs_times_name))\n",
    "\n",
    "#clean up before merging\n",
    "development_vocs_times_df = development_vocs_times_df.loc[development_vocs_times_df['species'].isin(development_vocs_df['species'].unique())]\n",
    "development_vocs_times_df = development_vocs_times_df.rename(columns={'source_file': 'full_wav_file'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ca57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get voc numbers needed for merging\n",
    "from tqdm import tqdm\n",
    "voc_numbers = []\n",
    "for i in tqdm(development_vocs_times_df['full_wav_file'].unique()):\n",
    "    num_vocs = len(development_vocs_times_df.loc[development_vocs_times_df['full_wav_file'] == i])\n",
    "    voc_numbers.extend(list(range(num_vocs)))\n",
    "development_vocs_times_df['clip_number'] = voc_numbers\n",
    "\n",
    "#mrecreate clip .wav names for merging\n",
    "development_vocs_times_df['source_file'] = [i.split('.')[0] + '_clip_' + str(j) + '.wav' for i,j in zip(development_vocs_times_df['full_wav_file'], development_vocs_times_df['clip_number'])]\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ccb2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merged_df = development_vocs_times_df.merge(development_vocs_df, on = 'source_file')\n",
    "merged_df['duration_x'] = [np.round(i,3) for i in merged_df['duration_x']]\n",
    "merged_df['duration_y'] = [np.round(i,3) for i in merged_df['duration_y']]\n",
    "assert merged_df['duration_x'].equals(merged_df['duration_y'])\n",
    "assert merged_df['species_x'].equals(merged_df['species_y'])\n",
    "merged_df = merged_df.drop(columns = ['duration_x', 'species_x'])\n",
    "merged_df = merged_df.rename(columns = {'duration_y':'duration', 'species_y':'species'})\n",
    "\n",
    "#check there are no duplicates\n",
    "assert merged_df.duplicated(subset = 'source_file').sum() == 0\n",
    "\n",
    "#drop scratches and Mus\n",
    "merged_df = merged_df.loc[merged_df['predicted_label'].isin(['cry', 'USV'])]\n",
    "merged_df = merged_df.loc[~merged_df['species'].isin(['MU', 'MZ'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "862f1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get interonset intervals by vocalization type \n",
    "\n",
    "# CALCULATE IOIs\n",
    "# use the start and stop times from the data frame you just generated to calculate interonset intervals for \n",
    "# cries and USVs at a given age\n",
    "from tqdm import tqdm\n",
    "merged_df['age'] = [int(i.split('_')[10][1:]) for i in merged_df['source_file']]\n",
    "pups = merged_df['full_wav_file'].unique()\n",
    "voc_types = merged_df['predicted_label'].unique()\n",
    "print(len(pups))\n",
    "cry_or_USV = []\n",
    "IOIs = []\n",
    "ages = []\n",
    "full_wav_files = []\n",
    "source_files = []\n",
    "pups_done = 0\n",
    "pups_total = len(merged_df)\n",
    "\n",
    "for voc_type in voc_types:\n",
    "    print(voc_type)\n",
    "    \n",
    "    for pup in tqdm(pups):\n",
    "            \n",
    "        tdf = merged_df.loc[merged_df['predicted_label'] == voc_type].loc[merged_df['full_wav_file'] == pup].sort_values(by = 'start_seconds')\n",
    "        \n",
    "        #get the interonset interval \n",
    "        IOIs.extend(tdf['start_seconds'].diff()) \n",
    " \n",
    "        #get the other info you need to add this back to the main phenotypes file\n",
    "        ages.extend(list(merged_df['age'].loc[merged_df['predicted_label'] == voc_type].loc[merged_df['full_wav_file'] == pup]))\n",
    "        full_wav_files.extend(list(merged_df['full_wav_file'].loc[merged_df['predicted_label'] == voc_type].loc[merged_df['full_wav_file'] == pup]))\n",
    "        cry_or_USV.extend(list(merged_df['predicted_label'].loc[merged_df['predicted_label'] == voc_type].loc[merged_df['full_wav_file'] == pup]))\n",
    "        source_files.extend(list(merged_df['source_file'].loc[merged_df['predicted_label'] == voc_type].loc[merged_df['full_wav_file'] == pup]))\n",
    "        pups_done+=1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b515d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a df for the interonset intervals\n",
    "ioi_df = pd.DataFrame()\n",
    "ioi_df['ioi'] = IOIs\n",
    "ioi_df['age'] = ages\n",
    "ioi_df['full_wav_file'] = full_wav_files\n",
    "ioi_df['species'] = [i.split('_')[0] for i in ioi_df['full_wav_file']]\n",
    "ioi_df['source_file'] = source_files\n",
    "ioi_df['clip_number'] = [i.split('_clip_')[-1].split('.')[0] for i in ioi_df['source_file']]\n",
    "ioi_df['voc_type'] = cry_or_USV\n",
    "ioi_df['log_ioi'] = np.log2(ioi_df['ioi'])\n",
    "ioi_df = ioi_df.dropna()\n",
    "ioi_df.to_csv('', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f642d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ioi distributions by pup in grey and pooled in black\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "save = False\n",
    "\n",
    "fontsize=9\n",
    "avg_width = 2\n",
    "avg_alpha = .9\n",
    "individual_alpha = 0.25\n",
    "individual_width = .5\n",
    "fontsize =6\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(nrows =8, \n",
    "                         ncols =2, \n",
    "                         constrained_layout=True, \n",
    "                         figsize = [3,9], \n",
    "                         dpi=600, \n",
    "                         sharex=True, \n",
    "                         sharey=True)\n",
    "\n",
    "viridis_big = cm.get_cmap('viridis', 8)\n",
    "newcmp = ListedColormap(viridis_big(np.linspace(0, 1, 8)))\n",
    "age_color_dict = {1: newcmp(0.01), \n",
    "                  3: newcmp(0.18), \n",
    "                  5: newcmp(0.35),\n",
    "                  7: newcmp(0.5),\n",
    "                  9: newcmp(0.68),\n",
    "                  11: newcmp(0.85),\n",
    "                  13: newcmp(0.98)}\n",
    "\n",
    "import seaborn as sns\n",
    "color_dict = {1:'blue', 3:'green'}\n",
    "all_cry_iois = []\n",
    "all_USV_iois = []\n",
    "species_list = ['BW', 'SW', 'NB', 'BK', 'PO', 'LO', 'GO', 'LL']\n",
    "\n",
    "for species, ax in zip(species_list, range(8)):\n",
    "    \n",
    "    #plot cries\n",
    "    cry_species_ioi_df = ioi_df.loc[ioi_df['species'] == species].loc[ioi_df['voc_type'] == 'cry']\n",
    "    USV_species_ioi_df = ioi_df.loc[ioi_df['species'] == species].loc[ioi_df['voc_type'] == 'USV']\n",
    "\n",
    "    df = cry_species_ioi_df\n",
    "\n",
    "    #for each pup\n",
    "    for pup in df['full_wav_file'].unique():\n",
    "        if len(df.loc[df['full_wav_file'] == pup]) > 10:\n",
    "            temp = df.loc[df['full_wav_file'] == pup]\n",
    "            sns.kdeplot(temp['ioi'], \n",
    "                        alpha=individual_alpha, \n",
    "                        linewidth=individual_width, \n",
    "                        color = age_color_dict[temp['age'].unique()[0]], \n",
    "                        ax=axes[ax, 0], log_scale=True)\n",
    "    \n",
    "    #pooled\n",
    "    sns.kdeplot(df['ioi'], \n",
    "                color='black',\n",
    "                linewidth =avg_width, \n",
    "                alpha = avg_alpha,  \n",
    "                ax=axes[ax, 0], \n",
    "                log_scale=True, \n",
    "                legend=False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[ax,0].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axes[ax,0].get_yticklabels() + axes[ax,0].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axes[ax,0].set_xlim([10e-3,10e2])\n",
    "    axes[ax,0].set_ylabel('')\n",
    "    axes[ax,0].set_xlabel('interonset interval (s)', fontsize=fontsize)\n",
    "    axes[ax,0].yaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[ax,0].xaxis.set_tick_params(width=.5, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "    \n",
    "    #plot USVs\n",
    "    df = USV_species_ioi_df\n",
    "\n",
    "    #for each pup\n",
    "    for pup in df['full_wav_file'].unique():\n",
    "        if len(df.loc[df['full_wav_file'] == pup]) > 10:\n",
    "            temp = df.loc[df['full_wav_file'] == pup]\n",
    "            sns.kdeplot(temp['ioi'], \n",
    "                        alpha = individual_alpha, \n",
    "                        linewidth=individual_width, \n",
    "                        color = age_color_dict[temp['age'].unique()[0]], \n",
    "                        ax=axes[ax, 1], \n",
    "                        log_scale=True)\n",
    "\n",
    "    \n",
    "    #pooled\n",
    "    sns.kdeplot(df['ioi'], \n",
    "                color='black',\n",
    "                linewidth =avg_width, \n",
    "                alpha = avg_alpha,  \n",
    "                ax=axes[ax, 1], \n",
    "                log_scale=True, \n",
    "                legend=False)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        axes[ax,1].spines[axis].set_linewidth(.5)\n",
    "        \n",
    "    for label in (axes[ax,1].get_yticklabels() + axes[ax,1].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "        \n",
    "    axes[ax,1].set_xlim([10e-3,10e2])\n",
    "    axes[ax,1].set_ylabel('')\n",
    "    axes[ax,1].set_xlabel('interonset interval (s)', fontsize=fontsize)\n",
    "    axes[ax,1].yaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[ax,1].xaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    sns.despine()\n",
    "    \n",
    "save_dir = ''\n",
    "if save:\n",
    "    save_name = 'ioi_plots.svg'\n",
    "    plt.savefig(os.path.join(save_dir,save_name),dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48196d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all the pups\n",
    "\n",
    "save = False\n",
    "fontsize =6\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows =8, \n",
    "                         ncols =4, \n",
    "                         constrained_layout=True, \n",
    "                         figsize = [4,8], \n",
    "                         dpi=600)\n",
    "\n",
    "\n",
    "species_list = ['BW', 'SW', 'NB', 'BK', 'PO', 'LO', 'GO', 'LL']\n",
    "\n",
    "for row, species in enumerate(species_list):\n",
    "    print(species)\n",
    "\n",
    "    #these are hand picked examples - I looked at max three pups from each age and picked ones that had \n",
    "    # a similar number of cries and USVs, each >~20\n",
    "    if species == 'BK':\n",
    "        age = 11\n",
    "        pup_num = 15\n",
    "\n",
    "    if species == 'BW':\n",
    "        age = 9\n",
    "        pup_num = 2\n",
    "\n",
    "    if species == 'NB':\n",
    "        age = 9\n",
    "        pup_num = 0\n",
    "\n",
    "    if species == 'SW':\n",
    "        age = 9\n",
    "        pup_num = 6\n",
    "\n",
    "    if species == 'PO':\n",
    "        age = 13\n",
    "        pup_num = 2\n",
    "\n",
    "    if species == 'LO':\n",
    "        age = 11\n",
    "        pup_num = 0\n",
    "\n",
    "    if species == 'GO':\n",
    "        age = 9\n",
    "        pup_num = 1\n",
    "\n",
    "    if species == 'LL':\n",
    "        age = 11\n",
    "        pup_num = 1\n",
    "        \n",
    "    pup = ioi_df['full_wav_file'].loc[ioi_df['species'] == species].loc[ioi_df['age'] == age].unique()[pup_num]\n",
    "    pup_name = pup.split('.')[0]\n",
    "    ISI_name = 'ioi'\n",
    "    cdf = ioi_df.loc[ioi_df['voc_type'] == 'cry'].loc[ioi_df['full_wav_file'] == pup][1:].sort_values(by = ISI_name)\n",
    "    wdf = ioi_df.loc[ioi_df['voc_type'] == 'USV'].loc[ioi_df['full_wav_file'] == pup][1:].sort_values(by = ISI_name)\n",
    "\n",
    "    #calculate difference between each ISI and every other ISI\n",
    "    import numpy as np\n",
    "\n",
    "    #initialize arrays\n",
    "    cIOIdiffs = np.zeros([len(cdf), len(cdf)])\n",
    "    wIOIdiffs = np.zeros([len(wdf), len(wdf)])\n",
    "\n",
    "    #for each cry logISI value, subtract it from all other logISI values including itself, take the absolute value, and add \n",
    "    #all these values to the cIOIdiffs array you just made\n",
    "    for crow in range(len(cIOIdiffs)):\n",
    "        cIOIdiff = list(abs(cdf['log_ioi'] - cdf['log_ioi'].iloc[crow]))\n",
    "        cIOIdiffs[crow] = cIOIdiff\n",
    "\n",
    "    #do the same thing but for USVs\n",
    "    for wrow in range(len(wIOIdiffs)):\n",
    "        wIOIdiff = list(abs(wdf['log_ioi'] - wdf['log_ioi'].iloc[wrow]))\n",
    "        wIOIdiffs[wrow] = wIOIdiff\n",
    "\n",
    "    #Make the plots\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #make a mask for the USVs to hide upper (redundant) triangle of matrix\n",
    "    wmask = np.zeros_like(wIOIdiffs)\n",
    "    wmask[np.triu_indices_from(wmask)] = True\n",
    "\n",
    "    #make a mask for the criesto hide upper (redundant) triangle of matrix\n",
    "    cmask = np.zeros_like(cIOIdiffs)\n",
    "    cmask[np.triu_indices_from(cmask)] = True\n",
    "\n",
    "    sns.histplot(cdf['ioi'], ax=axes[row,0], binwidth = .2, linewidth = .001, log_scale=True, color = 'deeppink')\n",
    "    sns.heatmap(cIOIdiffs, ax=axes[row,1], mask = cmask, cmap = 'viridis')\n",
    "    sns.histplot(wdf['ioi'], ax=axes[row,2], binwidth = .2, linewidth = .001, log_scale=True, color = 'thistle')\n",
    "    sns.heatmap(wIOIdiffs, ax=axes[row,3], mask = wmask, cmap = 'viridis', annot_kws={\"fontsize\":8}, cbar_kws = {'drawedges':False})\n",
    "\n",
    "    for i in [0,1,2,3]:\n",
    "\n",
    "        axes[row,i].yaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "        axes[row,i].xaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "                axes[row,i].spines[axis].set_linewidth(.5)\n",
    "\n",
    "        for label in (axes[row,i].get_yticklabels() + axes[row,i].get_xticklabels()):\n",
    "            label.set_fontname('Arial')\n",
    "            label.set_fontsize(fontsize)\n",
    "\n",
    "\n",
    "    axes[row,0].set_ylabel('')\n",
    "    axes[row,2].set_ylabel('')\n",
    "    axes[row,0].set_xlabel('')\n",
    "    axes[row,2].set_xlabel('')\n",
    "    axes[row,1].axes.get_xaxis().set_visible(False)\n",
    "    axes[row,1].axes.get_yaxis().set_visible(False)\n",
    "    axes[row,3].axes.get_xaxis().set_visible(False)\n",
    "    axes[row,3].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    cbar = axes[row,1].collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=6, grid_linewidth=0, width=0.5)\n",
    "    cbar.outline.set_linewidth(0)\n",
    "\n",
    "    cbar = axes[row,3].collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=6, grid_linewidth=0, width=0.5)\n",
    "    cbar.outline.set_linewidth(0)\n",
    "\n",
    "    sns.despine()\n",
    "    fig.show()\n",
    "    \n",
    "    save_dir = ''\n",
    "    if save:\n",
    "        save_name = species+'_pups_example_IOIs.jpeg'\n",
    "        plt.savefig(os.path.join(save_dir,save_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a df of differences between every IOI and every other IOI\n",
    "save=False\n",
    "\n",
    "species = 'BK'\n",
    "age = 11\n",
    "pup_num = 15\n",
    "\n",
    "fontsize =6\n",
    "ytick_length = 2\n",
    "ytick_pad = 0.5\n",
    "\n",
    "\n",
    "#plot\n",
    "fig, axes = plt.subplots(nrows =1, \n",
    "                         ncols =4, \n",
    "                         constrained_layout=True, \n",
    "                         figsize = [4,1], \n",
    "                         dpi=600)\n",
    "\n",
    "pup = ioi_df['full_wav_file'].loc[ioi_df['species'] == species].loc[ioi_df['age'] == age].unique()[pup_num]\n",
    "pup_name = pup.split('.')[0]\n",
    "ISI_name = 'ioi'\n",
    "cdf = ioi_df.loc[ioi_df['voc_type'] == 'cry'].loc[ioi_df['full_wav_file'] == pup][1:].sort_values(by = ISI_name)\n",
    "wdf = ioi_df.loc[ioi_df['voc_type'] == 'USV'].loc[ioi_df['full_wav_file'] == pup][1:].sort_values(by = ISI_name)\n",
    "\n",
    "#calculate difference between each ISI and every other ISI\n",
    "import numpy as np\n",
    "\n",
    "#initialize arrays\n",
    "cIOIdiffs = np.zeros([len(cdf), len(cdf)])\n",
    "wIOIdiffs = np.zeros([len(wdf), len(wdf)])\n",
    "    \n",
    "#for each cry logISI value, subtract it from all other logISI values including itself, take the absolute value, and add \n",
    "#all these values to the cIOIdiffs array you just made\n",
    "for crow in range(len(cIOIdiffs)):\n",
    "    cIOIdiff = list(abs(cdf['log_ioi'] - cdf['log_ioi'].iloc[crow]))\n",
    "    cIOIdiffs[crow] = cIOIdiff\n",
    "\n",
    "#do the same thing but for USVs\n",
    "for wrow in range(len(wIOIdiffs)):\n",
    "    wIOIdiff = list(abs(wdf['log_ioi'] - wdf['log_ioi'].iloc[wrow]))\n",
    "    wIOIdiffs[wrow] = wIOIdiff\n",
    "\n",
    "#Make the plots\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#make a mask for the USVs to hide upper (redundant) triangle of matrix\n",
    "wmask = np.zeros_like(wIOIdiffs)\n",
    "wmask[np.triu_indices_from(wmask)] = True\n",
    "\n",
    "#make a mask for the criesto hide upper (redundant) triangle of matrix\n",
    "cmask = np.zeros_like(cIOIdiffs)\n",
    "cmask[np.triu_indices_from(cmask)] = True\n",
    "\n",
    "sns.histplot(cdf['ioi'], ax=axes[0], binwidth = .2, linewidth = .001, log_scale=True, color = 'deeppink')\n",
    "sns.heatmap(cIOIdiffs, ax=axes[1], mask = cmask, cmap = 'viridis')\n",
    "sns.histplot(wdf['ioi'], ax=axes[2], binwidth = .2, linewidth = .001, log_scale=True, color = 'thistle')\n",
    "sns.heatmap(wIOIdiffs, ax=axes[3], mask = wmask, cmap = 'viridis', annot_kws={\"fontsize\":8}, cbar_kws = {'drawedges':False})\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    \n",
    "    axes[i].yaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    axes[i].xaxis.set_tick_params(width=.1, rotation = 0, length = ytick_length, pad = ytick_pad)\n",
    "    \n",
    "    for axis in ['top','bottom','left','right']:\n",
    "            axes[i].spines[axis].set_linewidth(.5)\n",
    "\n",
    "    for label in (axes[i].get_yticklabels() + axes[i].get_xticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(fontsize)\n",
    "\n",
    "          \n",
    "axes[0].set_ylabel('')\n",
    "axes[2].set_ylabel('')\n",
    "axes[0].set_xlabel('')\n",
    "axes[2].set_xlabel('')\n",
    "axes[1].axes.get_xaxis().set_visible(False)\n",
    "axes[1].axes.get_yaxis().set_visible(False)\n",
    "axes[3].axes.get_xaxis().set_visible(False)\n",
    "axes[3].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "cbar = axes[1].collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=6, grid_linewidth=0, width=0.5)\n",
    "cbar.outline.set_linewidth(0)\n",
    "\n",
    "cbar = axes[3].collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=6, grid_linewidth=0, width=0.5)\n",
    "cbar.outline.set_linewidth(0)\n",
    "\n",
    "sns.despine()\n",
    "fig.show()\n",
    "\n",
    "save_dir = ''\n",
    "if save:\n",
    "    save_name = ('_').join([pup_name,'example_IOIs.jpeg'])\n",
    "    plt.savefig(os.path.join(save_dir,save_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manuscript",
   "language": "python",
   "name": "manuscript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
